<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="x-ua-compatible" content="ie=edge" />
    <meta http-equiv="Content-Style-Type" content="text/css" />
    <meta name="author" content="district10" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Machine Learning</title>
    <link rel="stylesheet" href="../github-markdown.css" type="text/css" />
    <link rel="stylesheet" href="../highlight.css" type="text/css" />
    <link rel="stylesheet" href="../notes.css" type="text/css" />
<!--<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>-->
    <script src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body class="markdown-body">
<div id="navigator">
    <a id="gotoindex" href="index.html" title="&#12304;&#22238;&#21040;&#31508;&#35760;&#32034;&#24341; | Back to Index&#12305;">&#9763;</a></div>
<div id="main-body">
<h1 id="machine-learning">Machine Learning</h1>
<p>Featured links</p>
<ul>
<li><a href="backpropagation.html" class="heart featured" title="backpropagation.md">Principles of training multi-layer neural network using backpropagation</a></li>
</ul>
<p>&#32593;&#19978;&#30475;&#21040;&#19968;&#21477;&#24456;&#34507;&#30140;&#30340;&#35805;&#65306;</p>
<blockquote>
<p>&#24050;&#32463;&#24456;&#20037;&#27809;&#30475; lstm &#20102;, &#24456;&#22810;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#24050;&#32463;&#19981;&#20102;&#35299;&#20102;&#35830;&#12290;</p>
</blockquote>
<dl>
<dt>MISC Notes <code class="fold">@</code></dt>
<dd><ul>
<li><strong>contrived</strong> example, &#36896;&#30340;&#20363;&#23376;</li>
<li>logistic, <code>[lo'd&#658;&#618;st&#618;k]</code>, adj. &#21518;&#21220;&#23398;&#30340;&#65307;&#31526;&#21495;&#36923;&#36753;&#30340;</li>
<li>hessian, <code>['h&#603;&#643;&#601;n]</code></li>
<li>theano, thy yah noo</li>
</ul>
</dd>
<dt>python notes <code class="fold">@</code></dt>
<dd><ul>
<li><p>cp src dst, mv src dst<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># shutil: shell util</span>
<span class="im">import</span> shutil               <span class="co"># from shutil import copyfile copy, copyfile</span>

shutil.move(src, dst)       <span class="co"># recursive</span>
shutil.copy(src, dst)       <span class="co"># dst can be a dir</span>
shutil.copyfile(src, dst)   <span class="co"># dst can only be a file</span></code></pre></div></li>
</ul>
</dd>
<dt>yhlleo &#25512;&#33616;&#30340;&#35770;&#25991; <code class="fold">@</code></dt>
<dd><pre><code>ml
&#9500;&#9472;&#9472; DeepLearning
&#9474;&#160;&#160; &#9500;&#9472;&#9472; Bag-of-Words models.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; Beyond Bags of Features Spatial Pyramid Matching-CVPR2006.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; Deep Learning - Methods and Applications.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; Fast accurate detection of 100000 object classes on a single machine-CVPR2013.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; faster r-cnn towards real-time object detection with region proposal networks-NIPS2015.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; Fast R-CNN-ICCV2015.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; ImageNet Classification with Deep Convolutional Neural Networks-NIPS2012.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; LabelMe a database and web-based tool for image.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; Microsoft COCO Common Objects in Context-ECCV2014.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; Object Detection with Discriminatively Trained Part Based Model-ppt.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; Object Detection with Discriminatively Trained Part-Based Models-PAMI2014.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; R-CNN for object detection-ppt.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; Recognition using Regions-CVPR2009.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; Regionlets for Generic Object Detection-ICCV2013.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; Rich feature hierarchies for accurate object detection and semantic segmentation-CVPR2013.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; Selective Search for Object Recognition-IJCV2013.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; Semantic Segmentation with Second-Order Pooling.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition-PAMI2015.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; What is an object-CVPR2010.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; &#21338;&#23458;Bag-of-words.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; &#21338;&#23458;SPM.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; &#22914;&#20309;&#35780;&#20215;rcnn&#12289;fast-rcnn&#21644;faster-rcnn&#36825;&#19968;&#31995;&#21015;&#26041;&#27861;&#65311; - &#26426;&#22120;&#23398;&#20064; - &#30693;&#20046;.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; &#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#29702;&#35299;&#65306;SSP.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; &#35770;&#25991;&#31508;&#35760;Fast R-CNN.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; &#35770;&#25991;&#31508;&#35760;SPP-net.pdf
&#9474;&#160;&#160; &#9492;&#9472;&#9472; &#35770;&#25991;&#31508;&#35760;SPP.pdf
&#9500;&#9472;&#9472; EdgeContour
&#9474;&#160;&#160; &#9500;&#9472;&#9472; CVPR2015-DeepContour-A-Deep-Convolutional-Feature-Learned-by-Positive-sharing-Loss-for-Contour-Detection-draft-version.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; CVPR2015-DeepEdge-A-Multi-Scale-Bifurcated-Deep-Network-for-Top-Down-Contour-Detection.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; DeepContour - A Deep Convolutional Feature Learned by Positive-sharing Lossfor Contour Detection.pdf
&#9474;&#160;&#160; &#9500;&#9472;&#9472; DeepEdge - A Multi-Scale Bifurcated Deep Networkfor Top-Down Contour Detection.pdf
&#9474;&#160;&#160; &#9492;&#9472;&#9472; EdgeLineDetection.pdf
&#9492;&#9472;&#9472; TextDetection
    &#9500;&#9472;&#9472; 1510.03283v1.pdf
    &#9500;&#9472;&#9472; 5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf
    &#9500;&#9472;&#9472; Automatic Script Identification in the Wild.pdf
    &#9500;&#9472;&#9472; Detecting Texts of Arbitrary Orientations in Natural Images.pdf
    &#9500;&#9472;&#9472; FCS_TextSurvey_2015.pdf
    &#9500;&#9472;&#9472; Gordo_Supervised_Mid-Level_Features_2015_CVPR_paper.pdf
    &#9500;&#9472;&#9472; Text Flow - A Unified Text Detection System in Natural Scene Images.pdf
    &#9500;&#9472;&#9472; wangwucoatesng_icpr2012.pdf
    &#9492;&#9472;&#9472; Yao_Strokelets_2014_CVPR_paper.pdf

3 directories, 40 files

- 1510.03283v1.pdf
- 5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf
- Automatic Script Identification in the Wild.pdf
- Bag-of-Words models.pdf
- Beyond Bags of Features Spatial Pyramid Matching-CVPR2006.pdf
- CVPR2015-DeepContour-A-Deep-Convolutional-Feature-Learned-by-Positive-sharing-Loss-for-Contour-Detection-draft-version.pdf
- CVPR2015-DeepEdge-A-Multi-Scale-Bifurcated-Deep-Network-for-Top-Down-Contour-Detection.pdf
- Deep Learning - Methods and Applications.pdf
- DeepContour - A Deep Convolutional Feature Learned by Positive-sharing Lossfor Contour Detection.pdf
- DeepEdge - A Multi-Scale Bifurcated Deep Networkfor Top-Down Contour Detection.pdf
- Detecting Texts of Arbitrary Orientations in Natural Images.pdf
- EdgeLineDetection.pdf
- FCS_TextSurvey_2015.pdf
- Fast R-CNN-ICCV2015.pdf
- Fast accurate detection of 100000 object classes on a single machine-CVPR2013.pdf
- Gordo_Supervised_Mid-Level_Features_2015_CVPR_paper.pdf
- ImageNet Classification with Deep Convolutional Neural Networks-NIPS2012.pdf
- LabelMe a database and web-based tool for image.pdf
- Microsoft COCO Common Objects in Context-ECCV2014.pdf
- Object Detection with Discriminatively Trained Part Based Model-ppt.pdf
- Object Detection with Discriminatively Trained Part-Based Models-PAMI2014.pdf
- R-CNN for object detection-ppt.pdf
- Recognition using Regions-CVPR2009.pdf
- Regionlets for Generic Object Detection-ICCV2013.pdf
- Rich feature hierarchies for accurate object detection and semantic segmentation-CVPR2013.pdf
- Selective Search for Object Recognition-IJCV2013.pdf
- Semantic Segmentation with Second-Order Pooling.pdf
- Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition-PAMI2015.pdf
- Text Flow - A Unified Text Detection System in Natural Scene Images.pdf
- What is an object-CVPR2010.pdf
- Yao_Strokelets_2014_CVPR_paper.pdf
- faster r-cnn towards real-time object detection with region proposal networks-NIPS2015.pdf
- wangwucoatesng_icpr2012.pdf
- &#21338;&#23458;Bag-of-words.pdf
- &#21338;&#23458;SPM.pdf
- &#22914;&#20309;&#35780;&#20215;rcnn&#12289;fast-rcnn&#21644;faster-rcnn&#36825;&#19968;&#31995;&#21015;&#26041;&#27861;&#65311; - &#26426;&#22120;&#23398;&#20064; - &#30693;&#20046;.pdf
- &#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#29702;&#35299;&#65306;SSP.pdf
- &#35770;&#25991;&#31508;&#35760;Fast R-CNN.pdf
- &#35770;&#25991;&#31508;&#35760;SPP-net.pdf
- &#35770;&#25991;&#31508;&#35760;SPP.pdf</code></pre>
</dd>
<dt>&#20108;&#65306;&#35838;&#31243;&#36164;&#28304; <code class="fold">@</code></dt>
<dd><ol style="list-style-type: decimal">
<li>Tom Mitchell&#65306;http://work.caltech.edu/library/181.html http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml</li>
<li>Andrew Ng&#65306;https://www.coursera.org/learn/machine-learning/home/welcome</li>
<li>NewYork University&#65306;http://cs.nyu.edu/~dsontag/courses/ml14/</li>
<li>Stanford CS231:http://vision.stanford.edu/teaching/cs231n/index.html</li>
<li>Youshua Bengio&#65306;http://deeplearning.net/tutorial/&#20182;&#32534;&#20889;&#30340;&#20070;&#12298;Deep Learning&#12299;&#65306;http://deeplearning.net/tutorial/contents.html</li>
<li>Andrew Stanford&#35838;&#31243;&#65306;UFLDL&#65306;http://ufldl.stanford.edu/tutorial/</li>
</ol>
</dd>
<dt>&#19977;&#12289;&#20195;&#30721;&#36164;&#28304;&#65306; <code class="fold">@</code></dt>
<dd><ol style="list-style-type: decimal">
<li>Keras&#65306;https://github.com/fchollet/keras Keras Documentation&#65306;http://keras.io/</li>
<li>Scikit-Learn: Machine Learning in Python: http://scikit-learn.org/stable/</li>
<li>PredictionIO: Open Source Machine Learning Server: https://prediction.io/</li>
<li>Visual Recognition and Search(&#35745;&#31639;&#26426;&#35270;&#35273;&#36164;&#28304;&#27719;&#24635;&#32593;&#31449;): http://rogerioferis.com/VisualRecognitionAndSearch2014/Resources.html</li>
<li>JMLR Machine Learning Open Source Software: http://jmlr.org/mloss/</li>
</ol>
</dd>
<dt><a href="https://www.zhihu.com/question/35887527">&#22914;&#20309;&#35780;&#20215; rcnn&#12289;fast-rcnn &#21644; faster-rcnn &#36825;&#19968;&#31995;&#21015;&#26041;&#27861;&#65311; - &#30693;&#20046;</a> <code class="fold">@</code></dt>
<dd><p>(res) &#22914;&#20309;&#35780;&#20215; rcnn&#12289;fast-rcnn &#21644; faster-rcnn &#36825;&#19968;&#31995;&#21015;&#26041;&#27861;&#65311; - &#26426;&#22120;&#23398;&#20064; - &#30693;&#20046;.pdf</p>
<p>&#25552;&#21040;&#36825;&#20004;&#20010;&#24037;&#20316;&#65292;&#19981;&#24471;&#19981;&#25552;&#21040; RBG &#22823;&#31070; rbg&#8217;s home page&#65292;&#35813;&#22823;&#31070;&#22312;&#35835;&#21338;&#22763;&#30340;&#26102;&#20505;&#23601;&#22240;&#20026; dpm &#33719;&#24471;&#36807; pascal voc &#30340;&#32456;&#36523;&#25104;&#23601;&#22870;&#12290;&#21338;&#22763;&#21518;&#26399;&#38388;&#26356;&#26159;&#19981;&#26029;&#21457;&#21147;&#65292;RCNN &#21644; Fast- RCNN &#23601;&#26159;&#20182;&#30340;&#20856;&#22411;&#20316;&#21697;&#12290;</p>
<p>RCNN&#65306;RCNN &#21487;&#20197;&#30475;&#20316;&#26159; RegionProposal+CNN &#36825;&#19968;&#26694;&#26550;&#30340;&#24320;&#23665;&#20043;&#20316;&#65292;&#22312; imgenet/voc/mscoco &#19978;&#22522;&#26412;&#19978;&#25152;&#26377; top &#30340;&#26041;&#27861;&#37117;&#26159;&#36825;&#20010;&#26694;&#26550;&#65292;&#21487;&#35265;&#20854;&#24433;&#21709;&#20043;&#22823;&#12290;RCNN &#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#37325;&#22797;&#35745;&#31639;&#65292;&#21518;&#26469; MSRA &#30340; kaiming &#32452;&#30340; SPPNET &#20570;&#20102;&#30456;&#24212;&#30340;&#21152;&#36895;&#12290;</p>
<p>Fast-RCNN&#65306;RCNN &#30340;&#21152;&#36895;&#29256;&#26412;&#65292;&#22312;&#25105;&#30475;&#26469;&#65292;&#36825;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#21152;&#36895;&#29256;&#26412;&#65292;&#20854;&#20248;&#28857;&#36824;&#21253;&#25324;&#65306;</p>
<ul>
<li>&#39318;&#20808;&#65292;&#23427;&#25552;&#20379;&#20102;&#22312; caffe &#30340;&#26694;&#26550;&#19979;&#65292;&#22914;&#20309;&#23450;&#20041;&#33258;&#24049;&#30340;&#23618; / &#21442;&#25968; / &#32467;&#26500;&#30340;&#33539;&#20363;&#65292;&#36825;&#20010;&#33539;&#20363;&#30340;&#19968;&#20010;&#37325;&#35201;&#30340;&#24212;&#29992;&#26159; python layer &#30340;&#24212;&#29992;&#65292;&#25105;&#22312;&#36825;&#37324;&#25903;&#25345;&#22810; label &#30340; caffe&#65292;&#26377;&#27604;&#36739;&#22909;&#30340;&#23454;&#29616;&#21527;&#65311; - &#23380;&#28059;&#30340;&#22238;&#31572;&#20063;&#25552;&#21040;&#20102;&#12290;</li>
<li>training and testing end-to-end &#36825;&#19968;&#28857;&#24456;&#37325;&#35201;&#65292;&#20026;&#20102;&#36798;&#21040;&#36825;&#19968;&#28857;&#20854;&#23450;&#20041;&#20102; ROIPooling &#23618;&#65292;&#22240;&#20026;&#26377;&#20102;&#36825;&#20010;&#65292;&#20351;&#24471;&#35757;&#32451;&#25928;&#26524;&#25552;&#21319;&#19981;&#23569;&#12290;</li>
<li>&#36895;&#24230;&#19978;&#30340;&#25552;&#21319;&#65292;&#22240;&#20026;&#26377;&#20102; Fast-RCNN&#65292;&#36825;&#31181;&#22522;&#20110; CNN</li>
</ul>
<p>&#30340; real-time &#30340;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#30475;&#21040;&#20102;&#24076;&#26395;&#65292;&#22312;&#24037;&#31243;&#19978;&#30340;&#23454;&#36341;&#20063;&#26377;&#20102;&#21487;&#33021;&#65292;&#21518;&#32493;&#20063;&#20986;&#29616;&#20102;&#35832;&#22914; Faster-RCNN/YOLO &#31561;&#30456;&#20851;&#24037;&#20316;&#12290;</p>
<p>&#36825;&#20010;&#39046;&#22495;&#30340;&#33033;&#32476;&#26159;&#65306;RCNN -&gt; SPPNET -&gt; Fast-RCNN -&gt; Faster-RCNN&#12290;&#20851;&#20110;&#20855;&#20307;&#30340;&#32454;&#33410;&#65292;&#24314;&#35758;&#39064;&#20027;&#36824;&#26159;&#38405;&#35835;&#30456;&#20851;&#25991;&#29486;&#21543;&#12290;</p>
<p>&#36825;&#20351;&#25105;&#30475;&#21040;&#20102;&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#30340;&#24076;&#26395;&#12290;&#36215;&#30721;&#26377;&#36825;&#20040;&#19968;&#37096;&#20998;&#20154;&#65292;&#20182;&#20204;&#19981;&#20165;&#20165;&#26159;&#20026;&#20102;&#20960;&#20010;&#30334;&#20998;&#28857;&#30340;&#25552;&#21319;&#65292;&#32780;&#26159;&#20999;&#23454;&#36367;&#23454;&#22312;&#20570;&#36129;&#29486;&#65292;&#30456;&#20449;&#19981;&#20037;&#36825;&#20010;&#39046;&#22495;&#20250;&#26377;&#26032;&#30340;&#24037;&#20316;&#20986;&#26469;&#12290;&#20197;&#19978;&#32431;&#23646;&#20010;&#20154;&#35266;&#28857;&#65292;&#27426;&#36814;&#25209;&#35780;&#25351;&#27491;&#12290;</p>
<p>&#26159;&#36825;&#26679;&#30340;&#65292;&#22914;&#26524;&#37117;&#29992;&#19968;&#21477;&#35805;&#26469;&#25551;&#36848;</p>
<p>RCNN &#35299;&#20915;&#30340;&#26159;&#65292;&#8220;&#20026;&#20160;&#20040;&#19981;&#29992; CNN &#20570; classification &#21602;&#65311;&#8221;&#65288;&#20294;&#26159;&#36825;&#20010;&#26041;&#27861;&#30456;&#24403;&#20110;&#36807;&#19968;&#36941; network &#20986; bounding box&#65292;&#20877;&#36807;&#21478;&#19968;&#20010;&#20986; label&#65292;&#21407;&#25991;&#20889;&#30340;&#24456;&#19981;&#8220;elegant&#8221;</p>
<p>Fast-RCNN &#35299;&#20915;&#30340;&#26159;&#65292;&#8220;&#20026;&#20160;&#20040;&#19981;&#19968;&#36215;&#36755;&#20986; bounding box &#21644; label &#21602;&#65311;&#8221;&#65288;&#20294;&#26159;&#36825;&#20010;&#26102;&#20505;&#29992; selective search generate regional proposal &#30340;&#26102;&#38388;&#23454;&#22312;&#22826;&#38271;&#20102;</p>
<p>Faster-RCNN &#35299;&#20915;&#30340;&#26159;&#65292;&#8220;&#20026;&#20160;&#20040;&#36824;&#35201;&#29992; selective search &#21602;&#65311;&#8221;</p>
<p>&#20110;&#26159;&#23601;&#36798;&#21040;&#20102; real-time&#12290;&#24320;&#23665;&#20043;&#20316;&#30830;&#23454;&#26159;&#24320;&#23665;&#20043;&#20316;&#65292;&#20294;&#26159;&#20063;&#26159;&#39034;&#24212;&#20102; &#8220;Deep learning &#25630;&#19968;&#20999; vision&#8221;&#36825;&#19968;&#28526;&#27969;&#21543;&#12290;</p>
<p>refs and see also</p>
<ul>
<li><a href="http://www.cs.berkeley.edu/~rbg/index.html">rbg&#8217;s home page</a></li>
<li><a href="https://github.com/rbgirshick">rbgirshick (Ross Girshick)</a></li>
</ul>
</dd>
<dt><a href="https://www.zhihu.com/question/22553761">&#22914;&#20309;&#31616;&#21333;&#24418;&#35937;&#21448;&#26377;&#36259;&#22320;&#35762;&#35299;&#31070;&#32463;&#32593;&#32476;&#26159;&#20160;&#20040;&#65311; - &#30693;&#20046;</a> <code class="fold">@</code></dt>
<dd><p>2012 &#24180;&#22810;&#20262;&#22810;&#22823;&#23398;&#30340; Krizhevsky &#31561;&#20154;&#26500;&#36896;&#20102;&#19968;&#20010;&#36229;&#22823;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; &#65292;&#26377; 9 &#23618;&#65292;&#20849; 65 &#19975;&#20010;&#31070;&#32463;&#20803;&#65292;6 &#21315;&#19975;&#20010;&#21442;&#25968;&#12290;&#32593;&#32476;&#30340;&#36755;&#20837;&#26159;&#22270;&#29255;&#65292;&#36755;&#20986;&#26159; 1000 &#20010;&#31867;&#65292;&#27604;&#22914;&#23567;&#34411;&#12289;&#32654;&#27954;&#35961;&#12289;&#25937;&#29983;&#33337;&#31561;&#31561;&#12290;&#36825;&#20010;&#27169;&#22411;&#30340;&#35757;&#32451;&#38656;&#35201;&#28023;&#37327;&#22270;&#29255;&#65292;&#23427;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#20063;&#23436;&#29190;&#20808;&#21069;&#25152;&#26377;&#20998;&#31867;&#22120;&#12290;&#32445;&#32422;&#22823;&#23398;&#30340; Zeiler &#21644; Fergusi &#25226;&#36825;&#20010;&#32593;&#32476;&#20013;&#26576;&#20123;&#31070;&#32463;&#20803;&#25361;&#20986;&#26469;&#65292;&#25226;&#22312;&#20854;&#19978;&#21709;&#24212;&#29305;&#21035;&#22823;&#30340;&#37027;&#20123;&#36755;&#20837;&#22270;&#20687;&#25918;&#22312;&#19968;&#36215;&#65292;&#30475;&#23427;&#20204;&#26377;&#20160;&#20040;&#20849;&#21516;&#28857;&#12290;&#20182;&#20204;&#21457;&#29616;&#20013;&#38388;&#23618;&#30340;&#31070;&#32463;&#20803;&#21709;&#24212;&#20102;&#26576;&#20123;&#21313;&#20998;&#25277;&#35937;&#30340;&#29305;&#24449;&#12290;</p>
<ul>
<li>&#31532;&#19968;&#23618;&#31070;&#32463;&#20803;&#20027;&#35201;&#36127;&#36131;&#35782;&#21035;&#39068;&#33394;&#21644;&#31616;&#21333;&#32441;&#29702;</li>
<li>&#31532;&#20108;&#23618;&#30340;&#19968;&#20123;&#31070;&#32463;&#20803;&#21487;&#20197;&#35782;&#21035;&#26356;&#21152;&#32454;&#21270;&#30340;&#32441;&#29702;&#65292;&#27604;&#22914;&#24067;&#32441;&#12289;&#21051;&#24230;&#12289;&#21494;&#32441;&#12290;</li>
<li>&#31532;&#19977;&#23618;&#30340;&#19968;&#20123;&#31070;&#32463;&#20803;&#36127;&#36131;&#24863;&#21463;&#40657;&#22812;&#37324;&#30340;&#40644;&#33394;&#28891;&#20809;&#12289;&#40481;&#34507;&#40644;&#12289;&#39640;&#20809;&#12290;</li>
<li>&#31532;&#22235;&#23618;&#30340;&#19968;&#20123;&#31070;&#32463;&#20803;&#36127;&#36131;&#35782;&#21035;&#33804;&#29399;&#30340;&#33080;&#12289;&#19971;&#26143;&#29922;&#34411;&#21644;&#19968;&#22534;&#22278;&#24418;&#29289;&#20307;&#30340;&#23384;&#22312;&#12290;</li>
<li>&#31532;&#20116;&#23618;&#30340;&#19968;&#20123;&#31070;&#32463;&#20803;&#21487;&#20197;&#35782;&#21035;&#20986;&#33457;&#12289;&#22278;&#24418;&#23627;&#39030;&#12289;&#38190;&#30424;&#12289;&#40479;&#12289;&#40657;&#30524;&#22280;&#21160;&#29289;&#12290;</li>
</ul>
</dd>
<dt><a href="https://www.zhihu.com/question/30174067">&#22914;&#20309;&#21521;&#38750;&#29289;&#29702;&#19987;&#19994;&#30340;&#21516;&#23398;&#35299;&#37322;&#37325;&#25972;&#21270;&#32676;&#65311; - &#30693;&#20046;</a> <code class="fold">@</code></dt>
<dd><p>&#24590;&#20040;&#21150;&#21602;&#65311;&#20320;&#24819;&#20102;&#24819;&#65292;&#35273;&#24471;&#38081;&#29699;&#36825;&#20040;&#22823;&#65292;&#20320;&#19981;&#29992;&#25226;&#27169;&#25311;&#25630;&#24471;&#36825;&#20040;&#31934;&#32454;&#20063;&#33021;&#24471;&#21040;&#27491;&#30830;&#31572;&#26696;&#12290;&#25152;&#20197;&#20320;&#20915;&#23450;&#25226;&#27169;&#25311;&#29992;&#30340;&#27700;&#20998;&#23376;&#20307;&#31215;&#21152; 10 &#20493;&#65292;&#36825;&#26679;&#23601;&#21482;&#35201;&#27169;&#25311; 10^25 &#20010;&#20998;&#23376;&#20102;&#12290;&#20294;&#26159;&#20809;&#36825;&#26679;&#25630;&#19981;&#34892;&#65292;&#24471;&#20986;&#30340;&#32467;&#26524;&#32943;&#23450;&#19981;&#23545;&#65292;&#22240;&#20026;&#26377;&#20123;&#32435;&#31859;&#32423;&#30340;&#23567;&#36816;&#21160;&#36896;&#25104;&#30340;&#23439;&#35266;&#25928;&#26524;&#27809;&#20102;&#12290;&#36825;&#26102;&#20320;&#26377;&#19968;&#20010;&#23398;&#29983;&#35828;&#65292;&#32769;&#26495;&#65292;&#20854;&#23454;&#21681;&#21487;&#20197;&#35797;&#30528;&#25913;&#25913;&#21478;&#22806;&#37027; 4000 &#20010;&#21442;&#25968;&#65292;&#35828;&#19981;&#23450;&#33021;&#25226;&#22833;&#21435;&#30340;&#19996;&#35199;&#32473;&#34917;&#20607;&#22238;&#26469;&#12290;&#20320;&#35273;&#24471;&#38752;&#35889;&#65292;&#24320;&#21160;&#32874;&#26126;&#30340;&#22823;&#33041;&#24819;&#20102;&#24819;&#65292;&#24515;&#31639;&#20986;&#20102;&#27599;&#20010;&#21442;&#25968;&#38656;&#35201;&#30340;&#25913;&#21464;&#12290;&#20110;&#26159;&#20320;&#29992;&#26356;&#22823;&#30340;&#20998;&#23376;&#21644;&#26032;&#30340;&#21442;&#25968;&#37325;&#26032;&#35745;&#31639;&#65292;&#31934;&#30830;&#30340;&#20877;&#29616;&#20102;&#20043;&#21069;&#24471;&#21040;&#30340;&#25968;&#25454;&#12290;&#65288;&#27880;&#24847;&#65292;&#36825;&#26102;&#20320;&#24050;&#32463;&#23545;&#20320;&#30340;&#31995;&#32479;&#36827;&#34892;&#20102;&#19968;&#27425; renormalization&#65289;</p>
<p>&#22312;&#20320;&#30340; nature &#25991;&#31456;&#37324;&#65292;&#25226;&#20026;&#20102;&#31616;&#21270;&#35745;&#31639;&#21457;&#26126;&#30340;&#36825;&#20010;&#26041;&#27861;&#21483; Renormalization group (RG)&#12290;&#25226;&#27599;&#27425;&#27169;&#25311;&#26102;&#27700;&#20998;&#23376;&#30340;&#22823;&#23567;&#21483;&#20570; RG scale, &#28982;&#21518;&#20320;&#25226;&#27599;&#27425;&#29992;&#30340;&#21442;&#25968;&#25353;&#29031;&#27700;&#20998;&#23376;&#30340;&#22823;&#23567;&#21015;&#20102;&#20010;&#34920;&#65292;&#25226;&#23427;&#20204;&#22312;&#23610;&#23544;&#22686;&#21152;&#26102;&#30340;&#21464;&#21270;&#65292;&#21483;&#20570;&#21442;&#25968;&#30340; RG running&#12290;&#20320;&#39044;&#35265;&#21040;&#22330;&#35770;&#37324;&#30340;&#24212;&#29992;&#65292;&#25226;&#29992;&#36825;&#31181;&#26041;&#27861;&#24471;&#21040;&#30340;&#36825;&#20010;&#26032;&#27169;&#22411;&#65292;&#21483;&#20570; low energy effective field theory &#65288;EFT&#65289;.</p>
<p>&#26368;&#21518;&#65292;&#20320;&#26377;&#28857;&#24778;&#35766;&#30340;&#21457;&#29616;&#65292;&#24403;&#20320;&#19968;&#27493;&#27493;&#22686;&#22823;&#27700;&#20998;&#23376;&#23610;&#23544;&#26102;&#65292;&#26412;&#26469;&#37117;&#24456;&#20851;&#38190;&#30340; 4000 &#20010;&#21442;&#25968;&#65292;&#26377;&#20123;&#24178;&#33030;&#21464;&#25104; 0 &#20102;&#65292;&#26377;&#20123;&#21442;&#25968;&#21644;&#20854;&#23427;&#30340;&#21442;&#25968;&#25104;&#27491;&#27604;&#20102;&#12290;&#24635;&#20043;&#21040;&#26368;&#21518;&#65292;&#20320;&#21482;&#29992;&#20102;&#22823;&#27010;10 &#20010;&#33258;&#30001;&#21442;&#25968;&#23601;&#23436;&#32654;&#30340;&#25551;&#36848;&#20102;&#36825;&#19968;&#26479;&#27700;&#12290;&#20320;&#25226;&#37027;&#20123;&#26368;&#21518;&#27809;&#29992;&#30340;&#21442;&#25968;&#21483; irrelevant parameters&#65292;&#25226;&#23427;&#20204;&#25551;&#36848;&#30340;&#24418;&#29366; / &#20316;&#29992;&#21147;&#21483; irrelevant operator. &#20320;&#25226;&#36825;&#20123;irrelevant parameter/operator &#37117;&#21435;&#25481;&#65292;&#24471;&#21040;&#30340;&#37027;&#20010;&#31934;&#31616;&#30340;&#29702;&#35770;&#27169;&#22411;&#23601;&#21483;&#20570; renormalizable theory&#12290;&#23427;&#21644;&#20320;&#20043;&#21069;&#24471;&#21040;&#30340; EFT &#20960;&#20046;&#26159;&#19968;&#26679;&#30340;&#12290;</p>
<p>&#21520;&#20010;&#27133;&#65292;&#8220;&#37325;&#25972;&#21270;&#32676;&#8221;&#30495;&#26159;&#29289;&#29702;&#21517;&#35789;&#30028;&#30340;&#19968;&#26421;&#22855;&#33897;&#65292;&#25226;&#19968;&#20010;&#26412;&#26469;&#24179;&#26131;&#36817;&#20154;&#30340;&#35789;&#32763;&#35793;&#30340;&#19981;&#26126;&#35273;&#21385;&#12290;&#36825;&#20010;&#35789;&#33521;&#25991;&#26159; renormalization group&#65288;RG&#65289;. Normalize &#22823;&#23478;&#37117;&#35748;&#24471;&#65292;&#22522;&#26412;&#24847;&#24605;&#26159;&#32473;&#19968;&#20010;&#21464;&#37327;&#20056;&#20010;&#24120;&#25968;&#65292;&#35753;&#23427;&#26356;&#31526;&#21512;&#19968;&#20123;&#31616;&#21333;&#35201;&#27714;&#12290;&#27604;&#22914;&#20960;&#20309;&#37324;&#35828; normalized vector, &#23601;&#26159;&#35828;&#25913;&#21464;&#20102;&#19968;&#20010;&#30690;&#37327;&#30340;&#23450;&#20041;&#65292;&#35753;&#23427;&#30340;&#38271;&#24230;&#31561;&#20110;&#19968;. re-normalize &#23601;&#26159;&#19981;&#26029;&#30340; normalize. group &#36825;&#37324;&#26159;&#27867;&#25351;&#21464;&#25442;&#65292;&#19981;&#25351;&#25968;&#23398;&#19978;&#20005;&#26684;&#30340;&#32676;&#12290;renormalization group &#30340;&#23383;&#38754;&#24847;&#24605;&#23601;&#26159;&#8220;&#19981;&#26029;&#37325;&#26032;&#23450;&#20041;&#21442;&#25968;&#30340;&#19968;&#32452;&#21464;&#25442;&#8221;&#12290;</p>
<p><strong>&#37325;&#25972;&#21270;&#32676;&#26377;&#25928;&#26412;&#36136;&#21407;&#22240;&#26159;&#19981;&#21516;&#23610;&#24230;&#30340;&#36807;&#31243;&#20043;&#38388;&#24448;&#24448;&#26377;&#19968;&#31181;&#30456;&#23545;&#30340;&#29420;&#31435;&#24615;&#12290;</strong>&#65288;&#24212;&#35813;&#35828;&#36825;&#31181;&#38382;&#39064;&#28304;&#33258;&#19981;&#21516;&#23610;&#24230;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#65292;&#37325;&#25972;&#21270;&#32676;&#26469;&#33258;&#23427;&#20204;&#20043;&#38388;&#30340;&#20849;&#36890;&#24615;&#65289;&#22914;&#26524;&#20320;&#30340;&#31995;&#32479;&#26159;&#36825;&#26679;&#30340;&#65292;&#37027;&#37325;&#25972;&#21270;&#32676;&#30340;&#26041;&#27861;&#20250;&#32473;&#20320;&#26377;&#29992;&#30340;&#32467;&#26524;&#12290;</p>
<p>refs and see also</p>
<ul>
<li><a href="https://www.zhihu.com/question/29854624">&#22914;&#20309;&#29702;&#35299;&#12300;&#28145;&#24230;&#23398;&#20064;&#21644;&#37325;&#25972;&#21270;&#32676;&#21487;&#20197;&#24314;&#31435;&#20005;&#26684;&#26144;&#23556;&#12301;&#65311;&#36825;&#19968;&#32467;&#35770;&#23545;&#39046;&#22495;&#26377;&#20309;&#24433;&#21709;&#65311; - &#30693;&#20046;</a></li>
</ul>
</dd>
<dt><a href="https://www.zhihu.com/question/31430100">&#22914;&#20309;&#30475;&#24453; 2014 &#24180;&#20197;&#26469;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;Computer Vision&#65289;&#30028;&#21019;&#19994;&#28526;&#65311; - &#30693;&#20046;</a> <code class="fold">@</code></dt>
<dd><p>&#39318;&#20808;&#65292;&#27595;&#24248;&#32622;&#30097;&#65292;computer vision &#20316;&#20026;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#27491;&#22788;&#22312;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#21490;&#19978;&#21457;&#23637;&#36895;&#24230;&#26368;&#24778;&#20154;&#30340;&#38454;&#27573;. &#20174; research &#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#26159; vision &#26368;&#22909;&#30340;&#19968;&#20010;&#26102;&#20195;&#65292;&#20063;&#26159;&#26368;&#22351;&#30340;&#19968;&#20010;&#26102;&#20195;&#12290;</p>
<p>&#21033;&#30410;&#30456;&#20851;&#65306;&#25105;&#22312; Cogtu&#65292;&#22899;&#26379;&#21451;&#22312; Linkface</p>
</dd>
<dt><a href="https://www.zhihu.com/question/34681168">CNN(&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;)&#12289;RNN(&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;)&#12289;DNN(&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;) &#30340;&#20869;&#37096;&#32593;&#32476;&#32467;&#26500;&#26377;&#20160;&#20040;&#21306;&#21035;&#65311; - &#30693;&#20046;</a> <code class="fold">@</code></dt>
<dd><p>&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#36215;&#28304;&#20110;&#19978;&#19990;&#32426;&#20116;&#12289;&#20845;&#21313;&#24180;&#20195;&#65292;&#24403;&#26102;&#21483;&#24863;&#30693;&#26426;&#65288;perceptron&#65289;&#65292;&#25317;&#26377;&#36755;&#20837;&#23618;&#12289;&#36755;&#20986;&#23618;&#21644;&#19968;&#20010;&#38544;&#21547;&#23618;&#12290;&#36755;&#20837;&#30340;&#29305;&#24449;&#21521;&#37327;&#36890;&#36807;&#38544;&#21547;&#23618;&#21464;&#25442;&#36798;&#21040;&#36755;&#20986;&#23618;&#65292;&#22312;&#36755;&#20986;&#23618;&#24471;&#21040;&#20998;&#31867;&#32467;&#26524;&#12290;&#26089;&#26399;&#24863;&#30693;&#26426;&#30340;&#25512;&#21160;&#32773;&#26159; Rosenblatt&#12290;</p>
<p>&#38543;&#30528;&#25968;&#23398;&#30340;&#21457;&#23637;&#65292;&#36825;&#20010;&#32570;&#28857;&#30452;&#21040;&#19978;&#19990;&#32426;&#20843;&#21313;&#24180;&#20195;&#25165;&#34987; Rumelhart&#12289;Williams&#12289;Hinton&#12289; LeCun &#31561;&#20154;&#65288;&#21453;&#27491;&#23601;&#26159;&#19968;&#31080;&#22823;&#29275;&#65289;&#21457;&#26126;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;multilayer perceptron&#65289;&#20811;&#26381;&#12290;</p>
<p>&#22810;&#23618;&#24863;&#30693;&#26426;&#21487;&#20197;&#25670;&#33073;&#26089;&#26399;&#31163;&#25955;&#20256;&#36755;&#20989;&#25968;&#30340;&#26463;&#32538;&#65292;&#20351;&#29992; sigmoid &#25110; tanh &#31561;&#36830;&#32493;&#20989;&#25968;&#27169;&#25311;&#31070;&#32463;&#20803;&#23545;&#28608;&#21169;&#30340;&#21709;&#24212;&#65292;&#22312;&#35757;&#32451;&#31639;&#27861;&#19978;&#21017;&#20351;&#29992; Werbos &#21457;&#26126;&#30340;&#21453;&#21521;&#20256;&#25773; BP &#31639;&#27861;&#12290;&#23545;&#65292;&#36825;&#36135;&#23601;&#26159;&#25105;&#20204;&#29616;&#22312;&#25152;&#35828;&#30340;&#31070;&#32463;&#32593;&#32476; NN&#8212;&#8212;&#31070;&#32463;&#32593;&#32476;&#21548;&#36215;&#26469;&#19981;&#30693;&#36947;&#27604;&#24863;&#30693;&#26426;&#39640;&#31471;&#21040;&#21738;&#37324;&#21435;&#20102;&#65281;&#36825;&#20877;&#27425;&#21578;&#35785;&#25105;&#20204;&#36215;&#19968;&#20010;&#22909;&#21548;&#30340;&#21517;&#23383;&#23545;&#20110;&#30740;&#65288;zhuang&#65289;&#31350;&#65288;bi&#65289;&#24456;&#37325;&#35201;&#65281;</p>
<p>&#22810;&#23618;&#24863;&#30693;&#26426;&#32473;&#25105;&#20204;&#24102;&#26469;&#30340;&#21551;&#31034;&#26159;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#25968;&#30452;&#25509;&#20915;&#23450;&#20102;&#23427;&#23545;&#29616;&#23454;&#30340;&#21051;&#30011;&#33021;&#21147;&#8212;&#8212; &#21033;&#29992;&#27599;&#23618;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#25311;&#21512;&#26356;&#21152;&#22797;&#26434;&#30340;&#20989;&#25968;&#12290;&#65288;Bengio &#22914;&#26159;&#35828;&#65306;functions that can be compactly represented by a depth k architecture might require an exponential number of computational elements to be represented by a depth k &#8722; 1 architecture.&#65289;</p>
<p>&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#23618;&#25968;&#30340;&#21152;&#28145;&#65292;&#20248;&#21270;&#20989;&#25968;&#36234;&#26469;&#36234;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;</p>
<p>&#8220;&#26799;&#24230;&#28040;&#22833;&#8221;&#29616;&#35937;&#26356;&#21152;&#20005;&#37325;&#12290;</p>
<p>&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24120;&#24120;&#20351;&#29992;sigmoid&#20316;&#20026;&#31070;&#32463;&#20803;&#30340;&#36755;&#20837;&#36755;&#20986;&#20989;&#25968;&#12290;&#23545;&#20110;&#24133;&#24230;&#20026;1&#30340;&#20449;&#21495;&#65292;&#22312;BP&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#26102;&#65292;&#27599;&#20256;&#36882;&#19968;&#23618;&#65292;&#26799;&#24230;&#34928;&#20943;&#20026;&#21407;&#26469;&#30340;0.25&#12290;&#23618;&#25968;&#19968;&#22810;&#65292;&#26799;&#24230;&#25351;&#25968;&#34928;&#20943;&#21518;&#20302;&#23618;&#22522;&#26412;&#19978;&#25509;&#21463;&#19981;&#21040;&#26377;&#25928;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;</p>
<p>2006 &#24180;&#65292;Hinton &#21033;&#29992;&#39044;&#35757;&#32451;&#26041;&#27861;&#32531;&#35299;&#20102;&#23616;&#37096;&#26368;&#20248;&#35299;&#38382;&#39064;&#65292;&#23558;&#38544;&#21547;&#23618;&#25512;&#21160;&#21040;&#20102; 7 &#23618;&#65292;&#31070;&#32463;&#32593;&#32476;&#30495;&#27491;&#24847;&#20041;&#19978;&#26377;&#20102;&#8220;&#28145;&#24230;&#8221;&#65292;&#30001;&#27492;&#25581;&#24320;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#28909;&#28526;&#12290;&#36825;&#37324;&#30340;&#8220;&#28145;&#24230;&#8221;&#24182;&#27809;&#26377;&#22266;&#23450;&#30340;&#23450;&#20041;&#8212;&#8212;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013; 4 &#23618;&#32593;&#32476;&#23601;&#33021;&#22815;&#34987;&#35748;&#20026;&#26159;&#8220;&#36739;&#28145;&#30340;&#8221;&#65292;&#32780;&#22312;&#22270;&#20687;&#35782;&#21035;&#20013; 20 &#23618;&#20197;&#19978;&#30340;&#32593;&#32476;&#23649;&#35265;&#19981;&#40092;&#12290;&#20026;&#20102;&#20811;&#26381;&#26799;&#24230;&#28040;&#22833;&#65292;ReLU&#12289;maxout &#31561;&#20256;&#36755;&#20989;&#25968;&#20195;&#26367;&#20102; sigmoid&#65292;&#24418;&#25104;&#20102;&#22914;&#20170; DNN &#30340;&#22522;&#26412;&#24418;&#24335;&#12290;&#21333;&#20174;&#32467;&#26500;&#19978;&#26469;&#35828;&#65292;&#20840;&#36830;&#25509;&#30340; DNN &#21644;&#22270; 1 &#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#26159;&#27809;&#26377;&#20219;&#20309;&#21306;&#21035;&#30340;.</p>
<ul>
<li>&#39640;&#36895;&#20844;&#36335;&#32593;&#32476;&#65288;highway network&#65289;&#21644;</li>
<li>&#28145;&#24230;&#27531;&#24046;&#23398;&#20064;&#65288;deep residual learning&#65289;</li>
</ul>
<p>&#20840;&#36830;&#25509; DNN &#30340;&#32467;&#26500;&#37324;&#19979;&#23618;&#31070;&#32463;&#20803;&#21644;&#25152;&#26377;&#19978;&#23618;&#31070;&#32463;&#20803;&#37117;&#33021;&#22815;&#24418;&#25104;&#36830;&#25509;&#65292;&#24102;&#26469;&#30340;&#28508;&#22312;&#38382;&#39064;&#26159;&#21442;&#25968;&#25968;&#37327;&#30340;&#33192;&#32960;&#12290;</p>
<p>&#20107;&#23454;&#19978;&#65292;&#19981;&#35770;&#26159;&#37027;&#31181;&#32593;&#32476;&#65292;&#20182;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#24120;&#37117;&#28151;&#21512;&#30528;&#20351;&#29992;&#65292;&#27604;&#22914; CNN &#21644; RNN &#22312;&#19978;&#23618;&#36755;&#20986;&#20043;&#21069;&#24448;&#24448;&#20250;&#25509;&#19978;&#20840;&#36830;&#25509;&#23618;&#65292;&#24456;&#38590;&#35828;&#26576;&#20010;&#32593;&#32476;&#21040;&#24213;&#23646;&#20110;&#21738;&#20010;&#31867;&#21035;&#12290;&#19981;&#38590;&#24819;&#35937;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#28909;&#24230;&#30340;&#24310;&#32493;&#65292;&#26356;&#28789;&#27963;&#30340;&#32452;&#21512;&#26041;&#24335;&#12289;&#26356;&#22810;&#30340;&#32593;&#32476;&#32467;&#26500;&#23558;&#34987;&#21457;&#23637;&#20986;&#26469;&#12290;&#23613;&#31649;&#30475;&#36215;&#26469;&#21315;&#21464;&#19975;&#21270;&#65292;&#20294;&#30740;&#31350;&#32773;&#20204;&#30340;&#20986;&#21457;&#28857;&#32943;&#23450;&#37117;&#26159;&#20026;&#20102;&#35299;&#20915;&#29305;&#23450;&#30340;&#38382;&#39064;&#12290;&#39064;&#20027;&#22914;&#26524;&#24819;&#36827;&#34892;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#19981;&#22952;&#20180;&#32454;&#20998;&#26512;&#19968;&#19979;&#36825;&#20123;&#32467;&#26500;&#21508;&#33258;&#30340;&#29305;&#28857;&#20197;&#21450;&#23427;&#20204;&#36798;&#25104;&#30446;&#26631;&#30340;&#25163;&#27573;&#12290;</p>
<p>&#20837;&#38376;&#30340;&#35805;&#21487;&#20197;&#21442;&#32771;&#65306;</p>
<ul>
<li>Ng &#20889;&#30340; Ufldl&#65306;UFLDL &#25945;&#31243; - Ufldl, &#20063;&#21487;&#20197;&#30475;</li>
<li>Theano &#20869;&#33258;&#24102;&#30340;&#25945;&#31243;&#65292;&#20363;&#23376;&#38750;&#24120;&#20855;&#20307;&#65306;Deep Learning Tutorials</li>
</ul>
<dl>
<dt>1&#12289;<a href="http://deeplearning.stanford.edu/tutorial/">Unsupervised Feature Learning and Deep Learning Tutorial</a></dt>
<dd><p>&#20027;&#39029;&#65306;<a href="http://ufldl.stanford.edu/">Deep Learning</a></p>
<ul>
<li><a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B">UFLDL&#25945;&#31243; - Ufldl</a></li>
<li><a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial">UFLDL Tutorial - Ufldl</a></li>
</ul>
<p>&#36825;&#26159;&#25105;&#26368;&#24320;&#22987;&#25509;&#35302;&#31070;&#32463;&#32593;&#32476;&#26102;&#30475;&#30340;&#36164;&#26009;&#65292;&#25226;&#36825;&#20010;&#20180;&#32454;&#30740;&#31350;&#23436;&#20250;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#35757;&#32451;&#65288;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65289;&#26377;&#19968;&#20010;&#22522;&#26412;&#30340;&#35748;&#35782;&#65292;&#31639;&#26159;&#19968;&#20010;&#22522;&#26412;&#21151;&#12290;</p>
<p>
<i class="icon-camera-retro icon-large"></i> icon-camera-retro
</p>
</dd>
<dt>2&#12289;<a href="http://deeplearning.net/tutorial/">Deep Learning Tutorials &#8212; DeepLearning 0.1 documentation</a></dt>
<dd><p>&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#21253;&#65292;&#37324;&#38754;&#26377;&#24456;&#22810;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340; python &#20195;&#30721;&#36824;&#26377;&#19968;&#20123;&#23545;&#27169;&#22411;&#20197;&#21450;&#20195;&#30721;&#32454;&#33410;&#30340;&#35299;&#37322;&#12290;&#25105;&#35273;&#24471;&#23398;&#20064;&#28145;&#24230;&#23398;&#20064;&#20809;&#20102;&#35299;&#27169;&#22411;&#26159;&#19981;&#38590;&#30340;&#65292;&#38590;&#28857;&#22312;&#20110;&#25226;&#27169;&#22411;&#33853;&#22320;&#20889;&#25104;&#20195;&#30721;&#65292;&#22240;&#20026;&#37324;&#38754;&#20250;&#26377;&#24456;&#22810;&#32454;&#33410;&#21482;&#26377;&#21160;&#25163;&#20889;&#20102;&#20195;&#30721;&#25165;&#20250;&#20102;&#35299;&#12290;&#20294; Theano &#20063;&#26377;&#32570;&#28857;&#65292;&#23601;&#26159;&#26497;&#20854;&#38590;&#20197;&#35843;&#35797;&#65292;&#20197;&#33267;&#20110;&#25105;&#21518;&#26469;&#23601;&#31639;&#33258;&#24049;&#21160;&#25163;&#20889;&#20960;&#30334;&#34892;&#30340;&#20195;&#30721;&#20063;&#19981;&#24895;&#24847;&#20877;&#29992;&#23427;&#30340;&#24037;&#20855;&#21253;&#12290;&#25152;&#20197;&#25105;&#35273;&#24471; Theano &#30340;&#27491;&#30830;&#29992;&#27861;&#36824;&#26159;&#22312;&#20110;&#30475;&#37324;&#38754;&#35299;&#37322;&#30340;&#25991;&#23383;&#65292;&#19981;&#35201;&#23475;&#24597;&#33521;&#25991;&#65292;&#36825;&#26159;&#24517;&#32463;&#20043;&#36335;&#12290;PS&#65306;&#25512;&#33616;&#20351;&#29992; python &#35821;&#35328;&#65292;&#30446;&#21069;&#26469;&#30475;&#27604;&#36739;&#20027;&#27969;&#12290;&#65288;&#26356;&#26032;&#65306;&#33258;&#24049;&#20889;&#22353;&#23454;&#22312;&#22826;&#22810;&#20102;&#65292;CUDA &#20063;&#19981;&#30693;&#36947;&#24590;&#20040;&#29992;&#65292;&#36824;&#26159;&#20054;&#20054;&#29992; theano &#21543;&#8230;&#65289;</p>
</dd>
<dt>3&#12289;<a href="http://vision.stanford.edu/teaching/cs231n/">Stanford University CS231n: Convolutional Neural Networks for Visual Recognition</a></dt>
<dd><p>&#26031;&#22374;&#31119;&#30340;&#19968;&#38376;&#35838;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#26446;&#39134;&#39134;&#25945;&#25480;&#20027;&#35762;&#12290;&#36825;&#38376;&#35838;&#20250;&#31995;&#32479;&#30340;&#35762;&#19968;&#19979;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#36824;&#26377;&#19968;&#20123;&#35838;&#21518;&#20064;&#39064;&#65292;&#39064;&#30446;&#24456;&#26377;&#20195;&#34920;&#24615;&#65292;&#20063;&#26159;&#29992;python&#20889;&#30340;&#65292;&#26159;&#22312;&#19968;&#20221;&#20195;&#30721;&#20013;&#22635;&#20889;&#19968;&#37096;&#20998;&#32570;&#22833;&#30340;&#20195;&#30721;&#12290;&#22914;&#26524;&#25226;&#36825;&#20010;&#23436;&#25972;&#23398;&#23436;&#65292;&#30456;&#20449;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23601;&#19981;&#26159;&#19968;&#20010;&#22823;&#38382;&#39064;&#20102;&#12290;</p>
</dd>
<dt>4&#12289;<a href="http://open.163.com/special/opencourse/machinelearning.html">&#26031;&#22374;&#31119;&#22823;&#23398;&#20844;&#24320;&#35838; &#65306;&#26426;&#22120;&#23398;&#20064;&#35838;&#31243;_&#20840;20&#38598;_&#32593;&#26131;&#20844;&#24320;&#35838;</a></dt>
<dd><p>&#36825;&#21487;&#33021;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26368;&#32463;&#20856;&#26368;&#30693;&#21517;&#30340;&#20844;&#24320;&#35838;&#20102;&#65292;&#30001;&#22823;&#29275;Andrew Ng&#20027;&#35762;&#65292;&#36825;&#20010;&#23601;&#19981;&#20165;&#20165;&#26159;&#28145;&#24230;&#23398;&#20064;&#20102;&#65292;&#23427;&#26159;&#24102;&#20320;&#39046;&#30053;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#30340;&#27010;&#24565;&#65292;&#28982;&#21518;&#24314;&#31435;&#36215;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#20320;&#23545;&#26426;&#22120;&#23398;&#20064;&#36825;&#20010;&#23398;&#31185;&#26377;&#19968;&#20010;&#36739;&#20026;&#23436;&#25972;&#30340;&#35748;&#35782;&#12290;&#36825;&#20010;&#25105;&#35273;&#24471;&#25152;&#26377;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#30340;&#20154;&#37117;&#24212;&#35813;&#30475;&#19968;&#19979;&#65292;&#25105;&#29978;&#33267;&#22312;&#26576;&#20844;&#21496;&#30340;&#25307;&#32856;&#35201;&#27714;&#19978;&#30475;&#21040;&#36807;&#65306;&#35748;&#30495;&#30475;&#36807;&#24182;&#28145;&#20837;&#30740;&#31350;&#36807;Andrew Ng&#30340;&#26426;&#22120;&#23398;&#20064;&#35838;&#31243;&#65292;&#30001;&#27492;&#21487;&#35265;&#20854;&#37325;&#35201;&#24615;&#12290;</p>
</dd>
</dl>
</dd>
<dt><a href="https://www.zhihu.com/question/29411132">&#26377;&#21738;&#20123; LSTM(Long Short Term Memory) &#21644; RNN(Recurrent) &#32593;&#32476;&#30340;&#25945;&#31243;&#65311; - &#30693;&#20046;</a> <code class="fold">@</code></dt>
<dd><p>&#20808;&#32473;&#20986;&#19968;&#20010;&#26368;&#24555;&#30340;&#20102;&#35299;+&#19978;&#25163;&#30340;&#25945;&#31243;&#65306;</p>
<p>&#30452;&#25509;&#30475; theano &#23448;&#32593;&#30340; LSTM &#25945;&#31243; + &#20195;&#30721;&#65306;<a href="http://deeplearning.net/tutorial/lstm.html">LSTM Networks for Sentiment Analysis &#8212; DeepLearning 0.1 documentation</a> &#20294;&#26159;&#65292;&#21069;&#25552;&#26159;&#20320;&#26377; RNN &#30340;&#22522;&#30784;&#65292;&#22240;&#20026; LSTM &#26412;&#36523;&#19981;&#26159;&#19968;&#20010;&#23436;&#25972;&#30340;&#27169;&#22411;&#65292;LSTM &#26159;&#23545; RNN &#38544;&#21547;&#23618;&#30340;&#25913;&#36827;&#12290;&#19968;&#33324;&#25152;&#31216;&#30340; LSTM &#32593;&#32476;&#20840;&#21483;&#20840;&#20102;&#24212;&#35813;&#26159;&#20351;&#29992; LSTM &#21333;&#20803;&#30340; RNN &#32593;&#32476;&#12290;&#25945;&#31243;&#23601;&#32473;&#20102;&#20010; LSTM &#30340;&#22270;&#65292;&#23427;&#21482;&#26159; RNN &#26694;&#26550;&#20013;&#30340;&#19968;&#37096;&#20998;&#65292;&#22914;&#26524;&#20320;&#19981;&#30693;&#36947; RNN &#20272;&#35745;&#30475;&#19981;&#25026;&#12290;&#27604;&#36739;&#22909;&#30340;&#26159;&#65292;&#20320;&#21482;&#38656;&#35201;&#20102;&#35299;&#21069;&#39304;&#36807;&#31243;&#65292;&#20320;&#37117;&#19981;&#38656;&#35201;&#33258;&#24049;&#27714;&#23548;&#23601;&#33021;&#20889;&#20195;&#30721;&#20351;&#29992;&#20102;&#12290;&#34917;&#20805;&#65292;&#20170;&#22825;&#21018;&#21457;&#29616;&#19968;&#20010;&#20013;&#25991;&#30340;&#21338;&#23458;&#65306;<a href="http://blog.csdn.net/a635661820/article/details/45390671">LSTM &#31616;&#20171;&#20197;&#21450;&#25968;&#23398;&#25512;&#23548; (FULL BPTT) - &#22825;&#36947;&#37228;&#21220;&#65292;&#20570;&#19968;&#20010;&#21153;&#23454;&#30340;&#29702;&#24819;&#20027;&#20041;&#32773; - &#21338;&#23458;&#39057;&#36947; - CSDN.NET</a> &#19981;&#36807;&#65292;&#31245;&#24494;&#28145;&#20837;&#19979;&#21435;&#36824;&#26159;&#24471;&#32769;&#32769;&#23454;&#23454;&#30340;&#22909;&#22909;&#23398;&#65292;&#19979;&#38754;&#26159;&#25105;&#35748;&#20026;&#27604;&#36739;&#22909;&#30340;</p>
<dl>
<dt>&#23436;&#25972;LSTM&#23398;&#20064;&#27969;&#31243;&#65306; <code class="fold">@</code></dt>
<dd><p>&#25105;&#19968;&#30452;&#37117;&#35273;&#24471;&#20102;&#35299;&#19968;&#20010;&#27169;&#22411;&#30340;&#21069;&#19990;&#20170;&#29983;&#23545;&#27169;&#22411;&#29702;&#35299;&#26377;&#24040;&#22823;&#30340;&#24110;&#21161;&#12290;&#21040;LSTM&#36825;&#37324;&#65288;&#20551;&#35774;&#39064;&#20027;&#38646;&#22522;&#30784;&#65289;&#37027;&#27604;&#36739;&#22909;&#30340;&#36335;&#32447;&#26159;MLP-&gt;RNN-&gt;LSTM&#12290;&#36824;&#26377;LSTM&#26412;&#36523;&#30340;&#21457;&#23637;&#36335;&#32447;&#65288;97&#24180;&#26368;&#21407;&#22987;&#30340;LSTM&#21040;forget gate&#21040;peephole&#20877;&#21040;CTC&#65289;&#25353;&#29031;&#36825;&#20010;&#36335;&#32447;&#23398;&#36215;&#26469;&#20250;&#27604;&#36739;&#39034;&#65292;&#25152;&#20197;&#25105;&#20248;&#20808;&#25512;&#33616;&#30340;&#20004;&#20010;&#25945;&#31243;&#37117;&#26159;&#25353;&#29031;&#36825;&#20010;&#36335;&#32447;&#26469;&#30340;&#65306;</p>
<ol style="list-style-type: decimal">
<li><p>&#22810;&#20262;&#22810;&#22823;&#23398;&#30340; Alex Graves &#30340;RNN&#19987;&#33879; &#12298; Supervised Sequence Labelling with Recurrent Neural Networks &#12299;</p></li>
<li><p>Felix Gers&#30340;&#21338;&#22763;&#35770;&#25991; &#12298; Long short-term memory in recurrent neural networks &#12299;</p></li>
</ol>
<p>&#36825;&#20004;&#20010;&#20869;&#23481;&#37117;&#25402;&#22810;&#30340;&#65292;&#19981;&#36807;&#21487;&#20197;&#36339;&#30528;&#30475;&#65292;&#21453;&#27491;&#25105;&#26159;&#27809;&#30475;&#23436;&#9489;(&#65507;&#1044; &#65507;)&#9485;</p>
<p>&#36824;&#26377;&#19968;&#20010;&#26368;&#26032;&#30340;&#65288;&#20170;&#24180;2015&#65289;&#30340;&#32508;&#36848;&#65292; &#12298; A Critical Review of Recurrent Neural Networks for Sequence Learning &#12299; &#19981;&#36807;&#24456;&#22810;&#20869;&#23481;&#37117;&#26469;&#33258;&#20197;&#19978;&#20004;&#20010;&#26448;&#26009;&#12290;</p>
<p>&#20854;&#20182;&#21487;&#20197;&#24403;&#20570;&#25945;&#31243;&#30340;&#26448;&#26009;&#36824;&#26377;&#65306;</p>
<ul>
<li>&#12298; From Recurrent Neural Network to Long Short Term Memory Architecture Application to Handwriting Recognition Author &#12299;</li>
<li>&#12298; Generating Sequences With Recurrent Neural Networks &#12299; &#65288;&#36825;&#20010;&#26377;&#23545;&#24212;&#28304;&#30721;&#65292;&#34429;&#28982;&#23454;&#20363;&#29992;&#27861;&#26159;&#38169;&#30340;&#65292;&#33258;&#24049;&#29992;&#30340;&#26102;&#20505;&#36824;&#24471;&#25913;&#20195;&#30721;&#65292;&#20027;&#35201;&#26159;&#25688;&#20986;&#19968;&#20123;&#26469;&#29992;&#65292;&#20379;&#21442;&#32771;&#65289;&#28982;&#21518;&#21602;&#65292;&#21487;&#20197;&#24320;&#22987;&#32534;&#30721;&#20102;&#12290;&#38500;&#20102;&#21069;&#38754;&#25552;&#21040;&#30340;theano&#25945;&#31243;&#36824;&#26377;&#19968;&#20123;&#35770;&#25991;&#30340;&#24320;&#28304;&#20195;&#30721;&#65292;&#21040;github&#19978;&#25628;&#23601;&#22909;&#20102;&#12290;</li>
</ul>
<p>&#39034;&#20415;&#23433;&#21033;&#19968;&#19979;theano&#65292;theano&#30340;&#33258;&#21160;&#27714;&#23548;&#21644;GPU&#36879;&#26126;&#23545;&#26032;&#25163;&#20197;&#21450;&#23398;&#26415;&#30028;&#30740;&#31350;&#32773;&#26469;&#35828;&#38750;&#24120;&#26041;&#20415;&#65292;LSTM&#25299;&#25169;&#32467;&#26500;&#23545;&#20110;&#27714;&#23548;&#26469;&#35828;&#24456;&#22797;&#26434;&#65292;&#19978;&#26469;&#23601;&#20889;LSTM&#21453;&#21521;&#27714;&#23548;&#36824;&#35201;GPU&#32534;&#31243;&#20195;&#30721;&#38750;&#24120;&#36153;&#26102;&#38388;&#30340;&#65292;&#32780;&#19988;&#25630;&#23398;&#26415;&#19981;&#26159;&#23454;&#29616;&#19968;&#20010;&#29616;&#26377;&#27169;&#22411;&#23436;&#20102;&#65292;&#24471;&#23581;&#35797;&#21019;&#26032;&#65292;&#25913;&#27169;&#22411;&#65292;&#27599;&#25913;&#19968;&#27425;&#23545;&#24212;&#27714;&#23548;&#20195;&#30721;&#30340;&#20462;&#25913;&#37117;&#25402;&#40635;&#28902;&#30340;&#12290;</p>
<p>&#20854;&#23454;&#21040;&#36825;&#24212;&#35813;&#31639;&#26159;&#19968;&#20010;&#38454;&#27573;&#20102;&#65292;&#22914;&#26524;&#20320;&#24819;&#32487;&#32493;&#28145;&#20837;&#21487;&#20197;&#20855;&#20307;&#30475;&#30475;&#20960;&#31687;&#32463;&#20856;&#35770;&#25991;&#65292;&#27604;&#22914; LSTM&#20197;&#21450;&#21508;&#20010;&#25913;&#36827;&#23545;&#24212;&#30340;&#32463;&#20856;&#35770;&#25991;&#12290;</p>
<p>&#36824;&#26377;&#27004;&#19978;&#25552;&#21040;&#30340; &#12298; LSTM: A Search Space Odyssey &#12299; &#36890;&#36807;&#20174;&#26032;&#36827;&#34892;&#21508;&#31181;&#23454;&#39564;&#26469;&#23545;&#27604;&#32771;&#26597;LSTM&#30340;&#21508;&#31181;&#25913;&#36827;&#65288;&#32452;&#20214;&#65289;&#30340;&#25928;&#26524;&#12290;&#25402;&#26377;&#24847;&#20041;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#25351;&#23548;&#22914;&#20309;&#20351;&#29992;LSTM&#26041;&#38754;&#12290;</p>
<p>LSTM&#32593;&#32476;&#26412;&#36136;&#36824;&#26159;RNN&#32593;&#32476;&#65292;&#22522;&#20110;LSTM&#30340;RNN&#26550;&#26500;&#19978;&#30340;&#21464;&#21270;&#26377;&#26368;&#20808;&#30340;BRNN&#65288;&#21452;&#21521;&#65289;&#65292;&#36824;&#26377;&#20170;&#24180;Socher&#20182;&#20204;&#25552;&#20986;&#30340;&#26641;&#29366;LSTM&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#21644;&#21477;&#23376;&#30456;&#20851;&#24230;&#35745;&#31639; &#12298; Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks &#12299;</p>
<p>&#20170;&#24180;ACL&#65288;2015&#65289;&#19978;&#26377;&#19968;&#31687;&#23618;&#27425;&#30340;LSTM &#12298; A Hierarchical Neural Autoencoder for Paragraphs and Documents &#12299; &#12290;&#20351;&#29992;&#19981;&#21516;&#30340;LSTM&#20998;&#21035;&#22788;&#29702;&#35789;&#12289;&#21477;&#23376;&#21644;&#27573;&#33853;&#32423;&#21035;&#36755;&#20837;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;autoencoder&#65289;&#26469;&#26816;&#27979;LSTM&#30340;&#25991;&#26723;&#29305;&#24449;&#25277;&#21462;&#21644;&#37325;&#24314;&#33021;&#21147;&#12290;</p>
<p>&#36824;&#26377;&#19968;&#31687;&#25991;&#31456; &#12298; Chung J, Gulcehre C, Cho K, et al. Gated feedback recurrent neural networks[J]. arXiv preprint arXiv:1502.02367, 2015. &#12299;&#65292;&#25226;gated&#30340;&#24605;&#24819;&#20174;&#35760;&#24518;&#21333;&#20803;&#25193;&#23637;&#21040;&#20102;&#32593;&#32476;&#26550;&#26500;&#19978;&#65292;&#25552;&#20986;&#22810;&#23618;RNN&#21508;&#20010;&#23618;&#30340;&#38544;&#21547;&#23618;&#25968;&#25454;&#21487;&#20197;&#30456;&#20114;&#21033;&#29992;&#65288;&#20043;&#21069;&#30340;&#22810;&#23618;RNN&#22810;&#38544;&#21547;&#23618;&#21482;&#26159;&#21333;&#21521;&#33258;&#24213;&#21521;&#19978;&#36830;&#25509;&#65289;&#65292;&#19981;&#36807;&#38656;&#35201;&#35774;&#32622;&#38376;&#65288;gated&#65289;&#26469;&#35843;&#33410;&#12290;</p>
<p>&#35760;&#24518;&#21333;&#20803;&#26041;&#38754;&#65292;Bahdanau Dzmitry&#20182;&#20204;&#22312;&#26500;&#24314;RNN&#26694;&#26550;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26102;&#20505;&#20351;&#29992;&#20102; GRU&#21333;&#20803;&#65288;gated recurrent unit&#65289;&#26367;&#20195;LSTM&#65292;&#20854;&#23454;LSTM&#21644;GRU&#37117;&#21487;&#20197;&#35828;&#26159;gated hidden unit&#12290;&#20004;&#32773;&#25928;&#26524;&#30456;&#36817;&#65292;&#20294;&#26159;GRU&#30456;&#23545;LSTM&#26469;&#35828;&#21442;&#25968;&#26356;&#23569;&#65292;&#25152;&#20197;&#26356;&#21152;&#19981;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#65288;&#22823;&#23478;&#22534;&#27169;&#22411;&#22534;&#21040;dropout&#20063;&#19981;&#31649;&#29992;&#30340;&#26102;&#20505;&#21487;&#20197;&#35797;&#35797;&#25442;&#19978;GRU&#36825;&#31181;&#21442;&#25968;&#23569;&#30340;</p>
<dl>
<dt>&#22270;&#20687;&#22788;&#29702;&#65288;&#23545;&#65292;&#19981;&#29992;CNN&#29992;RNN&#65289;&#65306; <code class="fold">@</code></dt>
<dd><p>&#12298; Visin F, Kastner K, Cho K, et al. ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks[J]. arXiv preprint arXiv:1505.00393, 2015 &#12299;</p>
<p>4&#21521;RNN&#65288;&#20351;&#29992;LSTM&#21333;&#20803;&#65289;&#26367;&#20195;CNN&#12290;</p>
<p>&#20351;&#29992;LSTM&#35835;&#25026;python&#31243;&#24207;&#65306;</p>
<p>&#12298; Zaremba W, Sutskever I. Learning to execute[J]. arXiv preprint arXiv:1410.4615, 2014. &#12299;</p>
<p>&#20351;&#29992;&#22522;&#20110;LSTM&#30340;&#28145;&#24230;&#27169;&#22411;&#29992;&#20110;&#35835;&#25026;python&#31243;&#24207;&#24182;&#19988;&#32473;&#20986;&#27491;&#30830;&#30340;&#31243;&#24207;&#36755;&#20986;&#12290;&#25991;&#31456;&#30340;&#36755;&#20837;&#26159;&#30701;&#23567;&#31616;&#21333;python&#31243;&#24207;&#65292;&#36825;&#20123;&#31243;&#24207;&#30340;&#36755;&#20986;&#22823;&#37117;&#26159;&#31616;&#21333;&#30340;&#25968;&#23383;&#65292;&#20363;&#22914;0-9&#20043;&#20869;&#21152;&#20943;&#27861;&#31243;&#24207;&#12290;&#27169;&#22411;&#19968;&#20010;&#23383;&#31526;&#19968;&#20010;&#23383;&#31526;&#30340;&#36755;&#20837;python&#31243;&#24207;&#65292;&#32463;&#36807;&#22810;&#23618; LSTM&#21518;&#36755;&#20986;&#25968;&#23383;&#32467;&#26524;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;99%</p>
</dd>
<dt>&#25163;&#20889;&#35782;&#21035;&#65306;</dt>
<dd><p>&#12298; Liwicki M, Graves A, Bunke H, et al. A novel approach to on-line handwriting recognition based on bidirectional long short-term memory &#12299;</p>
</dd>
</dl>
<p>&#26426;&#22120;&#32763;&#35793;</p>
<p>&#23545;&#35805;&#29983;&#25104;</p>
<p>&#21477;&#27861;&#20998;&#26512;</p>
<p>&#20449;&#24687;&#26816;&#32034;</p>
<p>&#22270;&#25991;&#36716;&#25442;</p>
</dd>
</dl>
<hr>
<p>&#39318;&#20808;&#65292;&#23545;&#20110;&#27809;&#26377;RNN&#22522;&#30784;&#30340;&#21516;&#23398;&#65292;&#24378;&#28872;&#24314;&#35758;&#20808;&#30475;&#19968;&#19979;&#19979;&#38754;&#36825;&#31687;&#35770;&#25991;:</p>
<p>A Critical Review of Recurrent Neural Networks for Sequence Learning</p>
<p>&#37324;&#38754;&#30340;&#25968;&#23398;&#31526;&#21495;&#23450;&#20041;&#28165;&#26970;&#65292;&#38750;&#24120;&#36866;&#21512;&#27809;&#26377;&#20219;&#20309;&#22522;&#30784;&#30340;&#31461;&#38795;&#23545;RNN&#21644;LSTM&#24314;&#31435;&#19968;&#20010;&#22522;&#26412;&#30340;&#35748;&#35782;&#12290;&#28982;&#21518;&#65292;&#30475;&#23436;&#36825;&#31687;&#35770;&#25991;&#20197;&#21518;&#65292;&#21487;&#20197;&#25509;&#30528;&#30475;&#19979;&#38754;&#36825;&#31687;&#21338;&#23458;:</p>
<p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks &#8211; colah&#8217;s blog</a></p>
<p>&#37324;&#38754;&#23545;LSTM&#32467;&#26500;&#20026;&#20160;&#20040;&#36825;&#26679;&#35774;&#35745;&#65292;&#20570;&#20102;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#35299;&#37322;&#65292;&#38750;&#24120;&#30340;&#35814;&#32454;&#12290;&#30475;&#23436;&#19978;&#38754;&#20004;&#20010;tutorial, &#20320;&#23545;LSTM&#30340;&#32467;&#26500;&#24050;&#32463;&#22522;&#26412;&#20102;&#35299;&#20102;&#12290;&#22914;&#26524;&#24076;&#26395;&#23545;&#20110;&#22914;&#20309;&#35757;&#32451;LSTM, &#20102;&#35299; BPTT&#31639;&#27861;&#30340;&#24037;&#20316;&#32454;&#33410;&#65292;&#21487;&#20197;&#30475;Alex Graves&#30340;&#35770;&#25991;:</p>
<p>Supervised Sequence Labelling with Recurrent Neural Networks</p>
<p>&#36825;&#31687;&#35770;&#25991;&#37324;&#26377;&#27604;&#36739;&#35814;&#32454;&#30340;&#20844;&#24335;&#25512;&#23548;&#65292;&#20294;&#26159;&#23545;&#20110;LSTM&#30340;&#32467;&#26500;&#21364;&#35762;&#30340;&#27604;&#36739;&#28151;&#20081;&#65292;&#25152;&#20197;&#19981;&#24314;&#35758;&#20837;&#38376;&#23601;&#30475;&#36825;&#31687;&#35770;&#25991;&#12290;&#30475;&#23436;&#20102;&#19978;&#38754;&#31687;&#35770;&#25991;&#65295;&#25945;&#31243;&#20197;&#21518;&#65292;&#23545;&#20110;LSTM&#30340;&#29702;&#35770;&#30693;&#35782;&#23601;&#22522;&#26412;&#25484;&#25569;&#20102;&#65292;&#19979;&#38754;&#23601;&#38656;&#35201;&#22312;&#23454;&#36341;&#20013;&#36827;&#19968;&#27493;&#21152;&#28145;&#29702;&#35299;&#65292;&#25105;&#36824;&#27809;&#26377;&#21435;&#23454;&#36341;&#65292;&#21518;&#38754;&#30340;&#31572;&#26696;&#31561;&#23454;&#36341;&#23436;&#20197;&#21518;&#22238;&#26469;&#20877;&#34917;&#19978;&#12290;&#19981;&#36807;&#26681;&#25454;&#26377;&#32463;&#39564;&#30340;&#23398;&#38271;&#20171;&#32461;&#65292;&#20351;&#29992;Theano&#33258;&#24049;&#23454;&#29616;&#19968;&#36941;LSTM&#26159;&#19968;&#20010;&#19981;&#38169;&#30340;&#36873;&#25321;</p>
<p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></p>
</dd>
<dt><a href="https://www.zhihu.com/question/30924352/answer/50176654">Facebook &#30340;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460; (FAIR) &#26377;&#21738;&#20123;&#21385;&#23475;&#30340;&#22823;&#29275;&#21644;&#25216;&#26415;&#31215;&#32047;&#65311; - &#30000;&#28170;&#26635;&#30340;&#22238;&#31572; - &#30693;&#20046;</a> <code class="fold">@</code></dt>
<dd><p>&#20154;&#21592;&#26041;&#38754;&#65292;Yann LeCun&#27627;&#26080;&#30097;&#38382;&#26159;&#25972;&#20010;&#32452;&#30340;Director&#65292;&#20854;&#23427;&#22823;&#29275;&#26377;VC&#32500;&#21644;SVM&#30340;&#32532;&#36896;&#32773; Vladimir Vapnik&#65292;&#25552;&#20986;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#30340;L&#233;on Bottou&#65292;&#20570;&#20986;&#39640;&#24615;&#33021;PHP&#34394;&#25311;&#26426;HHVM&#30340; Keith Adams, Rob Fergus, Jason Weston, Marc&#8217;Aurelio Ranzato, Tomas Mikolov, Florent Perronnin, Piotr Dollar, Herv&#233; J&#233;gou, Ronan Collobert, Yaniv Taigman&#31561;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#20195;&#65292;&#30740;&#31350;&#21644;&#24037;&#31243;&#24050;&#32463;&#26377;&#34701;&#21512;&#30340;&#36235;&#21183;&#65292;&#22240;&#27492;FAIR&#36825;&#20004;&#26041;&#38754;&#30340;&#22823;&#29275;&#37117;&#26377;&#12290;&#24037;&#20316;&#27668;&#27675;&#19978;&#26469;&#35828;&#65292;&#32452;&#20869;&#36739;&#24179;&#31561;&#65292;&#35752;&#35770;&#33258;&#30001;&#65292;&#22522;&#26412;&#27809;&#26377;&#20256;&#32479;&#30340;&#19978;&#19979;&#32423;&#35266;&#24565;&#12290;&#33509;&#26159;&#20219;&#20309;&#20154;&#26377;&#26377;&#36259;&#30340;&#24819;&#27861;&#65292;&#22823;&#23478;&#37117;&#20250;&#20542;&#21548;&#24182;&#19988;&#20316;&#20986;&#35780;&#35770;&#12290;&#35201;&#26159;&#24819;&#27861;&#27491;&#30830;&#65292;Yann&#20063;&#20250; like&#12290;</p>
<p>&#27809;&#26377;&#20154;&#36924;&#30528;&#24178;&#27963;&#65292;&#20294;&#22823;&#23478;&#37117;&#22312;&#21162;&#21147;&#24178;&#27963;&#12290;</p>
<p>&#35201;&#26159;&#20154;&#31867;&#33021;&#25484;&#25569;&#33258;&#32452;&#32455;&#32435;&#31859;&#26426;&#26800;&#30340;&#35774;&#35745;&#24037;&#20855;&#21644;&#29983;&#20135;&#27969;&#27700;&#32447;&#65292;&#37027;&#20877;&#20250;&#26377;&#19968;&#27425;&#36136;&#30340;&#39134;&#36291;&#12290;&#32780;&#33258;&#28982;&#30028;&#21018;&#24320;&#22987;&#23601;&#26397;&#30528;&#33258;&#32452;&#32455;&#30340;&#36335;&#23376;&#36208;&#65292;&#29992;&#30340;&#26159;&#22825;&#39030;&#26143;&#20154;&#30340;&#31185;&#25216;&#65292;&#19977;&#32679;&#37240;&#24490;&#29615;&#65292;&#20809;&#21512;&#20316;&#29992;&#65292;&#31163;&#23376;&#27893;&#65292;&#37117;&#22312;&#24494;&#35266;&#20307;&#31995;&#19979;&#23436;&#25104;&#65292;&#27809;&#26377;&#25705;&#25830;&#21147;&#27809;&#26377;&#33021;&#37327;&#32791;&#25955;&#65292;&#25928;&#29575;&#37117;&#25509;&#36817;&#30334;&#20998;&#20043;&#30334;&#65292;&#36825;&#36825;&#23601;&#26159;&#20026;&#21861;&#22823;&#33041;&#33021;&#32791;&#20302;&#30340;&#21407;&#22240;&#12290;&#20154;&#31867;&#30340;&#31185;&#25216;&#30456;&#27604;&#20043;&#19979;&#31528;&#37325;&#20302;&#25928;&#65292;&#24046;&#24471;&#36828;&#12290;</p>
<p>&#22522;&#26412;&#19978;&#29616;&#22312;&#28145;&#24230;&#23398;&#20064;&#36825;&#19968;&#22359;&#37117;&#27809;&#26377;&#22312;&#27169;&#25311;&#20154;&#33041;&#65292;&#32780;&#26159;&#33258;&#24049;&#23450;&#20041;&#26368;&#31616;&#21333;&#30340;&#65288;&#29978;&#33267;&#22312;&#29983;&#29289;&#23398;&#19978;&#26159;&#38169;&#35823;&#30340;&#65289;&#30446;&#26631;&#20989;&#25968;&#21644;&#32593;&#32476;&#38142;&#25509;&#32467;&#26500;&#65292;&#32473;&#23450;&#20102;&#25968;&#25454;&#38598;&#65292;&#29031;&#30528;&#25968;&#23398;&#20934;&#21017;&#29992;&#26799;&#24230;&#19979;&#38477;&#27714;&#35299;&#26368;&#20248;&#21442;&#25968;&#12290;&#19981;&#21516;&#30340;&#21407;&#21017;&#20250;&#24471;&#21040;&#19981;&#21516;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20160;&#20040;&#19981;&#29992;&#21644;&#29983;&#29289;&#23398;&#19968;&#26679;&#30340;&#27169;&#22411;&#65311;&#22240;&#20026;&#29983;&#29289;&#23398;&#27169;&#22411;&#22826;&#22797;&#26434;&#20102;&#65292;&#19981;&#22914;&#20570;&#20010;&#31616;&#21333;&#25968;&#23398;&#25277;&#35937;&#65292;&#25928;&#26524;&#22909;&#23601;&#35828;&#26126;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#25235;&#21040;&#20102;&#26412;&#36136;&#12290;&#19981;&#28982;&#30495;&#24515;&#26159;&#19968;&#27493;&#20063;&#21160;&#19981;&#20102;&#12290;</p>
<p>&#20854;&#23454;&#20960;&#26465;&#31616;&#21333;&#21407;&#29702;&#20415;&#21487;&#20197;&#21019;&#36896;&#21450;&#20854;&#23439;&#22823;&#30340;&#31354;&#38388;&#65292;&#27604;&#22914;&#35828;19&#36335;&#22260;&#26827;&#35268;&#21017;&#31616;&#21333;&#65292;&#20294;&#20854;&#20013;&#21464;&#21270;&#26080;&#31351;&#26080;&#23613;&#65292;&#27604;&#23431;&#23449;&#30340;&#22522;&#26412;&#31890;&#23376;&#24635;&#21644;&#36824;&#22810;&#24471;&#22810;&#65292;&#22312;&#36825;&#26679;&#30340;&#31354;&#38388;&#37324;&#30021;&#28216;&#65292;&#32477;&#19981;&#20250;&#26377;&#37325;&#22797;&#20043;&#24863;&#12290;&#20154;&#24037;&#35774;&#35745;&#30340;&#35268;&#21017;&#22914;&#27492;&#65292;&#33258;&#28982;&#30028;&#26356;&#26159;&#22914;&#27492;&#65292;&#30899;&#27682;&#27687;&#27694;&#22235;&#20010;&#20803;&#32032;&#65292;&#20960;&#20046;&#32452;&#25104;&#20102;&#20840;&#37096;&#26377;&#26426;&#30028;&#12290;</p>
<p>&#20687;&#28151;&#27788;&#31995;&#32479;&#22914;&#22825;&#27668;&#65292;&#24494;&#23567;&#30340;&#21021;&#20540;&#21464;&#21270;&#20250;&#23545;&#26410;&#26469;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#65292;&#20107;&#23454;&#19978;&#30830;&#23454;&#26159;&#19981;&#21487;&#39044;&#27979;&#30340;&#12290;&#20294;&#26159;&#20154;&#33041;&#26159;&#28151;&#27788;&#30340;&#20040;&#65311;&#35828;&#19981;&#20934;&#12290;&#24182;&#19981;&#19968;&#23450;&#21464;&#37327;&#22810;&#30340;&#31995;&#32479;&#23601;&#28151;&#27788;&#12290;&#27931;&#20262;&#20857;&#22855;&#24618;&#21560;&#24341;&#23376;&#21482;&#26377;&#19977;&#20010;&#21464;&#37327;&#21734;&#65292;&#20294;&#26159;&#29031;&#26679;&#28151;&#27788;&#65307;&#35745;&#31639;&#38050;&#26753;&#24367;&#26354;&#26377;&#26080;&#31351;&#22810;&#20010;&#21464;&#37327;&#65292;&#29031;&#26679;&#29992;&#26377;&#38480;&#20803;&#35299;&#24471;&#22949;&#22949;&#30340;&#65292;&#20026;&#21861;&#21568;&#65311;&#27599;&#20010;&#31995;&#32479;&#26377;&#19981;&#21516;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#19981;&#21487;&#20197;&#19968;&#27010;&#32780;&#35770;&#30340;&#12290;</p>
<p>&#25105;&#21322;&#24180;&#21069;&#20174;&#35895;&#27468;X&#30340;&#26080;&#20154;&#36710;&#32452;&#36339;&#21040;Facebook&#30340;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#65288;FAIR&#65289;&#65292;&#24863;&#35302;&#33391;&#22810;&#65292;&#36825;&#37324;&#20889;&#19968;&#20123;&#20998;&#20139;&#32473;&#22823;&#23478;&#12290;</p>
<p>&#34429;&#28982;F&#21644;G&#24182;&#31216;&#19968;&#27969;&#30340;IT&#20844;&#21496;&#65292;&#20294;&#26159;&#20854;&#23454;&#20869;&#37096;&#26159;&#24456;&#19981;&#19968;&#26679;&#30340;&#65292;&#29978;&#33267;&#21487;&#20197;&#35828;&#23436;&#20840;&#30456;&#21453;&#12290;&#21152;&#20837;FB&#20043;&#21069;&#65292;&#38382;&#36807;&#24456;&#22810;&#26379;&#21451;&#65292;&#22823;&#23478;&#30340;&#24847;&#35265;&#32508;&#21512;&#36215;&#26469;&#26159;FB&#26377;&#28857;&#8220;&#20081;&#8221;&#65292;&#27809;&#26377;&#32479;&#19968;&#30340;&#24179;&#21488;&#65292;&#21508;&#32452;&#31649;&#21508;&#32452;&#24537;&#65292;&#20195;&#30721;&#36136;&#37327;&#27604;G&#24046;&#24456;&#22810;&#65292;&#25991;&#26723;&#20063;&#23569;&#12290;&#36825;&#21548;&#36215;&#26469;&#25402;&#21523;&#20154;&#30340;&#65292;&#20294;&#35748;&#30495;&#24819;&#24819;&#65292;&#21453;&#36807;&#26469;&#35828;&#20081;&#25165;&#26377;&#26426;&#20250;&#12290;G&#26368;&#22823;&#30340;&#38382;&#39064;&#24688;&#24688;&#26159;&#19968;&#20999;&#37117;&#20117;&#28982;&#26377;&#24207;&#65292;&#33021;&#20986;&#22823;&#25104;&#26524;&#30340;&#22320;&#26041;&#37117;&#20986;&#23436;&#20102;&#65292;&#21592;&#24037;&#23601;&#20687;&#34746;&#19997;&#38025;&#65292;&#21482;&#35201;&#22312;&#33258;&#24049;&#30340;&#23703;&#20301;&#19978;&#20570;&#22909;&#20462;&#34917;&#23601;&#34892;&#20102;&#12290;</p>
<p>&#36825;&#20004;&#22825;&#32489;&#25928;&#32771;&#26680;&#65292;&#25105;&#32769;&#23110;&#35780;&#35770;&#35828;&#25105;&#36825;&#21322;&#24180;&#24178;&#30340;&#20107;&#24773;&#27604;&#22312;G&#23478;&#19968;&#24180;&#36824;&#22810;&#65292;&#26377;&#20135;&#21697;&#21457;&#24067;&#20063;&#26377;&#30740;&#31350;&#65292;&#25105;&#24819;&#36825;&#23601;&#26159;&#30495;&#27491;&#25226;&#20852;&#36259;&#29992;&#22312;&#24037;&#20316;&#19978;&#30340;&#32467;&#26524;&#12290;&#25105;&#36824;&#35760;&#24471;&#33258;&#24049;&#26368;&#21518;&#19968;&#22825;&#22312;G&#30340;&#26085;&#23376;&#65292; HR&#23567;&#22993;&#23064;&#26368;&#21518;&#38382;&#25105;&#20026;&#21861;&#31163;&#24320;&#65292;&#26159;&#19981;&#26159;&#22240;&#20026;X&#30340;&#24037;&#20316;&#22826;&#36763;&#33510;&#65292;&#38656;&#35201;&#19968;&#20123;&#24037;&#20316;&#21644;&#29983;&#27963;&#30340;&#24179;&#34913;&#65311;&#25105;&#31505;&#20102;&#31505;&#65292;&#25975;&#34893;&#20102;&#20960;&#21477;&#65292;&#24515;&#37324;&#24819;&#36215;&#20102;&#12298;&#20912;&#19982;&#28779;&#20043;&#27468;&#12299;&#37324;&#30340;&#37027;&#19968;&#21477;&#21488;&#35789;&#8212;&#8212;</p>
<blockquote>
<p>&#38634;&#35834;&#65292;&#20320;&#20160;&#20040;&#20063;&#19981;&#25026;&#12290;</p>
</blockquote>
<p>refs and see also</p>
<ul>
<li><a href="http://zhuanlan.zhihu.com/p/19953128">&#20851;&#20110;&#8221;&#20570;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#19968;&#23450;&#35201;&#23398;&#28857;&#29983;&#29289;&#8220; - &#36828;&#19996;&#36726;&#20107; - &#30693;&#20046;&#19987;&#26639;</a></li>
<li><a href="http://zhuanlan.zhihu.com/p/20111731">&#36716;&#32844;&#21322;&#24180;&#24635;&#32467; - &#36828;&#19996;&#36726;&#20107; - &#30693;&#20046;&#19987;&#26639;</a></li>
</ul>
</dd>
</dl>
<p><a href="https://www.zhihu.com/question/36853910">GitHub &#19978;&#26377;&#21738;&#20123;&#26377;&#36259;&#30340;&#20851;&#20110; NLP &#25110;&#32773; DL &#30340;&#39033;&#30446;&#65311; - &#30693;&#20046;</a></p>
<p><a href="https://www.zhihu.com/question/20691338">&#26426;&#22120;&#23398;&#20064;&#35813;&#24590;&#20040;&#20837;&#38376;&#65311; - &#30693;&#20046;</a></p>
<p><a href="http://suanfazu.com/t/ji-qi-xue-xi-jing-dian-lun-wen-slash-surveyhe-ji/14">&#26426;&#22120;&#23398;&#20064;&#32463;&#20856;&#35770;&#25991;/survey&#21512;&#38598; - &#26426;&#22120;&#23398;&#20064; - &#31639;&#27861;&#32452;</a></p>
<p><a href="http://suanfazu.com/t/topic/15">&#26426;&#22120;&#23398;&#20064;&#32463;&#20856;&#20070;&#31821; - &#26426;&#22120;&#23398;&#20064; - &#31639;&#27861;&#32452;</a></p>
<p><a href="http://ml.memect.com/article/machine-learning-guide.html">&#26426;&#22120;&#23398;&#20064;&#20837;&#38376;&#36164;&#28304;&#19981;&#23436;&#20840;&#27719;&#24635; | &#22909;&#19996;&#35199;&#20256;&#36865;&#38376;&#20986;&#21697;</a></p>
<p><a href="http://www.cnblogs.com/tornadomeet/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">&#26426;&#22120;&#23398;&#20064; - &#26631;&#31614; - tornadomeet - &#21338;&#23458;&#22253;</a></p>
<p><a href="http://gitxiv.com/best">Best | GitXiv</a></p>
<p><a href="https://www.zhihu.com/topic/19559450/top-answers">&#26426;&#22120;&#23398;&#20064; - &#35805;&#39064;&#31934;&#21326; - &#30693;&#20046;</a></p>
<p><a href="https://www.zhihu.com/topic/19813032/top-answers?page=1">&#28145;&#24230;&#23398;&#20064;&#65288;Deep Learning&#65289; - &#35805;&#39064;&#31934;&#21326; - &#30693;&#20046;</a></p>
<p><a href="https://www.zhihu.com/topic/19607065/top-answers">&#31070;&#32463;&#32593;&#32476; - &#35805;&#39064;&#31934;&#21326; - &#30693;&#20046;</a></p>
<pre><code>// euclidean distance
distance d -&gt; [0, inf], 1/(d+1) -&gt; [0, 1]

// pearson correlation score
best-fit line
similarity metric: sim_pearson, sim_distance</code></pre>
<p><a href="https://en.wikipedia.org/wiki/Metric_(mathematics)#Examples">Metric (mathematics) - Wikipedia, the free encyclopedia</a></p>
<p><a href="https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">Pearson product-moment correlation coefficient - Wikipedia, the free encyclopedia</a></p>
<p><a href="http://whudoc.qiniudn.com/2016/notepad++.7z" class="uri">http://whudoc.qiniudn.com/2016/notepad++.7z</a></p>
<p><a href="https://github.com/cataska/programming-collective-intelligence-code">cataska/programming-collective-intelligence-code: Examples from Programming Collective Intelligence</a></p>
<p><a href="https://book.douban.com/subject/3288908/">&#38598;&#20307;&#26234;&#24935;&#32534;&#31243; (&#35910;&#29923;)</a></p>
<p><a href="http://shop.oreilly.com/product/9780596529321.do">Programming Collective Intelligence&#160;-&#160;O&#8217;Reilly Media</a></p>
<dl>
<dt><a href="https://book.douban.com/subject/2209702/">Programming Collective Intelligence (&#35910;&#29923;)</a> <code class="fold">@</code></dt>
<dd><ul>
<li>simulated annealing&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;</li>
<li>genetic algorithms&#65288;&#36951;&#20256;&#31639;&#27861;&#65289; <code class="foldable">@</code>
<ul>
<li>population&#65288;&#31181;&#32676;&#65289;&#65292;hand-designed or &#38543;&#26426;&#30340;&#65307;user-defined task</li>
<li>elitism&#65288;&#31934;&#33521;&#36873;&#25300;&#65289;</li>
<li>mutation&#65288;&#21464;&#24322;&#65289;</li>
<li>crossover/breeding</li>
<li>&#20248;&#32988;&#21155;&#27760;&#30340; evolutionary pressure&#65306;survival of the fittest</li>
<li>a round-robin tournament</li>
<li>evaluating trees</li>
</ul></li>
<li>massand-spring algorithms&#65288;&#36136;&#28857;&#24377;&#31783;&#31639;&#27861;&#65289;</li>
<li>flipping around&#65288;&#35843;&#25442;&#27714;&#35299;&#65289;</li>
<li>decision trees <code class="foldable">@</code>
<ul>
<li>CART&#65288;classification and regression trees&#65289;</li>
<li>pruning the tree&#65288;&#21098;&#26525;&#65289;</li>
</ul></li>
<li>kNN: k-Nearest Neighbors <code class="foldable">@</code>
<ul>
<li>weighted kNN</li>
<li>cross validation&#65288;&#20132;&#21449;&#39564;&#35777;&#65289;</li>
</ul></li>
<li>kernel methods
<ul>
<li>kernel trick: radial-basis function &#65288;&#24452;&#21521;&#22522;&#20989;&#25968;&#65289;</li>
</ul></li>
<li>SVM <code class="foldable">@</code>
<ul>
<li>mamimum-margin hyperplane</li>
<li>&#20301;&#20110;&#20998;&#21106;&#32447;&#65306;&#25903;&#25345;&#21521;&#37327;</li>
<li>&#25214;&#21040;&#25903;&#25345;&#21521;&#37327;&#30340;&#31639;&#27861;&#65306;&#25903;&#25345;&#21521;&#37327;&#26426;</li>
</ul></li>
<li>data intensive&#65288;&#25968;&#25454;&#37327;&#22823;&#65289;</li>
<li>libsvm</li>
<li>bayesian classification</li>
<li>trading volume&#65288;&#25104;&#20132;&#37327;&#65289;</li>
<li>Non-negative Matrix Factorization (NFM)</li>
<li>tanimoto coefficient</li>
<li>gini impurity</li>
<li>entropy</li>
</ul>
<dl>
<dt>Other Books <code class="fold">@</code></dt>
<dd><ul>
<li><a href="https://book.douban.com/subject/4888560/">Data Mining (&#35910;&#29923;)</a></li>
<li><a href="https://book.douban.com/subject/6533777/">Data Mining (&#35910;&#29923;)</a></li>
<li><a href="https://book.douban.com/subject/2061116/">Pattern Recognition And Machine Learning (&#35910;&#29923;)</a></li>
<li><a href="http://research.microsoft.com/en-us/um/people/cmbishop/PRML/">Christopher M. Bishop | PRML</a></li>
</ul>
<div class="figure">
<img src="http://whudoc.qiniudn.com/2016/prmlfigs-png/Figure1.1.png" />

</div>
</dd>
</dl>
</dd>
</dl>
<p><a href="https://en.wikipedia.org/wiki/Backgammon">Backgammon - Wikipedia, the free encyclopedia</a></p>
<dl>
<dt>CS229, Machine Learning, Andrew Ng, Sanford University <code class="fold">@</code></dt>
<dd><dl>
<dt>Advice for applying Machine Learning <code class="fold">@</code></dt>
<dd><dl>
<dt>Key ideas <code class="fold">@</code></dt>
<dd><ol style="list-style-type: decimal">
<li>Diagnostics for debugging learning algorithms.</li>
</ol>
</dd>
<dd><ol start="2" style="list-style-type: decimal">
<li>Error analyses and ablative analysis.</li>
</ol>
</dd>
<dd><ol start="3" style="list-style-type: decimal">
<li>How to get started on a machine learning problem. Premature (statistical) optimization.</li>
</ol>
</dd>
<dt>Diagnostic for bias vs.&#160;variance <code class="fold">@</code></dt>
<dd><p>Suppose you suspect the problem is either:</p>
<ul>
<li>Overfitting (high variance).</li>
<li>Too few features to classify spam (high bias).</li>
</ul>
<p>Diagnostic:</p>
<ul>
<li>Variance: Training error will be much lower than test error.</li>
<li>Bias: Training error will also be high.</li>
</ul>
<div class="figure">
<img src="http://whudoc.qiniudn.com/2016/20160505134652.png" alt="Typical learning curve for high variance" />
<p class="caption">Typical learning curve for high variance</p>
</div>
<ul>
<li>Test error still decreasing as m increases. Suggests larger training set will help.</li>
<li>Large gap between training and test error.</li>
</ul>
<div class="figure">
<img src="http://whudoc.qiniudn.com/2016/20160505135614.png" alt="Typical learning curve for high bias" />
<p class="caption">Typical learning curve for high bias</p>
</div>
<ul>
<li>Even training error is unacceptably high.</li>
<li>Small gap between training and test error.</li>
</ul>
</dd>
</dl>
<p>Is the algorithm (gradient descent for logistic regression) converging?</p>
<dl>
<dt>Error analysis &amp; Ablative (<code>['&#230;bl&#601;t&#618;v]</code>, &#31163;&#26684;) analysis <code class="fold">@</code></dt>
<dd><div class="figure">
<img src="http://whudoc.qiniudn.com/2016/20160505142736.png" />

</div>
<p>Error analysis tries to explain the difference between current performance and perfect performance. Ablative analysis tries to explain the difference between some baseline (much poorer) performance and current performance.</p>
<p>Just what accounts for your improvement from 94 to 99.9%?</p>
</dd>
<dt>Getting started on a learning problem <code class="foldable">@</code></dt>
<dd><ul>
<li><p>Approach #1: <strong>Careful design</strong></p></li>
<li><p>Approach #2: <strong>Build-and-fix</strong></p>
<p>Implement something quick-and-dirty</p></li>
</ul>
</dd>
<dt>Premature statistical optimization</dt>
<dd><p>is bad.</p>
<p>The only way to find out what needs work is to implement something quickly, and find out what parts break.</p>
</dd>
</dl>
<p>Sammary <code class="foldable">@</code></p>
<ul>
<li>Time spent coming up with diagnostics for learning algorithms is time well-spent.</li>
<li>It&#8217;s often up to your own ingenuity (<code>[,&#618;nd&#658;&#601;'nu&#601;ti]</code>, n. &#29420;&#21019;&#24615;&#65307;&#31934;&#24039;) to come up with right diagnostics.</li>
<li>Error analyses and ablative analyses also give insight into the problem.</li>
<li>Two approaches to applying learning algorithms:
<ul>
<li>Design very carefully, then implement.
<ul>
<li>Risk of premature (statistical) optimization.</li>
</ul></li>
<li>Build a quick-and-dirty prototype, diagnose, and fix.</li>
</ul></li>
</ul>
</dd>
</dl>
</dd>
<dt>Wikipedia <code class="fold">@</code></dt>
<dd><p><a href="https://en.wikipedia.org/wiki/Machine_learning">Machine learning - Wikipedia, the free encyclopedia</a></p>
</dd>
<dt><a href="https://en.wikipedia.org/wiki/Tensor">Tensor - Wikipedia, the free encyclopedia</a> <code class="fold">@</code></dt>
<dd><p>Tensors are geometric objects that describe linear relations between geometric vectors, scalars, and other tensors. Elementary examples of such relations include the dot product, the cross product, and linear maps. Euclidean vectors, often used in physics and engineering applications, and scalars themselves are also tensors. A more sophisticated example is the Cauchy stress tensor T, which takes a direction v as input and produces the stress T(v) on the surface normal to this vector for output, thus expressing a relationship between these two vectors, shown in the figure (right).</p>
<div class="figure">
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Components_stress_tensor.svg/450px-Components_stress_tensor.svg.png" alt="Cauchy stress tensor, a second-order tensor. The tensor&#8217;s components, in a three-dimensional Cartesian coordinate system, form the matrix \begin{align} \sigma &amp; = \begin{bmatrix}\mathbf{T}^{(\mathbf{e}_1)}
\mathbf{T}^{(\mathbf{e}_2)} \mathbf{T}^{(\mathbf{e}_3)} \\ \end{bmatrix} \\
&amp; = \begin{bmatrix} \sigma_{11} &amp; \sigma_{12} &amp; \sigma_{13} \\ \sigma_{21}
&amp; \sigma_{22} &amp; \sigma_{23} \\ \sigma_{31} &amp; \sigma_{32} &amp; \sigma_{33}
\end{bmatrix}\\ \end{align} whose columns are the stresses (forces per unit area) acting on the e1, e2, and e3 faces of the cube." />
<p class="caption">Cauchy stress tensor, a second-order tensor. The tensor&#8217;s components, in a three-dimensional Cartesian coordinate system, form the matrix <span class="math display">\[\begin{align} \sigma &amp; = \begin{bmatrix}\mathbf{T}^{(\mathbf{e}_1)}
\mathbf{T}^{(\mathbf{e}_2)} \mathbf{T}^{(\mathbf{e}_3)} \\ \end{bmatrix} \\
&amp; = \begin{bmatrix} \sigma_{11} &amp; \sigma_{12} &amp; \sigma_{13} \\ \sigma_{21}
&amp; \sigma_{22} &amp; \sigma_{23} \\ \sigma_{31} &amp; \sigma_{32} &amp; \sigma_{33}
\end{bmatrix}\\ \end{align}\]</span> whose columns are the stresses (forces per unit area) acting on the e1, e2, and e3 faces of the cube.</p>
</div>
<p>In terms of a coordinate basis or fixed frame of reference, a tensor can be represented as an organized multidimensional array of numerical values. The order (also degree or rank) of a tensor is the dimensionality of the array needed to represent it, or equivalently, the number of indices needed to label a component of that array. For example, a linear map is represented by a matrix (a 2-dimensional array) in a basis, and therefore is a 2nd-order tensor. A vector is represented as a 1-dimensional array in a basis, and is a 1st-order tensor. Scalars are single numbers and are thus 0th-order tensors. Because they express a relationship between vectors, tensors themselves must be independent of a particular choice of coordinate system. The coordinate independence of a tensor then takes the form of a &#8220;covariant&#8221; transformation law that relates the array computed in one coordinate system to that computed in another one. The precise form of the transformation law determines the type (or valence) of the tensor. The tensor type is a pair of natural numbers (n, m), where n is the number of contravariant indices and m is the number of covariant indices. The total order of a tensor is the sum of these two numbers.</p>
<p>Tensors are important in physics because they provide a concise mathematical framework for formulating and solving physics problems in areas such as elasticity, fluid mechanics, and general relativity. Tensors were first conceived by Tullio Levi-Civita and Gregorio Ricci-Curbastro, who continued the earlier work of Bernhard Riemann and Elwin Bruno Christoffel and others, as part of the absolute differential calculus. The concept enabled an alternative formulation of the intrinsic differential geometry of a manifold in the form of the Riemann curvature tensor.</p>
</dd>
<dt><a href="http://deeplearning.net/software/theano/tutorial/">Tutorial &#8212; Theano 0.8.0 documentation</a> <code class="fold">@</code></dt>
<dd><p>Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently.</p>
<p>Theano features: <code class="foldable">@</code></p>
<ul>
<li><strong>tight integration with NumPy</strong> &#8211; Use numpy.ndarray in Theano-compiled functions.</li>
<li><strong>transparent use of a GPU</strong> &#8211; Perform data-intensive calculations up to 140x faster than with CPU.(float32 only)</li>
<li><strong>efficient symbolic differentiation</strong> &#8211; Theano does your derivatives for function with one or many inputs.</li>
<li>speed and stability optimizations &#8211; Get the right answer for log(1+x) even when x is really tiny.</li>
<li><strong>dynamic C code generation</strong> &#8211; Evaluate expressions faster.</li>
<li>extensive unit-testing and self-verification &#8211; Detect and diagnose many types of errors.</li>
</ul>
<p>Theano has been powering large-scale computationally intensive scientific investigations <strong>since 2007</strong>. But it is also approachable enough to be used in the classroom (University of Montreal&#8217;s deep learning/machine learning classes). <code class="fold">@</code></p>
<ul>
<li><p>You should learn some python. <a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p></li>
<li><p>Matrix <strong>conventions</strong> for machine learning <a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<ul>
<li><p>Every row is an example.</p></li>
<li><p>numpy, <code>numpyarray.shape()</code>, <code>arr[ r, c ]</code></p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> numpy.asarray([[<span class="dv">1</span>., <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>], [<span class="dv">5</span>, <span class="dv">6</span>]])
array([[ <span class="dv">1</span>.,  <span class="dv">2</span>.],
       [ <span class="dv">3</span>.,  <span class="dv">4</span>.],
       [ <span class="dv">5</span>.,  <span class="dv">6</span>.]])
<span class="op">&gt;&gt;&gt;</span> numpy.asarray([[<span class="dv">1</span>., <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>], [<span class="dv">5</span>, <span class="dv">6</span>]]).shape
(<span class="dv">3</span>, <span class="dv">2</span>)

<span class="co"># get entry value</span>
<span class="op">&gt;&gt;&gt;</span> numpy.asarray([[<span class="dv">1</span>., <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>], [<span class="dv">5</span>, <span class="dv">6</span>]])[<span class="dv">2</span>, <span class="dv">0</span>]
<span class="fl">5.0</span></code></pre></div></li>
<li><p>broadcasting (cast as in <code>static_cast</code>, <code>const_cast</code>)</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> a <span class="op">=</span> numpy.asarray([<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">3.0</span>])
<span class="op">&gt;&gt;&gt;</span> b <span class="op">=</span> <span class="fl">2.0</span>
<span class="op">&gt;&gt;&gt;</span> a <span class="op">*</span> b <span class="co"># broadcasting b (a 0-d array) to same size of a</span>
array([ <span class="dv">2</span>.,  <span class="dv">4</span>.,  <span class="dv">6</span>.])</code></pre></div>
<p>see more at <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">numpy user guide: basics broadcasting</a>.</p></li>
</ul></li>
<li><p>Basics <code class="foldable">@</code></p>
<ul>
<li><p>Baby Steps - Algebra <code class="fold">@</code></p>
<ul>
<li><p>Adding two Scalars</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> numpy
<span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> theano.tensor <span class="im">as</span> T
<span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> theano <span class="im">import</span> function

<span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> T.dscalar(<span class="st">&#39;x&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> y <span class="op">=</span> T.dscalar(<span class="st">&#39;y&#39;</span>)

    <span class="co">#    about type</span>
    <span class="co">#    In Theano, all symbols must be **typed**</span>
    <span class="co">#</span>
    <span class="co">#        d         :   double</span>
    <span class="co">#        dscalar   :   0-dim arrays (scalar) of doubles (d)</span>
    <span class="co">#</span>
    <span class="op">&gt;&gt;&gt;</span> <span class="bu">type</span>(x)
    <span class="op">&lt;</span><span class="kw">class</span> <span class="st">&#39;theano.tensor.var.TensorVariable&#39;</span><span class="op">&gt;</span>
    <span class="op">&gt;&gt;&gt;</span> x.<span class="bu">type</span>
    TensorType(float64, scalar)
    <span class="op">&gt;&gt;&gt;</span> T.dscalar
    TensorType(float64, scalar)
    <span class="op">&gt;&gt;&gt;</span> x.<span class="bu">type</span> <span class="op">is</span> T.dscalar
    <span class="va">True</span>

<span class="op">&gt;&gt;&gt;</span> z <span class="op">=</span> x <span class="op">+</span> y

    <span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> theano <span class="im">import</span> pp <span class="co"># pretty print</span>
    <span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(pp(z))
    (x <span class="op">+</span> y)

        <span class="co"># powerful python `eval`</span>
        <span class="op">&gt;&gt;&gt;</span> z.<span class="bu">eval</span>({x : <span class="fl">16.3</span>, y : <span class="fl">12.1</span>}) <span class="co"># value assignment</span>

<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> function([x, y], z)

    <span class="co"># f: function(input, output)</span>

<span class="op">&gt;&gt;&gt;</span> f(<span class="dv">2</span>, <span class="dv">3</span>)         <span class="co"># f([2,3]), [2,3] as the input</span>
array(<span class="fl">5.0</span>)
<span class="op">&gt;&gt;&gt;</span> numpy.allclose(f(<span class="fl">16.3</span>, <span class="fl">12.1</span>), <span class="fl">28.4</span>)
<span class="va">True</span></code></pre></div></li>
<li><p>Adding two Matrices</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> T.dmatrix(<span class="st">&#39;x&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> y <span class="op">=</span> T.dmatrix(<span class="st">&#39;y&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> z <span class="op">=</span> x <span class="op">+</span> y
<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> function([x, y], z)

    <span class="co"># matrix addition (version 1: python native, version 2: numpy)</span>

    <span class="op">&gt;&gt;&gt;</span> f([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]], [[<span class="dv">10</span>, <span class="dv">20</span>], [<span class="dv">30</span>, <span class="dv">40</span>]])
    array([[ <span class="dv">11</span>.,  <span class="dv">22</span>.],
           [ <span class="dv">33</span>.,  <span class="dv">44</span>.]])

    <span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> numpy
    <span class="op">&gt;&gt;&gt;</span> f(numpy.array([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]]), numpy.array([[<span class="dv">10</span>, <span class="dv">20</span>], [<span class="dv">30</span>, <span class="dv">40</span>]]))
    array([[ <span class="dv">11</span>.,  <span class="dv">22</span>.],
           [ <span class="dv">33</span>.,  <span class="dv">44</span>.]])</code></pre></div></li>
<li><p>Notes</p>
<p>The following types are available: <code class="fold">@</code></p>
<ul>
<li>scalar, vector, matrix, row, col, tensor3, tensor4</li>
<li>b: byte</li>
<li>w: word (16-bit integer)</li>
<li>i: int (32-bit)</li>
<li>l: long int (64-bit)</li>
<li>f: float (32-bit)</li>
<li>d: double (64-bit)</li>
<li>c: complex</li>
</ul></li>
<li><p>Exercise</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> theano
<span class="op">&gt;&gt;&gt;</span> a <span class="op">=</span> theano.tensor.vector()          <span class="co"># declare variable</span>
<span class="op">&gt;&gt;&gt;</span> out <span class="op">=</span> a <span class="op">+</span> a <span class="op">**</span> <span class="dv">10</span>                   <span class="co"># build symbolic expression</span>
<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> theano.function([a], out)       <span class="co"># compile function</span>
<span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(f([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]))
[    <span class="dv">0</span>.     <span class="dv">2</span>.  <span class="dv">1026</span>.]</code></pre></div>
<p>Modify and execute this code to compute this expression: <code>a ** 2 + b ** 2 + 2 * a * b</code>.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> __future__ <span class="im">import</span> absolute_import, print_function, division
<span class="im">import</span> theano
a <span class="op">=</span> theano.tensor.vector()              <span class="co"># declare variable</span>
b <span class="op">=</span> theano.tensor.vector()  <span class="co"># declare variable</span>
out <span class="op">=</span> a <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> b <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> a <span class="op">*</span> b       <span class="co"># build symbolic expression</span>
f <span class="op">=</span> theano.function([a, b], out)        <span class="co"># compile function</span>
<span class="bu">print</span>(f([<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">4</span>, <span class="dv">5</span>]))                <span class="co"># prints [ 25.  49.]</span></code></pre></div></li>
</ul></li>
<li><p>refs and see also</p>
<ul>
<li><a href="http://deeplearning.net/software/theano/library/tensor/basic.html#libdoc-basic-tensor">Basic Tensor Functionality &#8212; Theano 0.8.0 documentation</a></li>
</ul></li>
<li><p>More Examples <code class="fold">@</code></p>
<ul>
<li><p>Logistic Function</p>
<p><strong>erf, g(x) = 1/(1+e^(-x)) = (1+tanh(x/2))/2</strong></p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> theano
<span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> theano.tensor <span class="im">as</span> T
<span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> T.dmatrix(<span class="st">&#39;x&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> s <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> T.exp(<span class="op">-</span>x))
<span class="op">&gt;&gt;&gt;</span> logistic <span class="op">=</span> theano.function([x], s)
<span class="op">&gt;&gt;&gt;</span> logistic([[<span class="dv">0</span>, <span class="dv">1</span>], [<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>]])
array([[ <span class="fl">0.5</span>       ,  <span class="fl">0.73105858</span>],
       [ <span class="fl">0.26894142</span>,  <span class="fl">0.11920292</span>]])</code></pre></div></li>
<li><p>Computing More than one Thing at the Same Time</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> a, b <span class="op">=</span> T.dmatrices(<span class="st">&#39;a&#39;</span>, <span class="st">&#39;b&#39;</span>)        <span class="co"># &#27880;&#24847;&#36825;&#37324;: matrix-&gt;matrices</span>
<span class="op">&gt;&gt;&gt;</span> diff <span class="op">=</span> a <span class="op">-</span> b
<span class="op">&gt;&gt;&gt;</span> abs_diff <span class="op">=</span> <span class="bu">abs</span>(diff)
<span class="op">&gt;&gt;&gt;</span> diff_squared <span class="op">=</span> diff<span class="op">**</span><span class="dv">2</span>
<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> theano.function([a, b], [diff, abs_diff, diff_squared])</code></pre></div></li>
<li><p>Setting a Default Value for an Argument</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> theano <span class="im">import</span> In
<span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> theano <span class="im">import</span> function
<span class="op">&gt;&gt;&gt;</span> x, y <span class="op">=</span> T.dscalars(<span class="st">&#39;x&#39;</span>, <span class="st">&#39;y&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> z <span class="op">=</span> x <span class="op">+</span> y
<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> function([x, In(y, value<span class="op">=</span><span class="dv">1</span>)], z)    <span class="co"># default value of y is 1</span>
<span class="op">&gt;&gt;&gt;</span> f(<span class="dv">33</span>)
array(<span class="fl">34.0</span>)
<span class="op">&gt;&gt;&gt;</span> f(<span class="dv">33</span>, <span class="dv">2</span>)
array(<span class="fl">35.0</span>)

<span class="co"># These parameters can be set positionally or by name, as in</span>
<span class="co"># standard Python</span>
<span class="op">&gt;&gt;&gt;</span> x, y, w <span class="op">=</span> T.dscalars(<span class="st">&#39;x&#39;</span>, <span class="st">&#39;y&#39;</span>, <span class="st">&#39;w&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> z <span class="op">=</span> (x <span class="op">+</span> y) <span class="op">*</span> w
<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> function([x, In(y, value<span class="op">=</span><span class="dv">1</span>), In(w, value<span class="op">=</span><span class="dv">2</span>, name<span class="op">=</span><span class="st">&#39;w_by_name&#39;</span>)], z)
<span class="op">&gt;&gt;&gt;</span> f(<span class="dv">33</span>)
array(<span class="fl">68.0</span>)
<span class="op">&gt;&gt;&gt;</span> f(<span class="dv">33</span>, <span class="dv">2</span>)
array(<span class="fl">70.0</span>)
<span class="op">&gt;&gt;&gt;</span> f(<span class="dv">33</span>, <span class="dv">0</span>, <span class="dv">1</span>)
array(<span class="fl">33.0</span>)
<span class="op">&gt;&gt;&gt;</span> f(<span class="dv">33</span>, w_by_name<span class="op">=</span><span class="dv">1</span>)
array(<span class="fl">34.0</span>)
<span class="op">&gt;&gt;&gt;</span> f(<span class="dv">33</span>, w_by_name<span class="op">=</span><span class="dv">1</span>, y<span class="op">=</span><span class="dv">0</span>)
array(<span class="fl">33.0</span>)</code></pre></div></li>
<li><p>Using Shared Variables</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> theano <span class="im">import</span> shared
<span class="op">&gt;&gt;&gt;</span> state <span class="op">=</span> shared(<span class="dv">0</span>)           <span class="co"># shared value, like `static&#39; in c?</span>
<span class="op">&gt;&gt;&gt;</span> inc <span class="op">=</span> T.iscalar(<span class="st">&#39;inc&#39;</span>)
<span class="co">#                               return cur state</span>
<span class="op">&gt;&gt;&gt;</span> accumulator <span class="op">=</span> function([inc], state, updates<span class="op">=</span>[(state, state<span class="op">+</span>inc)])

    <span class="co"># get</span>
    <span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(state.get_value())
    <span class="dv">0</span>
    <span class="op">&gt;&gt;&gt;</span> accumulator(<span class="dv">1</span>)
    array(<span class="dv">0</span>)
    <span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(state.get_value())
    <span class="dv">1</span>
    <span class="op">&gt;&gt;&gt;</span> accumulator(<span class="dv">300</span>)
    array(<span class="dv">1</span>)
    <span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(state.get_value())
    <span class="dv">301</span>

    <span class="co"># set</span>
    <span class="op">&gt;&gt;&gt;</span> state.set_value(<span class="op">-</span><span class="dv">1</span>)
    <span class="op">&gt;&gt;&gt;</span> accumulator(<span class="dv">3</span>)
    array(<span class="op">-</span><span class="dv">1</span>)
    <span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(state.get_value())
    <span class="dv">2</span></code></pre></div></li>
<li><p>Copying functions</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> new_state <span class="op">=</span> theano.shared(<span class="dv">0</span>)
<span class="op">&gt;&gt;&gt;</span> new_accumulator <span class="op">=</span> accumulator.copy(swap<span class="op">=</span>{state:new_state})
<span class="op">&gt;&gt;&gt;</span> new_accumulator(<span class="dv">100</span>)
[array(<span class="dv">0</span>)]
<span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(new_state.get_value())
<span class="dv">100</span></code></pre></div></li>
<li><p>Using Random Numbers</p>
<ul>
<li><p>Brief Example</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> theano.tensor.shared_randomstreams <span class="im">import</span> RandomStreams
<span class="im">from</span> theano <span class="im">import</span> function
srng <span class="op">=</span> RandomStreams(seed<span class="op">=</span><span class="dv">234</span>)

<span class="co"># &#21035;&#24536;&#20102;&#65292;rv: random variable</span>

<span class="co"># a random stream of 2x2 matrices of draws from a uniform distribution</span>
rv_u <span class="op">=</span> srng.uniform((<span class="dv">2</span>,<span class="dv">2</span>))
<span class="co">#                                                  normal distribution</span>
rv_n <span class="op">=</span> srng.normal((<span class="dv">2</span>,<span class="dv">2</span>))

f <span class="op">=</span> function([], rv_u)              <span class="co"># no input, just grab out streamed value</span>
g <span class="op">=</span> function([], rv_n, no_default_updates<span class="op">=</span><span class="va">True</span>)    <span class="co"># Not updating rv_n.rng</span>

    <span class="co"># deps on update or not</span>
    <span class="op">&gt;&gt;&gt;</span> f() <span class="op">!=</span> f() <span class="co"># different numbers from f_val0</span>
    <span class="op">&gt;&gt;&gt;</span> g() <span class="op">!=</span> g() <span class="co"># same number everytime</span>

nearly_zeros <span class="op">=</span> function([], rv_u <span class="op">+</span> rv_u <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> rv_u)

    <span class="co"># &#36825;&#20010;&#29305;&#24615;&#30340;&#22909;&#22788;&#26159;&#65292;&#19981;&#29992;&#25226;&#36825;&#20010; generate &#30340;&#20540;&#23384;&#36215;&#26469;</span>
    <span class="co"># &#19981;&#22909;&#22312;&#20110;&#65292;&#22914;&#26524;&#20320;&#24076;&#26395;&#23427;&#19981;&#19968;&#26679;&#65292;&#23601;&#24471;&#20998;&#21035; generate &#20877;&#36816;&#31639;</span></code></pre></div></li>
<li><p>Seeding Streams</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> rng_val <span class="op">=</span> rv_u.rng.get_value(borrow<span class="op">=</span><span class="va">True</span>)   <span class="co"># Get the rng for rv_u</span>
<span class="op">&gt;&gt;&gt;</span> rng_val.seed(<span class="dv">89234</span>)                         <span class="co"># seeds the generator</span>
<span class="op">&gt;&gt;&gt;</span> rv_u.rng.set_value(rng_val, borrow<span class="op">=</span><span class="va">True</span>)    <span class="co"># Assign back seeded rng</span></code></pre></div></li>
<li>Sharing Streams Between Functions</li>
<li>Copying Random State Between Theano Graphs</li>
<li>Other Random Distributions</li>
<li><p>Other Implementations</p>
<p>TODO: <a href="http://deeplearning.net/software/theano/tutorial/examples.html#using-random-numbers" class="uri">http://deeplearning.net/software/theano/tutorial/examples.html#using-random-numbers</a></p></li>
</ul></li>
<li><p>A Real Example: Logistic Regression</p>
<p>&#20808;&#30475;&#30475; numpy &#25552;&#20379;&#30340;&#19968;&#20123; rand &#20989;&#25968;&#65306;</p>
<ul>
<li><p><code>numpy.random.rand(d0, d1, ..., dn)</code>, uniform distribubition, <code>[0, 1)</code></p></li>
<li><p><code>numpy.random.randint(low, high=None, size=None)</code>, descrete uniform distrib, <code>[low, high)</code></p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> np.random.randint(<span class="dv">2</span>, size<span class="op">=</span><span class="dv">10</span>)
array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>])
<span class="op">&gt;&gt;&gt;</span> np.random.randint(<span class="dv">1</span>, size<span class="op">=</span><span class="dv">10</span>)
array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>])

<span class="op">&gt;&gt;&gt;</span> np.random.randint(<span class="dv">5</span>, size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">4</span>))
array([[<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>],
       [<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">0</span>]])</code></pre></div>
<p>&#24403;&#27809; high &#30340;&#26102;&#20505;&#65292;&#20854;&#23454; low &#26159; high&#65292;0 &#26159; low&#8230;&#8230;&#36825; api &#20063;&#22826;&#24694;&#24515;&#20102;&#12290;</p>
<p>&#20854;&#23454;&#21487;&#20197;&#20889;&#25104;&#20004;&#20010; api:</p>
<ul>
<li>numpy.random.randint(high, size=None), <code>[0, high)</code></li>
<li>numpy.random.randint(low, high, size=None), <code>[low, high)</code></li>
</ul>
<p>&#21487;&#33021;&#22240;&#20026;&#36825;&#20010;&#25509;&#21475;&#22826;&#24694;&#24515;&#8230;&#8230;&#19979;&#38754;&#30340;&#20195;&#30721;&#29992;&#30340;&#26159; <code>low=..., high=...</code>.</p></li>
<li><p><code>numpy.random.randn(d0, d1, ..., dn)</code>, normal distrib, &#27491;&#24577;&#20998;&#24067;&#65292;&#36820;&#22238;&#22810;&#32500;&#25968;&#32452;&#12290; <strong>dims: d0, d1, &#8230;, dn</strong>.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> np.random.randn()
<span class="fl">2.1923875335537315</span> <span class="co">#random</span>

<span class="co">#  N(3, 6.25=2.5^2) (you can use: sigma * np.random.randn(...) + mu)</span>
<span class="op">&gt;&gt;&gt;</span> <span class="fl">2.5</span> <span class="op">*</span> np.random.randn(<span class="dv">2</span>, <span class="dv">4</span>) <span class="op">+</span> <span class="dv">3</span>
array([[<span class="op">-</span><span class="fl">4.49401501</span>,  <span class="fl">4.00950034</span>, <span class="op">-</span><span class="fl">1.81814867</span>,  <span class="fl">7.29718677</span>],  <span class="co">#random</span>
       [ <span class="fl">0.39924804</span>,  <span class="fl">4.68456316</span>,  <span class="fl">4.99394529</span>,  <span class="fl">4.84057254</span>]]) <span class="co">#random</span></code></pre></div></li>
</ul>
w.r.t.
<ul>
<li>with respect to</li>
<li>with regard to</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy
<span class="im">import</span> theano
<span class="im">import</span> theano.tensor <span class="im">as</span> T
rng <span class="op">=</span> numpy.random

N <span class="op">=</span> <span class="dv">400</span>                                   <span class="co"># training sample size</span>
feats <span class="op">=</span> <span class="dv">784</span>                               <span class="co"># number of input variables</span>

<span class="co"># generate a dataset: D = (input_values, target_class)</span>
<span class="co">#</span>
<span class="co">#   input:             [N, feats] of N(0, 1),</span>
<span class="co">#   output:            [0, 2) -&gt; 0/1 (binary)</span>
<span class="co">#</span>
D <span class="op">=</span> (rng.randn(N, feats), rng.randint(size<span class="op">=</span>N, low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span><span class="dv">2</span>))

training_steps <span class="op">=</span> <span class="dv">10000</span>

<span class="co"># Declare Theano symbolic variables</span>
x <span class="op">=</span> T.matrix(<span class="st">&quot;x&quot;</span>)
y <span class="op">=</span> T.vector(<span class="st">&quot;y&quot;</span>)

<span class="co"># initialize the weight vector w randomly</span>
<span class="co">#</span>
<span class="co">#   this and the following bias variable b are shared so they</span>
<span class="co">#   keep their values between training iterations (updates)</span>
<span class="co">#</span>
w <span class="op">=</span> theano.shared(rng.randn(feats), name<span class="op">=</span><span class="st">&quot;w&quot;</span>)

<span class="co"># initialize the bias term</span>
b <span class="op">=</span> theano.shared(<span class="dv">0</span>., name<span class="op">=</span><span class="st">&quot;b&quot;</span>)

<span class="bu">print</span>(<span class="st">&quot;Initial model:&quot;</span>)
<span class="bu">print</span>(w.get_value())
<span class="bu">print</span>(b.get_value())

<span class="co"># Construct Theano expression graph</span>
p_1 <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> T.exp(<span class="op">-</span>T.dot(x, w) <span class="op">-</span> b))   <span class="co"># Probability that target = 1</span>
prediction <span class="op">=</span> p_1 <span class="op">&gt;</span> <span class="fl">0.5</span>                    <span class="co"># The prediction thresholded</span>

<span class="co"># Cross-entropy loss function</span>
xent <span class="op">=</span> <span class="op">-</span>y <span class="op">*</span> T.log(p_1) <span class="op">-</span> (<span class="dv">1</span><span class="op">-</span>y) <span class="op">*</span> T.log(<span class="dv">1</span><span class="op">-</span>p_1)
<span class="co"># The cost to minimize</span>
cost <span class="op">=</span> xent.mean() <span class="op">+</span> <span class="fl">0.01</span> <span class="op">*</span> (w <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>()
gw, gb <span class="op">=</span> T.grad(cost, [w, b])             <span class="co"># Compute the gradient of the cost</span>

<span class="co"># w.r.t weight vector w and</span>
<span class="co"># bias term b</span>
<span class="co"># (we shall return to this in a</span>
<span class="co"># following section of this tutorial)</span>

<span class="co">#                                                    &#12300;train &#20989;&#25968;&#12301;</span>
train <span class="op">=</span> theano.function(
          <span class="co"># Compile, &#36825;&#37096;&#20998;&#24456;&#26377;&#24847;&#24605;&#65292;&#30452;&#25509;&#29992;&#20102; input&#65292;output &#21644; updates</span>
          inputs<span class="op">=</span>[x,y],
          outputs<span class="op">=</span>[prediction, xent],     <span class="co"># &#20004;&#20010;&#23450;&#20041;&#22909;&#30340; descrimination func</span>
          <span class="co"># pairwise update, ((old1, new1), (old2, new2), ...)</span>
          updates<span class="op">=</span>((w, w <span class="op">-</span> <span class="fl">0.1</span> <span class="op">*</span> gw), (b, b <span class="op">-</span> <span class="fl">0.1</span> <span class="op">*</span> gb)))

<span class="co"># TODO1</span>

<span class="co">#                                                    &#12300;predict &#20989;&#25968;&#12301;</span>
predict <span class="op">=</span> theano.function(inputs<span class="op">=</span>[x], outputs<span class="op">=</span>prediction)

<span class="co"># Train</span>
<span class="cf">for</span> i <span class="op">in</span> <span class="bu">range</span>(training_steps):     <span class="co"># loop 10000 times</span>
    pred, err <span class="op">=</span> train(D, D)   <span class="co"># &#25968;&#25454;&#38598;&#65292;&#26679;&#26412;: D, label: D</span>

<span class="bu">print</span>(<span class="st">&quot;Final model:&quot;</span>)
<span class="bu">print</span>(w.get_value())
<span class="bu">print</span>(b.get_value())
<span class="bu">print</span>(<span class="st">&quot;target values for D:&quot;</span>)
<span class="bu">print</span>(D)
<span class="bu">print</span>(<span class="st">&quot;prediction on D:&quot;</span>)
<span class="bu">print</span>(predict(D))</code></pre></div>
<p><code>updates</code> (iterable over pairs <code>(shared_variable, new_expression)</code>. List, tuple or dict.) &#8211; expressions for new SharedVariable values.</p>
<p>refs and see also</p>
<ul>
<li><a href="http://deeplearning.net/software/theano/library/compile/function.html#function.function">function - defines theano.function &#8212; Theano 0.8.0 documentation</a></li>
</ul></li>
</ul></li>
<li><p>Derivatives in Theano <a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> <code class="fold">@</code></p>
<ul>
<li><p>Computing Gradients</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> numpy
<span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> theano
<span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> theano.tensor <span class="im">as</span> T
<span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> theano <span class="im">import</span> pp
<span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> T.dscalar(<span class="st">&#39;x&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> y <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span>
<span class="op">&gt;&gt;&gt;</span> gy <span class="op">=</span> T.grad(y, x)
<span class="co"># TODO? &#30475;&#19981;&#25026;&#36825;&#20010; output</span>
<span class="op">&gt;&gt;&gt;</span> pp(gy)  <span class="co"># print out the gradient prior to optimization</span>
<span class="co">&#39;((fill((x ** TensorConstant{2}), TensorConstant{1.0}) * TensorConstant{2}) * (x ** (TensorConstant{2} - TensorConstant{1})))&#39;</span>
<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> theano.function([x], gy)
<span class="op">&gt;&gt;&gt;</span> f(<span class="dv">4</span>)
array(<span class="fl">8.0</span>)
<span class="op">&gt;&gt;&gt;</span> numpy.allclose(f(<span class="fl">94.2</span>), <span class="fl">188.4</span>)
<span class="va">True</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> T.dmatrix(<span class="st">&#39;x&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> s <span class="op">=</span> T.<span class="bu">sum</span>(<span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> T.exp(<span class="op">-</span>x)))      <span class="co"># x is a matrix!</span>
<span class="op">&gt;&gt;&gt;</span> gs <span class="op">=</span> T.grad(s, x)
<span class="op">&gt;&gt;&gt;</span> dlogistic <span class="op">=</span> theano.function([x], gs)
<span class="op">&gt;&gt;&gt;</span> dlogistic([[<span class="dv">0</span>, <span class="dv">1</span>], [<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>]])
array([[ <span class="fl">0.25</span>      ,  <span class="fl">0.19661193</span>],
       [ <span class="fl">0.19661193</span>,  <span class="fl">0.10499359</span>]])</code></pre></div></li>
<li><p>Computing the Jacobian ??</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> theano
<span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> theano.tensor <span class="im">as</span> T
<span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> T.dvector(<span class="st">&#39;x&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> y <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span>
<span class="op">&gt;&gt;&gt;</span> J, updates <span class="op">=</span> theano.scan( <span class="kw">lambda</span> i, y,x : T.grad(y[i], x),
                              sequences<span class="op">=</span>T.arange(y.shape),
                              non_sequences<span class="op">=</span>[y,x] )

    <span class="co"># scan automatically concatenates all these rows, generating a</span>
    <span class="co"># matrix which corresponds to the Jacobian</span>

<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> theano.function([x], J, updates<span class="op">=</span>updates)
<span class="op">&gt;&gt;&gt;</span> f([<span class="dv">4</span>, <span class="dv">4</span>])
array([[ <span class="dv">8</span>.,  <span class="dv">0</span>.],
       [ <span class="dv">0</span>.,  <span class="dv">8</span>.]])</code></pre></div></li>
<li><p>Computing the Hessian</p>
<p>The Hessian matrix can be considered related to the Jacobian matrix by <span class="math inline">\(H(f)(x)=J(&#8711;f)(x)H(f)(x)=J(&#8711;f)(x)\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> T.dvector(<span class="st">&#39;x&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> y <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span>
<span class="op">&gt;&gt;&gt;</span> cost <span class="op">=</span> y.<span class="bu">sum</span>()
<span class="op">&gt;&gt;&gt;</span> gy <span class="op">=</span> T.grad(cost, x)
<span class="op">&gt;&gt;&gt;</span> H, updates <span class="op">=</span> theano.scan( <span class="kw">lambda</span> i, gy,x : T.grad(gy[i], x),
                              sequences<span class="op">=</span>T.arange(gy.shape),
                              non_sequences<span class="op">=</span>[gy, x] )
<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> theano.function([x], H, updates<span class="op">=</span>updates)
<span class="op">&gt;&gt;&gt;</span> f([<span class="dv">4</span>, <span class="dv">4</span>])
array([[ <span class="dv">2</span>.,  <span class="dv">0</span>.],
       [ <span class="dv">0</span>.,  <span class="dv">2</span>.]])</code></pre></div></li>
<li><p>Jacobian times a Vector</p>
<p>Compared to evaluating the Jacobian and then doing the product, there are methods that compute the desired results while avoiding actual evaluation of the Jacobian. This can bring about significant performance gains. A description of one such algorithm can be found here:</p>
<p>Barak A. Pearlmutter,<br />
<em>&#8220;Fast Exact Multiplication by the Hessian&#8221;</em>,<br />
Neural Computation, 1994</p>
<ul>
<li><p>R-operator</p>
<p>The R operator is built to evaluate the product between a Jacobian and a vector, namely <span class="math inline">\(\frac{\partial f(x)}{\partial x} v\)</span>.</p>
<p>The formulation can be extended even for x being a matrix, or a tensor in general, case in which also the Jacobian becomes a tensor and the product becomes some kind of tensor product. Because in practice we end up needing to compute such expressions in terms of weight matrices, Theano supports this more generic form of the operation. In order to evaluate the R-operation of expression y, with respect to x, multiplying the Jacobian with v you need to do something similar to this:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> W <span class="op">=</span> T.dmatrix(<span class="st">&#39;W&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> V <span class="op">=</span> T.dmatrix(<span class="st">&#39;V&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> T.dvector(<span class="st">&#39;x&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> y <span class="op">=</span> T.dot(x, W)
<span class="op">&gt;&gt;&gt;</span> JV <span class="op">=</span> T.Rop(y, W, V)
<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> theano.function([W, V, x], JV)
<span class="op">&gt;&gt;&gt;</span> f([[<span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">1</span>]], [[<span class="dv">2</span>, <span class="dv">2</span>], [<span class="dv">2</span>, <span class="dv">2</span>]], [<span class="dv">0</span>,<span class="dv">1</span>])
array([ <span class="dv">2</span>.,  <span class="dv">2</span>.])</code></pre></div></li>
<li><p>L-operator</p>
<p>In similitude to the R-operator, the L-operator would compute a row vector times the Jacobian. The mathematical formula would be v . The L-operator is also supported for generic tensors (not only for vectors). Similarly, it can be implemented as follows:</p>
<p>similitude, <code>[s&#618;'m&#618;l&#618;tju:d]</code>, n.&#30456;&#20284;&#65307;&#31867;&#20284;&#65307;&#30456;&#20223;</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> W <span class="op">=</span> T.dmatrix(<span class="st">&#39;W&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> v <span class="op">=</span> T.dvector(<span class="st">&#39;v&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> T.dvector(<span class="st">&#39;x&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> y <span class="op">=</span> T.dot(x, W)
<span class="op">&gt;&gt;&gt;</span> VJ <span class="op">=</span> T.Lop(y, W, v)
<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> theano.function([v,x], VJ)
<span class="op">&gt;&gt;&gt;</span> f([<span class="dv">2</span>, <span class="dv">2</span>], [<span class="dv">0</span>, <span class="dv">1</span>])
array([[ <span class="dv">0</span>.,  <span class="dv">0</span>.],
       [ <span class="dv">2</span>.,  <span class="dv">2</span>.]])</code></pre></div></li>
<li><p>v, the point of evaluation</p>
<p>differs between the L-operator and the R-operator.</p></li>
</ul></li>
<li><p>Hessian times a Vector</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> T.dvector(<span class="st">&#39;x&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> v <span class="op">=</span> T.dvector(<span class="st">&#39;v&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> y <span class="op">=</span> T.<span class="bu">sum</span>(x <span class="op">**</span> <span class="dv">2</span>)
<span class="op">&gt;&gt;&gt;</span> gy <span class="op">=</span> T.grad(y, x)
<span class="op">&gt;&gt;&gt;</span> vH <span class="op">=</span> T.grad(T.<span class="bu">sum</span>(gy <span class="op">*</span> v), x)
<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> theano.function([x, v], vH)
<span class="op">&gt;&gt;&gt;</span> f([<span class="dv">4</span>, <span class="dv">4</span>], [<span class="dv">2</span>, <span class="dv">2</span>])
array([ <span class="dv">4</span>.,  <span class="dv">4</span>.])

<span class="co"># or, making use of the R-operator:</span>
<span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> T.dvector(<span class="st">&#39;x&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> v <span class="op">=</span> T.dvector(<span class="st">&#39;v&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> y <span class="op">=</span> T.<span class="bu">sum</span>(x <span class="op">**</span> <span class="dv">2</span>)
<span class="op">&gt;&gt;&gt;</span> gy <span class="op">=</span> T.grad(y, x)
<span class="op">&gt;&gt;&gt;</span> Hv <span class="op">=</span> T.Rop(gy, x, v)
<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> theano.function([x, v], Hv)
<span class="op">&gt;&gt;&gt;</span> f([<span class="dv">4</span>, <span class="dv">4</span>], [<span class="dv">2</span>, <span class="dv">2</span>])
array([ <span class="dv">4</span>.,  <span class="dv">4</span>.])</code></pre></div></li>
<li><p>Final Pointers</p>
<ul>
<li>The grad function works symbolically: it receives and returns Theano variables.</li>
<li>grad can be compared to a macro since it can be applied repeatedly.</li>
<li>Scalar costs only can be directly handled by grad. Arrays are handled through repeated applications.</li>
<li>Built-in functions allow to compute efficiently vector times Jacobian and vector times Hessian.</li>
<li>Work is in progress on the optimizations required to compute efficiently the full Jacobian and the Hessian matrix as well as the Jacobian times vector.</li>
</ul></li>
</ul></li>
<li><p>Conditions <a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> <code class="fold">@</code></p>
<ul>
<li><p>IfElse vs Switch</p>
<ul>
<li>ifelse, binary, lazy</li>
<li>switch, more general</li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> theano <span class="im">import</span> tensor <span class="im">as</span> T
<span class="im">from</span> theano.ifelse <span class="im">import</span> ifelse
<span class="im">import</span> theano, time, numpy

a,b <span class="op">=</span> T.scalars(<span class="st">&#39;a&#39;</span>, <span class="st">&#39;b&#39;</span>)
x,y <span class="op">=</span> T.matrices(<span class="st">&#39;x&#39;</span>, <span class="st">&#39;y&#39;</span>)

z_switch <span class="op">=</span> T.switch(T.lt(a, b), T.mean(x), T.mean(y))
z_lazy <span class="op">=</span> ifelse(T.lt(a, b), T.mean(x), T.mean(y))

f_switch <span class="op">=</span> theano.function([a, b, x, y], z_switch,
                           mode<span class="op">=</span>theano.Mode(linker<span class="op">=</span><span class="st">&#39;vm&#39;</span>))
f_lazyifelse <span class="op">=</span> theano.function([a, b, x, y], z_lazy,
                               mode<span class="op">=</span>theano.Mode(linker<span class="op">=</span><span class="st">&#39;vm&#39;</span>))

val1 <span class="op">=</span> <span class="dv">0</span>.
val2 <span class="op">=</span> <span class="dv">1</span>.
big_mat1 <span class="op">=</span> numpy.ones((<span class="dv">10000</span>, <span class="dv">1000</span>))
big_mat2 <span class="op">=</span> numpy.ones((<span class="dv">10000</span>, <span class="dv">1000</span>))

n_times <span class="op">=</span> <span class="dv">10</span>

tic <span class="op">=</span> time.clock()
<span class="cf">for</span> i <span class="op">in</span> <span class="bu">range</span>(n_times):
    f_switch(val1, val2, big_mat1, big_mat2)
<span class="bu">print</span>(<span class="st">&#39;time spent evaluating both values </span><span class="sc">%f</span><span class="st"> sec&#39;</span> <span class="op">%</span> (time.clock() <span class="op">-</span> tic))

tic <span class="op">=</span> time.clock()
<span class="cf">for</span> i <span class="op">in</span> <span class="bu">range</span>(n_times):
    f_lazyifelse(val1, val2, big_mat1, big_mat2)    <span class="co"># faster</span>
<span class="bu">print</span>(<span class="st">&#39;time spent evaluating one value </span><span class="sc">%f</span><span class="st"> sec&#39;</span> <span class="op">%</span> (time.clock() <span class="op">-</span> tic))</code></pre></div>
<p>There is no automatic optimization replacing a switch with a broadcasted scalar to an ifelse, as this is not always faster.</p></li>
</ul></li>
<li><p>Loop <code class="fold">@</code></p>
<ul>
<li><p>Scan</p>
<p>what is scan, scan vs for loop</p>
<ul>
<li>scan, recurrence, for looping</li>
<li>reduction, map are scans</li>
<li>scan is more than loop, and faster</li>
<li>can alse computes gradients through sequential steps</li>
<li>lower memory usage</li>
</ul>
<p>examples</p>
<ul>
<li><p>Scan Example:</p>
<p><strong>Computing the sequence x(t) = tanh(x(t - 1).dot(W) + y(t).dot(U) + p(T - t).dot(V))</strong></p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> theano
<span class="im">import</span> theano.tensor <span class="im">as</span> T
<span class="im">import</span> numpy <span class="im">as</span> np

<span class="co"># define tensor variables</span>
X <span class="op">=</span> T.vector(<span class="st">&quot;X&quot;</span>)
W <span class="op">=</span> T.matrix(<span class="st">&quot;W&quot;</span>)
b_sym <span class="op">=</span> T.vector(<span class="st">&quot;b_sym&quot;</span>)
U <span class="op">=</span> T.matrix(<span class="st">&quot;U&quot;</span>)
Y <span class="op">=</span> T.matrix(<span class="st">&quot;Y&quot;</span>)
V <span class="op">=</span> T.matrix(<span class="st">&quot;V&quot;</span>)
P <span class="op">=</span> T.matrix(<span class="st">&quot;P&quot;</span>)

results, updates <span class="op">=</span> theano.scan(<span class="kw">lambda</span> y, p, x_tm1: T.tanh(T.dot(x_tm1, W) <span class="op">+</span> T.dot(y, U) <span class="op">+</span> T.dot(p, V)),
          sequences<span class="op">=</span>[Y, P[::<span class="op">-</span><span class="dv">1</span>]], outputs_info<span class="op">=</span>[X])
compute_seq <span class="op">=</span> theano.function(inputs<span class="op">=</span>[X, W, Y, U, P, V], outputs<span class="op">=</span>results)

<span class="co"># test values</span>
x <span class="op">=</span> np.zeros((<span class="dv">2</span>), dtype<span class="op">=</span>theano.config.floatX)
x <span class="op">=</span> <span class="dv">1</span>
w <span class="op">=</span> np.ones((<span class="dv">2</span>, <span class="dv">2</span>), dtype<span class="op">=</span>theano.config.floatX)
y <span class="op">=</span> np.ones((<span class="dv">5</span>, <span class="dv">2</span>), dtype<span class="op">=</span>theano.config.floatX)
y[<span class="dv">0</span>, :] <span class="op">=</span> <span class="op">-</span><span class="dv">3</span>
u <span class="op">=</span> np.ones((<span class="dv">2</span>, <span class="dv">2</span>), dtype<span class="op">=</span>theano.config.floatX)
p <span class="op">=</span> np.ones((<span class="dv">5</span>, <span class="dv">2</span>), dtype<span class="op">=</span>theano.config.floatX)
p[<span class="dv">0</span>, :] <span class="op">=</span> <span class="dv">3</span>
v <span class="op">=</span> np.ones((<span class="dv">2</span>, <span class="dv">2</span>), dtype<span class="op">=</span>theano.config.floatX)

<span class="bu">print</span>(compute_seq(x, w, y, u, p, v))

<span class="co"># comparison with numpy</span>
x_res <span class="op">=</span> np.zeros((<span class="dv">5</span>, <span class="dv">2</span>), dtype<span class="op">=</span>theano.config.floatX)
x_res <span class="op">=</span> np.tanh(x.dot(w) <span class="op">+</span> y.dot(u) <span class="op">+</span> p.dot(v))
<span class="cf">for</span> i <span class="op">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">5</span>):
    x_res[i] <span class="op">=</span> np.tanh(x_res[i <span class="op">-</span> <span class="dv">1</span>].dot(w) <span class="op">+</span> y[i].dot(u) <span class="op">+</span> p[<span class="dv">4</span><span class="op">-</span>i].dot(v))
<span class="bu">print</span>(x_res)

[[<span class="op">-</span><span class="fl">0.99505475</span> <span class="op">-</span><span class="fl">0.99505475</span>]
 [ <span class="fl">0.96471973</span>  <span class="fl">0.96471973</span>]
 [ <span class="fl">0.99998585</span>  <span class="fl">0.99998585</span>]
 [ <span class="fl">0.99998771</span>  <span class="fl">0.99998771</span>]
 [ <span class="dv">1</span>.          <span class="dv">1</span>.        ]]
[[<span class="op">-</span><span class="fl">0.99505475</span> <span class="op">-</span><span class="fl">0.99505475</span>]
 [ <span class="fl">0.96471973</span>  <span class="fl">0.96471973</span>]
 [ <span class="fl">0.99998585</span>  <span class="fl">0.99998585</span>]
 [ <span class="fl">0.99998771</span>  <span class="fl">0.99998771</span>]
 [ <span class="dv">1</span>.          <span class="dv">1</span>.        ]]</code></pre></div></li>
<li><p>Scan Example: Computing norms of lines of X</p></li>
<li><p>Scan Example: Computing norms of columns of X</p></li>
<li><p>Scan Example: Computing trace of X</p></li>
<li><p>Scan Example: Computing the sequence x(t) = x(t - 2).dot(U) + x(t - 1).dot(V) + tanh(x(t - 1).dot(W) + b)</p></li>
<li><p>Scan Example: Computing the Jacobian of y = tanh(v.dot(A)) wrt x</p></li>
<li><p>Scan Example: Accumulate number of loop during a scan</p></li>
<li><p>Scan Example: Computing tanh(v.dot(W) + b) * d where d is binomial</p></li>
<li><p>Scan Example: Computing pow(A, k)</p></li>
<li><p>Scan Example: Calculating a Polynomial</p></li>
</ul></li>
<li><p>Exercise</p></li>
</ul></li>
<li><p>How Shape Information is Handled by Theano</p>
<ol style="list-style-type: decimal">
<li><p>shape is known in advance;</p></li>
<li><p>know only the shape, not the actual value of a variable. (This is done with the <code>Op.infer_shape</code> method.)</p></li>
</ol>
<ul>
<li><p>Shape Inference Problem</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> theano
<span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> theano.tensor.matrix(<span class="st">&#39;x&#39;</span>)
<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> theano.function([x], (x <span class="op">**</span> <span class="dv">2</span>).shape)
<span class="op">&gt;&gt;&gt;</span> theano.printing.debugprint(f)
MakeVector{dtype<span class="op">=</span><span class="st">&#39;int64&#39;</span>} [<span class="bu">id</span> A] <span class="st">&#39;&#39;</span>   <span class="dv">2</span>
 <span class="op">|</span>Shape_i{<span class="dv">0</span>} [<span class="bu">id</span> B] <span class="st">&#39;&#39;</span>   <span class="dv">1</span>
 <span class="op">|</span> <span class="op">|</span>x [<span class="bu">id</span> C]
 <span class="op">|</span>Shape_i{<span class="dv">1</span>} [<span class="bu">id</span> D] <span class="st">&#39;&#39;</span>   <span class="dv">0</span>
   <span class="op">|</span>x [<span class="bu">id</span> C]</code></pre></div>
<p>&#36825;&#31181;&#22270;&#30340;&#35828;&#26126;&#35265; <a href="#theano-graph-printing">printing &#8211; Graph Printing and Symbolic Print Statement</a>.</p></li>
<li><p>Specifing Exact Shape</p>
<p><strong>1</strong></p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co">#             convolution 2dim</span>
theano.tensor.nnet.conv2d( ...,
                           image_shape<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">5</span>),
                           filter_shape<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">4</span>) )</code></pre></div>
<p><code>signal.conv.conv2d</code> performs a basic 2D convolution of the input with the given filters. The input parameter can be a single 2D image or a 3D tensor, containing a set of images. Similarly, filters can be a single 2D filter or a 3D tensor, corresponding to a set of 2D filters.</p>
<p>Shape parameters are optional and will result in faster execution.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">theano.tensor.nnet.conv.conv2d( <span class="bu">input</span>, filters,
                                image_shape<span class="op">=</span><span class="va">None</span>,
                                filter_shape<span class="op">=</span><span class="va">None</span>,
                                border_mode<span class="op">=</span><span class="st">&#39;valid&#39;</span>,
                                subsample<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>),
                                <span class="op">**</span>kargs )</code></pre></div>
<p>Deprecated, old conv2d interface. This function will build the symbolic graph for convolving a stack of input images with a set of filters. The implementation is modelled after Convolutional Neural Networks (CNN). It is simply a wrapper to the ConvOp but provides a much cleaner interface.</p>
<p><strong>2</strong></p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> theano
<span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> theano.tensor.matrix()
<span class="op">&gt;&gt;&gt;</span> x_specify_shape <span class="op">=</span> theano.tensor.specify_shape(x, (<span class="dv">2</span>, <span class="dv">2</span>))
<span class="op">&gt;&gt;&gt;</span> f <span class="op">=</span> theano.function([x], (x_specify_shape <span class="op">**</span> <span class="dv">2</span>).shape)
<span class="op">&gt;&gt;&gt;</span> theano.printing.debugprint(f)
DeepCopyOp [<span class="bu">id</span> A] <span class="st">&#39;&#39;</span>   <span class="dv">0</span>
 <span class="op">|</span>TensorConstant{(<span class="dv">2</span>,) of <span class="dv">2</span>} [<span class="bu">id</span> B]</code></pre></div></li>
<li><p>Future Plans</p>
<p>nil.</p></li>
</ul></li>
</ul></li>
<li><p>Advanced <code class="fold">@</code></p>
<ul>
<li><p>Sparse <code class="fold">@</code></p>
<ul>
<li><p>Compressed Sparse Format</p>
<ul>
<li>Which format should I use?</li>
</ul></li>
<li><p>Handling Sparse in Theano</p>
<ul>
<li>To and Fro</li>
<li>Properties and Construction</li>
<li>Structured Operation</li>
<li>Gradient</li>
</ul></li>
</ul></li>
<li><p>Using the GPU <code class="fold">@</code></p>
<ul>
<li><p>CUDA backend</p>
<ul>
<li>Testing Theano with GPU</li>
<li>Returning a Handle to Device-Allocated Data</li>
<li>What Can Be Accelerated on the GPU</li>
<li>Tips for Improving Performance on GPU</li>
<li>GPU Async capabilities</li>
<li><p>Changing the Value of Shared Variables</p>
<ul>
<li>Exercise</li>
</ul></li>
</ul></li>
<li><p>GpuArray Backend</p>
<ul>
<li>Testing Theano with GPU</li>
<li>Returning a Handle to Device-Allocated Data</li>
<li>What Can be Accelerated on the GPU</li>
<li>GPU Async Capabilities</li>
</ul></li>
<li>Software for Directly Programming a GPU</li>
<li><p>Learning to Program with PyCUDA</p>
<ul>
<li>Exercise</li>
<li>Exercise</li>
</ul></li>
<li><p>Note</p></li>
</ul></li>
<li><p>Using multiple GPUs <code class="fold">@</code></p>
<ul>
<li>Defining the context map</li>
<li>A simple graph on two GPUs</li>
<li>Explicit transfers of data</li>
</ul></li>
</ul></li>
</ul>
<p>refs and see also</p>
<ul>
<li><a href="http://deeplearning.net/software/theano/tutorial/python-memory-management.html#python-memory-management">Python Memory Management &#8212; Theano 0.8.0 documentation</a></li>
<li><a href="http://www.deeplearning.net/tutorial/lstm.html">LSTM Networks for Sentiment Analysis &#8212; DeepLearning 0.1 documentation</a></li>
</ul>
<dl>
<dt>printing &#8211; Graph Printing and Symbolic Print Statement <code id="theano-graph-printing" class="tzx-anchor">@</code> <code class="fold">@</code></dt>
<dd><p>&#21451;&#22909;&#30340;&#25171;&#21360;&#32467;&#26524;&#65306;</p>
</dd>
</dl>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> theano.printing.pprint(prediction)
<span class="co">&#39;gt((TensorConstant{1} / (TensorConstant{1} + exp(((-(x \\dot w)) - b)))),</span>
<span class="co">TensorConstant{0.5})&#39;</span></code></pre></div>
<p>&#35843;&#35797;&#25171;&#21360;</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> theano.printing.debugprint(prediction)
    Elemwise{gt,no_inplace} [@A] <span class="st">&#39;&#39;</span>
    <span class="op">|</span>Elemwise{true_div,no_inplace} [@B] <span class="st">&#39;&#39;</span>
    <span class="op">|</span> <span class="op">|</span>DimShuffle{x} [@C] <span class="st">&#39;&#39;</span>
    <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="dv">1</span>} [@D]
    <span class="op">|</span> <span class="op">|</span>Elemwise{add,no_inplace} [@E] <span class="st">&#39;&#39;</span>
    <span class="op">|</span>   <span class="op">|</span>DimShuffle{x} [@F] <span class="st">&#39;&#39;</span>
    <span class="op">|</span>   <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="dv">1</span>} [@D]
    <span class="op">|</span>   <span class="op">|</span>Elemwise{exp,no_inplace} [@G] <span class="st">&#39;&#39;</span>
    <span class="op">|</span>     <span class="op">|</span>Elemwise{sub,no_inplace} [@H] <span class="st">&#39;&#39;</span>
    <span class="op">|</span>       <span class="op">|</span>Elemwise{neg,no_inplace} [@I] <span class="st">&#39;&#39;</span>
    <span class="op">|</span>       <span class="op">|</span> <span class="op">|</span>dot [@J] <span class="st">&#39;&#39;</span>
    <span class="op">|</span>       <span class="op">|</span>   <span class="op">|</span>x [@K]
    <span class="op">|</span>       <span class="op">|</span>   <span class="op">|</span>w [@L]
    <span class="op">|</span>       <span class="op">|</span>DimShuffle{x} [@M] <span class="st">&#39;&#39;</span>
    <span class="op">|</span>         <span class="op">|</span>b [@N]
    <span class="op">|</span>DimShuffle{x} [@O] <span class="st">&#39;&#39;</span>
      <span class="op">|</span>TensorConstant{<span class="fl">0.5</span>} [@P]</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> theano.printing.debugprint(predict)
    Elemwise{Composite{GT(scalar_sigmoid((<span class="op">-</span>((<span class="op">-</span>i0) <span class="op">-</span> i1))), i2)}} [@A] <span class="st">&#39;&#39;</span>   <span class="dv">4</span>
     <span class="op">|</span>CGemv{inplace} [@B] <span class="st">&#39;&#39;</span>   <span class="dv">3</span>
     <span class="op">|</span> <span class="op">|</span>Alloc [@C] <span class="st">&#39;&#39;</span>   <span class="dv">2</span>
     <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="fl">0.0</span>} [@D]
     <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>Shape_i{<span class="dv">0</span>} [@E] <span class="st">&#39;&#39;</span>   <span class="dv">1</span>
     <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span>x [@F]
     <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="fl">1.0</span>} [@G]
     <span class="op">|</span> <span class="op">|</span>x [@F]
     <span class="op">|</span> <span class="op">|</span>w [@H]
     <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="fl">0.0</span>} [@D]
     <span class="op">|</span>InplaceDimShuffle{x} [@I] <span class="st">&#39;&#39;</span>   <span class="dv">0</span>
     <span class="op">|</span> <span class="op">|</span>b [@J]
     <span class="op">|</span>TensorConstant{(<span class="dv">1</span>,) of <span class="fl">0.5</span>} [@K]</code></pre></div>
<p>graph&#30340;&#22270;&#29255;&#25171;&#21360;</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> theano.printing.pydotprint(prediction, outfile<span class="op">=</span><span class="st">&quot;pics/logreg_pydotprint_prediction.png&quot;</span>, var_with_name_simple<span class="op">=</span><span class="va">True</span>)
The output <span class="bu">file</span> <span class="op">is</span> available at pics<span class="op">/</span>logreg_pydotprint_prediction.png</code></pre></div>
<div class="figure">
<img src="http://deeplearning.net/software/theano/_images/logreg_pydotprint_prediction2.png" />

</div>
<p>refs and see also</p>
<ul>
<li><a href="http://www.cnblogs.com/shouhuxianjian/p/4590231.html">Theano2.1.5-&#22522;&#30784;&#30693;&#35782;&#20043;&#25171;&#21360;&#20986;theano&#30340;&#22270; - &#20185;&#23432; - &#21338;&#23458;&#22253;</a></li>
<li><a href="http://deeplearning.net/software/theano/tutorial/printing_drawing.html">Printing/Drawing Theano graphs &#8212; Theano 0.8.0 documentation</a></li>
<li><a href="http://deeplearning.net/software/theano/library/printing.html#libdoc-printing">printing &#8211; Graph Printing and Symbolic Print Statement &#8212; Theano 0.8.0 documentation</a></li>
</ul>
</dd>
</dl>
<hr />
<dl>
<dt><a href="http://tensorfly.cn/">TensorFlow&#20013;&#25991;&#31038;&#21306;-&#39318;&#39029;</a> <code class="fold">@</code></dt>
<dd><p>TensorFlow&#8482; &#26159;&#19968;&#20010;&#37319;&#29992;&#25968;&#25454;&#27969;&#22270;&#65288;data flow graphs&#65289;&#65292;&#29992;&#20110;&#25968;&#20540;&#35745;&#31639;&#30340;&#24320;&#28304;&#36719;&#20214;&#24211;&#12290;&#33410;&#28857;&#65288;Nodes&#65289;&#22312;&#22270;&#20013;&#34920;&#31034;&#25968;&#23398;&#25805;&#20316;&#65292;&#22270;&#20013;&#30340;&#32447;&#65288;edges&#65289;&#21017;&#34920;&#31034;&#22312;&#33410;&#28857;&#38388;&#30456;&#20114;&#32852;&#31995;&#30340;&#22810;&#32500;&#25968;&#25454;&#25968;&#32452;&#65292;&#21363;&#24352;&#37327;&#65288;tensor&#65289;&#12290;&#23427;&#28789;&#27963;&#30340;&#26550;&#26500;&#35753;&#20320;&#21487;&#20197;&#22312;&#22810;&#31181;&#24179;&#21488;&#19978;&#23637;&#24320;&#35745;&#31639;&#65292;&#20363;&#22914;&#21488;&#24335;&#35745;&#31639;&#26426;&#20013;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;CPU&#65288;&#25110;GPU&#65289;&#65292;&#26381;&#21153;&#22120;&#65292;&#31227;&#21160;&#35774;&#22791;&#31561;&#31561;&#12290; TensorFlow &#26368;&#21021;&#30001;Google&#22823;&#33041;&#23567;&#32452;&#65288;&#38582;&#23646;&#20110;Google&#26426;&#22120;&#26234;&#33021;&#30740;&#31350;&#26426;&#26500;&#65289;&#30340;&#30740;&#31350;&#21592;&#21644;&#24037;&#31243;&#24072;&#20204;&#24320;&#21457;&#20986;&#26469;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#20294;&#36825;&#20010;&#31995;&#32479;&#30340;&#36890;&#29992;&#24615;&#20351;&#20854;&#20063;&#21487;&#24191;&#27867;&#29992;&#20110;&#20854;&#20182;&#35745;&#31639;&#39046;&#22495;&#12290;</p>
<dl>
<dt>&#20160;&#20040;&#26159;&#25968;&#25454;&#27969;&#22270;&#65288;Data Flow Graph&#65289;? <code class="fold">@</code></dt>
<dd><p>&#25968;&#25454;&#27969;&#22270;&#29992;&#8220;&#32467;&#28857;&#8221;&#65288;nodes&#65289;&#21644;&#8220;&#32447;&#8221;(edges)&#30340;&#26377;&#21521;&#22270;&#26469;&#25551;&#36848;&#25968;&#23398;&#35745;&#31639;&#12290;&#8220;&#33410;&#28857;&#8221; &#19968;&#33324;&#29992;&#26469;&#34920;&#31034;&#26045;&#21152;&#30340;&#25968;&#23398;&#25805;&#20316;&#65292;&#20294;&#20063;&#21487;&#20197;&#34920;&#31034;&#25968;&#25454;&#36755;&#20837;&#65288;feed in&#65289;&#30340;&#36215;&#28857;/&#36755;&#20986;&#65288; push out&#65289;&#30340;&#32456;&#28857;&#65292;&#25110;&#32773;&#26159;&#35835;&#21462;/&#20889;&#20837;&#25345;&#20037;&#21464;&#37327;&#65288;persistent variable&#65289;&#30340;&#32456;&#28857;&#12290;&#8220;&#32447;&#8221;&#34920;&#31034;&#8220;&#33410;&#28857;&#8221;&#20043;&#38388;&#30340;&#36755;&#20837;/&#36755;&#20986;&#20851;&#31995;&#12290;&#36825;&#20123;&#25968;&#25454;&#8220;&#32447;&#8221;&#21487;&#20197;&#36755;&#36816;&#8220;size&#21487;&#21160;&#24577;&#35843;&#25972;&#8221;&#30340;&#22810;&#32500;&#25968;&#25454;&#25968;&#32452;&#65292;&#21363;&#8220;&#24352;&#37327;&#8221;&#65288;tensor&#65289;&#12290;&#24352;&#37327;&#20174;&#22270;&#20013;&#27969;&#36807;&#30340;&#30452;&#35266;&#22270;&#20687;&#26159;&#36825;&#20010;&#24037;&#20855;&#21462;&#21517;&#20026;&#8220;Tensorflow&#8221;&#30340;&#21407;&#22240;&#12290;&#19968;&#26086;&#36755;&#20837;&#31471;&#30340;&#25152;&#26377;&#24352;&#37327;&#20934;&#22791;&#22909;&#65292;&#33410;&#28857;&#23558;&#34987;&#20998;&#37197;&#21040;&#21508;&#31181;&#35745;&#31639;&#35774;&#22791;&#23436;&#25104;&#24322;&#27493;&#24182;&#34892;&#22320;&#25191;&#34892;&#36816;&#31639;&#12290;</p>
<div class="figure">
<img src="http://tensorfly.cn/images/tensors_flowing.gif" />

</div>
</dd>
</dl>
<p><a href="http://tensorfly.cn/tfdoc/get_started/introduction.html">&#20171;&#32461; | TensorFlow &#23448;&#26041;&#25991;&#26723;&#20013;&#25991;&#29256;</a></p>
<p>refs and see also</p>
<ul>
<li><a href="http://tensorfly.cn/tfdoc/get_started/introduction.html">&#20171;&#32461; | TensorFlow &#23448;&#26041;&#25991;&#26723;&#20013;&#25991;&#29256;</a></li>
<li><a href="http://tensorfly.cn/tfdoc/tutorials/mnist_beginners.html">MNIST &#20837;&#38376; | TensorFlow &#23448;&#26041;&#25991;&#26723;&#20013;&#25991;&#29256;</a></li>
<li><a href="http://tensorfly.cn/tfdoc/get_started/os_setup.html">&#19979;&#36733;&#21450;&#23433;&#35013; | TensorFlow &#23448;&#26041;&#25991;&#26723;&#20013;&#25991;&#29256;</a></li>
</ul>
</dd>
<dt><a href="http://playground.tensorflow.org/#activation=sigmoid&amp;batchSize=14&amp;dataset=xor&amp;regDataset=reg-plane&amp;learningRate=0.3&amp;regularizationRate=0&amp;noise=5&amp;networkShape=3,8,8,8,4,2&amp;seed=0.40288&amp;showTestData=true&amp;discretize=false&amp;percTrainData=70&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;discretize_hide=false">A Neural Network Playground</a> <code class="fold">@</code></dt>
<dd><p>torch -&gt; tensorflow</p>
</dd>
</dl>
<p><a href="http://neuralnetworksanddeeplearning.com/index.html">Neural networks and deep learning</a></p>
<dl>
<dt>AND, OR, and NAND.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> <code class="fold">@</code></dt>
<dd><p>NAND gate (negative-AND). The function NAND(a1, a2, &#8230;, an) is logically equivalent to NOT(a1 AND a2 AND &#8230; AND an).</p>
<table style="width:22%;">
<colgroup>
<col width="4%" />
<col width="6%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">A</th>
<th align="center">B</th>
<th>A NAND B</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">0 0 1 1</td>
<td align="center">0 1 0 1</td>
<td>1 1 1 0</td>
</tr>
</tbody>
</table>
<p>a NAND example</p>
<div class="figure">
<img src="http://neuralnetworksanddeeplearning.com/images/tikz2.png" />

</div>
<ul>
<li>input <code>00</code> -&gt; <code>(&#8722;2)&#8727;0+(&#8722;2)&#8727;0+3= 3</code> -&gt; output: 3</li>
<li>input <code>11</code> -&gt; <code>(&#8722;2)&#8727;1+(&#8722;2)&#8727;1+3=-1</code> -&gt; output: -1</li>
</ul>
<p>sigmoid: This shape is a smoothed out version of a step function.</p>
<p>output would be 1 or 0 depending on whether w&#8901;x+b was positive or negative</p>
<p>28 by 28 pixel images of scanned handwritten digits, and so the input layer contains 784=28&#215;28</p>
<div class="figure">
<img src="http://neuralnetworksanddeeplearning.com/images/tikz12.png" />

</div>
<p>Supposing the neural network functions in this way, we can give a plausible explanation for why it&#8217;s better to have 10 outputs from the network, rather than 4. If we had 4 outputs, then the first output neuron would be trying to decide what the most significant bit of the digit was. And there&#8217;s no easy way to relate that most significant bit to simple shapes like those shown above. It&#8217;s hard to imagine that there&#8217;s any good historical reason the component shapes of the digit will be closely related to (say) the most significant bit in the output.</p>
</dd>
<dt><a href="https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#pip-installation">Download and Setup</a> <code class="fold">@</code></dt>
<dd><div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">sudo</span> pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl
<span class="kw">python</span> -c <span class="st">&#39;import os; import inspect; import tensorflow; print(os.path.dirname(inspect.getfile(tensorflow)))&#39;</span>
<span class="kw">/usr/local/lib/python2.7/dist-packages/tensorflow</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="co"># Using &#39;python -m&#39; to find the program in the python search path:</span>
$ <span class="kw">python</span> -m tensorflow.models.image.mnist.convolutional
<span class="kw">Extracting</span> data/train-images-idx3-ubyte.gz
<span class="kw">Extracting</span> data/train-labels-idx1-ubyte.gz
<span class="kw">Extracting</span> data/t10k-images-idx3-ubyte.gz
<span class="kw">Extracting</span> data/t10k-labels-idx1-ubyte.gz
<span class="kw">...etc...</span>

<span class="co"># You can alternatively pass the path to the model program file to the python</span>
<span class="co"># interpreter (make sure to use the python distribution you installed</span>
<span class="co"># TensorFlow to, for example, .../python3.X/... for Python 3).</span>
$ <span class="kw">python</span> /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py
<span class="kw">...</span></code></pre></div>
</dd>
</dl>
<p><a href="http://www.cnblogs.com/zjgtan/archive/2013/06/08/3127490.html">&#26426;&#22120;&#23398;&#20064;&#65288;&#19968;&#65289;&#65306;&#29983;&#25104;&#23398;&#20064;&#31639;&#27861;Generative Learning algorithms - zjgtan - &#21338;&#23458;&#22253;</a></p>
<dl>
<dt>Rectifier (neural networks) <code class="fold">@</code></dt>
<dd><p>In the context of artificial neural networks, the rectifier is an activation function defined as</p>
<p><span class="math display">\[f(x) = \max(0, x)\]</span></p>
<p>where x is the input to a neuron. This is also known as a ramp function, and it is analogous to half-wave rectification in electrical engineering. This activation function has been argued to be more biologically plausible than the widely used logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent. The rectifier is, as of 2015, the most popular activation function for deep neural networks.</p>
<div class="figure">
<img src="https://upload.wikimedia.org/wikipedia/en/thumb/6/6c/Rectifier_and_softplus_functions.svg/330px-Rectifier_and_softplus_functions.svg.png" alt="Plot of the rectifier (blue) and softplus (green) functions near x = 0." />
<p class="caption">Plot of the rectifier (blue) and softplus (green) functions near x = 0.</p>
</div>
<p>A unit employing the rectifier is also called a rectified linear unit (ReLU).</p>
<p>A smooth approximation to the rectifier is the analytic function</p>
<p><span class="math display">\[f(x) = \ln(1 + e^x)\]</span></p>
<p>which is called the softplus function. The derivative of softplus is <span class="math inline">\(f&#39;(x) = e^x / (e^x+1) = 1 / (1 + e^{-x})\)</span>, i.e.&#160;the logistic function.</p>
<p>Rectified linear units find applications in computer vision, and speech recognition using deep neural nets.</p>
</dd>
<dt><a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B">UFLDL &#25945;&#31243; - Ufldl</a> <code class="fold">@</code></dt>
<dd><p>&#22914;&#26524;&#36873;&#25321; f(z) = 1/(1+(-z)) &#65292;&#20063;&#23601;&#26159; sigmoid &#20989;&#25968;&#65292;&#37027;&#20040;&#23427;&#30340;&#23548;&#25968;&#23601;&#26159; f&#8217;(z) = f(z) (1-f(z)) &#65288;&#22914;&#26524;&#36873;&#25321; tanh &#20989;&#25968;&#65292;&#37027;&#23427;&#30340;&#23548;&#25968;&#23601;&#26159; f&#8217;(z) = 1- (f(z))^2 &#65292;&#20320;&#21487;&#20197;&#26681;&#25454; sigmoid&#65288;&#25110; tanh&#65289;&#20989;&#25968;&#30340;&#23450;&#20041;&#33258;&#34892;&#25512;&#23548;&#36825;&#20010;&#31561;&#24335;&#12290;</p>
</dd>
</dl>
<p><a href="https://github.com/ty4z2008/Qix/blob/master/dl.md">Qix/dl.md at master &#183; ty4z2008/Qix</a></p>
<p><a href="http://www.cs.nyu.edu/~roweis/csc2515-2006/lectures.html">CSC2515F : lectures</a></p>
<dl>
<dt><a href="https://en.wikipedia.org/wiki/CIE_1931_color_space">CIE 1931 color space - Wikipedia, the free encyclopedia</a> <code class="fold">@</code></dt>
<dd><dl>
<dt><a href="https://en.wikipedia.org/wiki/Lab_color_space">Lab color space - Wikipedia, the free encyclopedia</a> <code class="foldable">@</code></dt>
<dd><p>A Lab color space is a color-opponent space with dimension L for lightness and a and b for the color-opponent dimensions, based on nonlinearly compressed (e.g.&#160;CIE XYZ color space) coordinates.</p>
</dd>
</dl>
</dd>
</dl>
<p><a href="https://en.wikipedia.org/wiki/Minimum_distance_estimation">Minimum distance estimation - Wikipedia, the free encyclopedia</a></p>
<p><a href="http://blog.csdn.net/huangbo10/article/details/22944007">&#20313;&#20975;&#22312;&#28165;&#21326;&#30340;&#35762;&#24231;&#31508;&#35760; - huangbo10 &#30340;&#19987;&#26639; - &#21338;&#23458;&#39057;&#36947; - CSDN.NET</a></p>
<p><a href="https://en.wikiversity.org/wiki/Learning_and_neural_networks">Learning and neural networks - Wikiversity</a></p>
<dl>
<dt><a href="http://karpathy.github.io/neuralnets/">Hacker&#8217;s guide to Neural Networks</a> <code class="fold">@</code></dt>
<dd><p><a href="http://cs.stanford.edu/people/karpathy/">Andrej Karpathy Academic Website</a></p>
<p>&#36825;&#37324;&#26377;&#26356;&#23436;&#25972;&#30340;&#20195;&#30721;&#21644; materials&#12290;</p>
<ul>
<li><a href="http://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a></li>
<li><a href="http://cs231n.stanford.edu/syllabus.html">Stanford University CS231n: Convolutional Neural Networks for Visual Recognition</a></li>
</ul>
<p>More materials</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Artificial_neural_network">Artificial neural network - Wikipedia, the free encyclopedia</a></li>
<li><a href="https://en.m.wikipedia.org/wiki/Artificial_neural_network">Artificial neural network - Wikipedia, the free encyclopedia</a></li>
</ul>
<!--
    [Hacker's guide to Neural Networks](file:///C:/Users/cvrs/Desktop/guide.htm)
-->
<p>My personal experience with Neural Networks is that everything became much clearer when I started ignoring full-page, dense derivations of backpropagation equations and just started writing code. Thus, this tutorial will contain very little math (I don&#8217;t believe it is necessary and it can sometimes even obfuscate simple concepts).</p>
<ul>
<li><dl>
<dt>Chapter 1: Real-valued Circuits <code class="fold">@</code></dt>
<dd><p>&#30475;&#25104;&#38376;&#30005;&#36335;&#65292;&#19981;&#21482;&#26159; <code>{0, 1}</code>&#65292;&#26356;&#26159; real values &#22312;&#32447;&#36335;&#19978; flow&#12290;&#38500;&#20102; <code>AND</code>&#65292;<code>OR</code>&#65292;<code>NOT</code> &#36824;&#26377; binary gates &#22914; <code>*</code> (multiply), <code>+</code> (add), <code>max</code>&#65292; unary gates &#27604;&#22914; <code>exp</code>&#65292;&#31561;&#31561;&#12290;</p>
<dl>
<dt>Base Case: Single Gate in the Circuit <code class="fold">@</code></dt>
<dd><p>base case&#65292;&#19968;&#20010;&#31616;&#21333;&#30340; single gate in the circuit&#12290;&#36827;&#34892; f(x,y) = x*y &#30340;&#25805;&#20316;&#65292; javascript &#20195;&#30721;&#22914;&#19979;&#65306;</p>
<div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="kw">var</span> forwardMultiplyGate <span class="op">=</span> <span class="kw">function</span>(x<span class="op">,</span> y) <span class="op">{</span>
  <span class="cf">return</span> x <span class="op">*</span> y<span class="op">;</span>
<span class="op">};</span>
<span class="at">forwardMultiplyGate</span>(<span class="op">-</span><span class="dv">2</span><span class="op">,</span> <span class="dv">3</span>)<span class="op">;</span> <span class="co">// returns -6. Exciting.</span></code></pre></div>
<p>&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35753;&#23427;&#30340;&#36755;&#20986;&#21464;&#22823;&#12290;&#36825;&#37324;&#26377;&#19977;&#20010;&#31574;&#30053;&#65306;</p>
<ul>
<li><dl>
<dt>Strategy #1: Random Local Search <code class="fold">@</code></dt>
<dd><p>&#38543;&#26426;&#22312;&#21608;&#22260;&#25214;&#28857;&#65292;&#28982;&#21518;&#30475;&#26159;&#21542;&#26356;&#22823;&#65292;&#26356;&#22823;&#23601;&#35760;&#24405;&#19979;&#26469;&#12290;</p>
<div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="co">// circuit with single gate for now</span>
<span class="kw">var</span> forwardMultiplyGate <span class="op">=</span> <span class="kw">function</span>(x<span class="op">,</span> y) <span class="op">{</span> <span class="cf">return</span> x <span class="op">*</span> y<span class="op">;</span> <span class="op">};</span>
<span class="kw">var</span> x <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">,</span> y <span class="op">=</span> <span class="dv">3</span><span class="op">;</span> <span class="co">// some input values</span>

<span class="co">// try changing x,y randomly small amounts and keep track of what works best</span>
<span class="kw">var</span> tweak_amount <span class="op">=</span> <span class="fl">0.01</span><span class="op">;</span>
<span class="kw">var</span> best_out <span class="op">=</span> <span class="op">-</span><span class="kw">Infinity</span><span class="op">;</span>
<span class="kw">var</span> best_x <span class="op">=</span> x<span class="op">,</span> best_y <span class="op">=</span> y<span class="op">;</span>
<span class="cf">for</span>(<span class="kw">var</span> k <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> k <span class="op">&lt;</span> <span class="dv">100</span><span class="op">;</span> k<span class="op">++</span>) <span class="op">{</span>
  <span class="kw">var</span> x_try <span class="op">=</span> x <span class="op">+</span> tweak_amount <span class="op">*</span> (<span class="va">Math</span>.<span class="at">random</span>() <span class="op">*</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>)<span class="op">;</span> <span class="co">// tweak x a bit</span>
  <span class="kw">var</span> y_try <span class="op">=</span> y <span class="op">+</span> tweak_amount <span class="op">*</span> (<span class="va">Math</span>.<span class="at">random</span>() <span class="op">*</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>)<span class="op">;</span> <span class="co">// tweak y a bit</span>
  <span class="kw">var</span> out <span class="op">=</span> <span class="at">forwardMultiplyGate</span>(x_try<span class="op">,</span> y_try)<span class="op">;</span>
  <span class="cf">if</span>(out <span class="op">&gt;</span> best_out) <span class="op">{</span>
    <span class="co">// best improvement yet! Keep track of the x and y</span>
    best_out <span class="op">=</span> out<span class="op">;</span>
    best_x <span class="op">=</span> x_try<span class="op">,</span> best_y <span class="op">=</span> y_try<span class="op">;</span>
  <span class="op">}</span>
<span class="op">}</span></code></pre></div>
</dd>
</dl></li>
<li><dl>
<dt>Stategy #2: Numerical Gradient <code class="fold">@</code></dt>
<dd><p>&#27714;&#20986;&#25968;&#20540;&#26799;&#24230;&#12290;&#8706;f(x,y)/&#8706;x = (f(x+h,y)&#8722;f(x,y))/h&#65292;&#36825;&#37324;&#30340; h &#26159;&#19968;&#20010; tweak amount&#12290;Javascript &#20195;&#30721;&#22914;&#19979;&#65306;</p>
<div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="kw">var</span> x <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">,</span> y <span class="op">=</span> <span class="dv">3</span><span class="op">;</span>
<span class="kw">var</span> out <span class="op">=</span> <span class="at">forwardMultiplyGate</span>(x<span class="op">,</span> y)<span class="op">;</span> <span class="co">// -6</span>
<span class="kw">var</span> h <span class="op">=</span> <span class="fl">0.0001</span><span class="op">;</span>

<span class="co">// &#23545; x &#30340;&#20559;&#23548;</span>
<span class="kw">var</span> xph <span class="op">=</span> x <span class="op">+</span> h<span class="op">;</span> <span class="co">// -1.9999</span>
<span class="kw">var</span> out2 <span class="op">=</span> <span class="at">forwardMultiplyGate</span>(xph<span class="op">,</span> y)<span class="op">;</span> <span class="co">// -5.9997</span>
<span class="kw">var</span> x_derivative <span class="op">=</span> (out2 <span class="op">-</span> out) / h<span class="op">;</span> <span class="co">// 3.0</span>

<span class="co">// &#23545; y &#30340;&#20559;&#23548;</span>
<span class="kw">var</span> yph <span class="op">=</span> y <span class="op">+</span> h<span class="op">;</span> <span class="co">// 3.0001</span>
<span class="kw">var</span> out3 <span class="op">=</span> <span class="at">forwardMultiplyGate</span>(x<span class="op">,</span> yph)<span class="op">;</span> <span class="co">// -6.0002</span>
<span class="kw">var</span> y_derivative <span class="op">=</span> (out3 <span class="op">-</span> out) / h<span class="op">;</span> <span class="co">// -2.0</span>

<span class="co">// &#25351;&#23450; stepsize&#65292;&#32473;&#36755;&#20837;&#28155;&#21152;&#19968;&#28857; tweak amount&#65292;</span>
<span class="co">// &#24471;&#21040;&#26032;&#30340;&#36755;&#20986;&#65292;&#32780;&#19988;&#36755;&#20986;&#20540;&#30495;&#30340;&#26356;&#22823;&#19968;&#20123;&#65281;</span>
<span class="kw">var</span> step_size <span class="op">=</span> <span class="fl">0.01</span><span class="op">;</span>
<span class="kw">var</span> out <span class="op">=</span> <span class="at">forwardMultiplyGate</span>(x<span class="op">,</span> y)<span class="op">;</span> <span class="co">// before: -6</span>
x <span class="op">=</span> x <span class="op">+</span> step_size <span class="op">*</span> x_derivative<span class="op">;</span> <span class="co">// x becomes -1.97</span>
y <span class="op">=</span> y <span class="op">+</span> step_size <span class="op">*</span> y_derivative<span class="op">;</span> <span class="co">// y becomes 2.98</span>
<span class="kw">var</span> out_new <span class="op">=</span> <span class="at">forwardMultiplyGate</span>(x<span class="op">,</span> y)<span class="op">;</span> <span class="co">// -5.87! exciting.</span></code></pre></div>
<blockquote>
<p>The derivative with respect to some input can be computed by tweaking that input by a small amount and observing the change on the output value.</p>
</blockquote>
</dd>
</dl></li>
<li><dl>
<dt>Strategy #3: Analytic Gradient <code class="fold">@</code></dt>
<dd><p>&#20998;&#26512;&#23548;&#25968;&#12290;&#30456;&#27604;&#25968;&#20540;&#23548;&#25968;&#65292;&#36825;&#20010;&#38656;&#35201;&#19968;&#28857;&#25968;&#23398;&#30693;&#35782;&#12290;&#22909;&#22788;&#26159;&#21487;&#20197;&#24471;&#21040; exact &#23548;&#25968;&#65292;&#32780;&#19988;&#35745;&#31639;&#37327;&#23567;&#12290;</p>
<blockquote>
<p>The analytic derivative requires no tweaking of the inputs. It can be derived using mathematics (calculus).</p>
</blockquote>
<p>Javascript &#20195;&#30721;&#65306;</p>
<div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="kw">var</span> x <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">,</span> y <span class="op">=</span> <span class="dv">3</span><span class="op">;</span>
<span class="kw">var</span> out <span class="op">=</span> <span class="at">forwardMultiplyGate</span>(x<span class="op">,</span> y)<span class="op">;</span> <span class="co">// before: -6</span>
<span class="kw">var</span> x_gradient <span class="op">=</span> y<span class="op">;</span> <span class="co">// by our complex mathematical derivation above</span>
<span class="kw">var</span> y_gradient <span class="op">=</span> x<span class="op">;</span>

<span class="kw">var</span> step_size <span class="op">=</span> <span class="fl">0.01</span><span class="op">;</span>
x <span class="op">+=</span> step_size <span class="op">*</span> x_gradient<span class="op">;</span> <span class="co">// -2.03</span>
y <span class="op">+=</span> step_size <span class="op">*</span> y_gradient<span class="op">;</span> <span class="co">// 2.98</span>
<span class="kw">var</span> out_new <span class="op">=</span> <span class="at">forwardMultiplyGate</span>(x<span class="op">,</span> y)<span class="op">;</span> <span class="co">// -5.87. Higher output! Nice.</span></code></pre></div>
</dd>
</dl></li>
</ul>
<p>&#23567;&#32467;&#12290;Lets recap what we have learned: <code class="fold">@</code></p>
<ul>
<li>INPUT: We are given a circuit, some inputs and compute an output value.</li>
<li>OUTPUT: We are then interested finding small changes to each input (independently) that would make the output higher.</li>
<li>Strategy #1: One silly way is to randomly search for small pertubations of the inputs and keep track of what gives the highest increase in output.</li>
<li>Strategy #2: We saw we can do much better by computing the gradient. Regardless of how complicated the circuit is, the numerical gradient is very simple (but relatively expensive) to compute. We compute it by probing the circuit&#8217;s output value as we tweak the inputs one at a time.</li>
<li>Strategy #3: In the end, we saw that we can be even more clever and analytically derive a direct expression to get the analytic gradient. It is identical to the numerical gradient, it is fastest by far, and there is no need for any tweaking.</li>
</ul>
</dd>
<dt>Recursive Case: Circuits with Multiple Gates <code class="fold">@</code></dt>
<dd><p>f(x,y,z) = (x+y)*z&#65292;&#30452;&#25509;&#30475;&#20195;&#30721;&#21543;&#65292;&#30475;&#35299;&#37322;&#19981;&#22914;&#30475;&#20195;&#30721;&#12290;</p>
<p>&#20998;&#19977;&#20010;&#36807;&#31243;&#65306;</p>
<ul>
<li><dl>
<dt>forword&#65292;&#35745;&#31639;&#36755;&#20986;&#65306; <code class="fold">@</code></dt>
<dd><div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="kw">var</span> forwardMultiplyGate <span class="op">=</span> <span class="kw">function</span>(a<span class="op">,</span> b) <span class="op">{</span>
  <span class="cf">return</span> a <span class="op">*</span> b<span class="op">;</span>
<span class="op">};</span>
<span class="kw">var</span> forwardAddGate <span class="op">=</span> <span class="kw">function</span>(a<span class="op">,</span> b) <span class="op">{</span>
  <span class="cf">return</span> a <span class="op">+</span> b<span class="op">;</span>
<span class="op">};</span>
<span class="kw">var</span> forwardCircuit <span class="op">=</span> <span class="kw">function</span>(x<span class="op">,</span>y<span class="op">,</span>z) <span class="op">{</span>
  <span class="kw">var</span> q <span class="op">=</span> <span class="at">forwardAddGate</span>(x<span class="op">,</span> y)<span class="op">;</span>
  <span class="kw">var</span> f <span class="op">=</span> <span class="at">forwardMultiplyGate</span>(q<span class="op">,</span> z)<span class="op">;</span>
  <span class="cf">return</span> f<span class="op">;</span>
<span class="op">};</span>

<span class="kw">var</span> x <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">,</span> y <span class="op">=</span> <span class="dv">5</span><span class="op">,</span> z <span class="op">=</span> <span class="op">-</span><span class="dv">4</span><span class="op">;</span>
<span class="kw">var</span> f <span class="op">=</span> <span class="at">forwardCircuit</span>(x<span class="op">,</span> y<span class="op">,</span> z)<span class="op">;</span> <span class="co">// output is -12</span></code></pre></div>
</dd>
</dl></li>
<li><dl>
<dt>backward&#65292;&#21453;&#21521;&#20256;&#25773;&#65292;&#35745;&#31639;&#20559;&#23548;&#65288;&#20915;&#23450;&#20102;&#36755;&#20837;&#30340;&#26356;&#26032;&#65289;&#65306; <code class="fold">@</code></dt>
<dd><div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="co">// initial conditions</span>
<span class="kw">var</span> x <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">,</span> y <span class="op">=</span> <span class="dv">5</span><span class="op">,</span> z <span class="op">=</span> <span class="op">-</span><span class="dv">4</span><span class="op">;</span>
<span class="kw">var</span> q <span class="op">=</span> <span class="at">forwardAddGate</span>(x<span class="op">,</span> y)<span class="op">;</span> <span class="co">// q is 3</span>
<span class="kw">var</span> f <span class="op">=</span> <span class="at">forwardMultiplyGate</span>(q<span class="op">,</span> z)<span class="op">;</span> <span class="co">// output is -12</span>

<span class="co">// gradient of the MULTIPLY gate with respect to its inputs</span>
<span class="co">// wrt is short for &quot;with respect to&quot;</span>
<span class="kw">var</span> derivative_f_wrt_z <span class="op">=</span> q<span class="op">;</span> <span class="co">// 3</span>
<span class="kw">var</span> derivative_f_wrt_q <span class="op">=</span> z<span class="op">;</span> <span class="co">// -4</span>

<span class="co">// derivative of the ADD gate with respect to its inputs</span>
<span class="kw">var</span> derivative_q_wrt_x <span class="op">=</span> <span class="fl">1.0</span><span class="op">;</span>
<span class="kw">var</span> derivative_q_wrt_y <span class="op">=</span> <span class="fl">1.0</span><span class="op">;</span>

<span class="co">// chain rule</span>
<span class="kw">var</span> derivative_f_wrt_x <span class="op">=</span> derivative_q_wrt_x <span class="op">*</span> derivative_f_wrt_q<span class="op">;</span> <span class="co">// -4</span>
<span class="kw">var</span> derivative_f_wrt_y <span class="op">=</span> derivative_q_wrt_y <span class="op">*</span> derivative_f_wrt_q<span class="op">;</span> <span class="co">// -4</span></code></pre></div>
</dd>
</dl></li>
<li><dl>
<dt>forward&#65292;&#26681;&#25454;&#21453;&#21521;&#20256;&#25773;&#24471;&#21040;&#30340;&#20559;&#23548;&#65292;&#26356;&#26032;&#36755;&#20837;&#65292;&#20877;&#35745;&#31639;&#36755;&#20986;&#65306; <code class="fold">@</code></dt>
<dd><div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="co">// final gradient, from above: [-4, -4, 3]</span>
<span class="kw">var</span> gradient_f_wrt_xyz <span class="op">=</span> [derivative_f_wrt_x<span class="op">,</span> derivative_f_wrt_y<span class="op">,</span> derivative_f_wrt_z]

<span class="co">// let the inputs respond to the force/tug:</span>
<span class="kw">var</span> step_size <span class="op">=</span> <span class="fl">0.01</span><span class="op">;</span>
x <span class="op">=</span> x <span class="op">+</span> step_size <span class="op">*</span> derivative_f_wrt_x<span class="op">;</span> <span class="co">// -2.04</span>
y <span class="op">=</span> y <span class="op">+</span> step_size <span class="op">*</span> derivative_f_wrt_y<span class="op">;</span> <span class="co">// 4.96</span>
z <span class="op">=</span> z <span class="op">+</span> step_size <span class="op">*</span> derivative_f_wrt_z<span class="op">;</span> <span class="co">// -3.97</span>

<span class="co">// Our circuit now better give higher output:</span>
<span class="kw">var</span> q <span class="op">=</span> <span class="at">forwardAddGate</span>(x<span class="op">,</span> y)<span class="op">;</span> <span class="co">// q becomes 2.92</span>
<span class="kw">var</span> f <span class="op">=</span> <span class="at">forwardMultiplyGate</span>(q<span class="op">,</span> z)<span class="op">;</span> <span class="co">// output is -11.59, up from -12! Nice!</span></code></pre></div>
<p>&#21457;&#29616;&#30830;&#23454;&#36755;&#20986;&#20540;&#30830;&#23454;&#21152;&#22823;&#20102;&#12290;</p>
</dd>
</dl></li>
</ul>
<p>&#29702;&#35299;&#21040;&#20102;&#36825;&#37324;&#65292;&#20320;&#23601;&#26126;&#30333;&#20102;&#21453;&#21521;&#20256;&#25773;&#21040;&#24213;&#26159;&#22312;&#24178;&#22043;&#12290;</p>
<p>&#23567;&#32467;&#12290;Lets recap once again what we learned:</p>
<ul>
<li>In the previous chapter we saw that in the case of a single gate (or a single expression), we can derive the analytic gradient using simple calculus. We interpreted the gradient as a force, or a tug on the inputs that pulls them in a direction which would make this gate&#8217;s output higher.</li>
<li>In case of multiple gates everything stays pretty much the same way: every gate is hanging out by itself completely unaware of the circuit it is embedded in. Some inputs come in and the gate computes its output and the derivate with respect to the inputs. The only difference now is that suddenly, something can pull on this gate from above. That&#8217; s the gradient of the final circuit output value with respect to the ouput this gate computed. It is the circuit asking the gate to output higher or lower numbers, and with some force. The gate simply takes this force and multiplies it to all the forces it computed for its inputs before (chain rule). This has the desired effect:</li>
<li>If a gate experiences a strong positive pull from above, it will also pull harder on its own inputs, scaled by the force it is experiencing from above</li>
<li>And if it experiences a negative tug, this means that circuit wants its value to decrease not increase, so it will flip the force of the pull on its inputs to make its own output value smaller.</li>
</ul>
<blockquote>
<p>A nice picture to have in mind is that as we pull on the circuit&#8217;s output value at the end, this induces pulls downward through the entire circuit, all the way down to the inputs.</p>
<p>&#23601;&#26159;&#35828;&#25105;&#20204;&#22312;&#36755;&#20986;&#31471;&#26045;&#21152;&#19968;&#20010;&#21147;&#65292;&#36825;&#20010;&#21147;&#33021;&#22815;&#36820;&#22238;&#21435;&#20316;&#29992;&#21040;&#30005;&#36335;&#30340;&#36755;&#20837;&#12290;</p>
</blockquote>
</dd>
<dt>Patterns in the &#8220;backward&#8221; flow <code class="fold">@</code></dt>
<dd><p>&#35828;&#30340;&#23601;&#26159; <code>+</code> &#21644; <code>*</code> &#21453;&#21521;&#20256;&#25773;&#30340;&#35268;&#24459;&#12290;&#65288;&#23545; <code>+</code> &#32780;&#35328;&#65292;&#26159;&#25226; sigma &#30452;&#25509;&#20998;&#25955;&#22238;&#21435;&#65307;&#23545; <code>*</code> &#32780;&#35328;&#65292;&#26159;&#20132;&#25442;&#36755;&#20837;&#20540;&#12290;&#65289;</p>
</dd>
<dt>Example: Single Neuron <code class="fold">@</code></dt>
<dd><p>f(x,y,a,b,c) = &#963;(ax+by+c)&#12290;</p>
<p>sigmoid &#20989;&#25968;&#24456;&#31526;&#21512;&#26174;&#31034;&#35268;&#24459;&#65292;&#26159;&#19968;&#20010; sigmoid &#24418;&#29366;&#65288;&#8220;S&#8221; &#24418;&#65289;&#12290;&#20294; &#8220;S&#8221; &#24418;&#30340;&#20989;&#25968;&#22810;&#20102;&#21435;&#65292;&#20026;&#20160;&#20040;&#35201;&#29992; sigmoid = 1/(1+e^(-x))&#65311;&#22240;&#20026;&#36825;&#20010;&#20989;&#25968;&#24456;&#26041;&#20415;&#35745;&#31639;&#23548;&#25968;&#12290;&#32780;&#19988;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#19981;&#20165;&#20165;&#26159; sigmoid &#20989;&#25968;&#30340;&#23548;&#25968;&#26159; sig&#8217;(x) = sig(x)(1-sig(x))&#65292;&#26356;&#26159; sig(x) &#22312; forward &#24050;&#32463;&#31639;&#36807;&#20102;&#65281;&#20063;&#23601;&#26159;&#35828; sigmoid &#20989;&#25968;&#27714;&#23548;&#25968;&#65292;&#35745;&#31639;&#20219;&#21153;&#30340;&#36127;&#25285;&#21644; x(1-x) &#19968;&#26679;&#8230;&#8230;&#26159;&#19981;&#26159;&#24456;&#36190;&#65311;&#65281;</p>
<p>&#32447;&#36335;&#20013;&#27599;&#26465;&#32447;&#20854;&#23454;&#26377;&#20004;&#26679;&#25968;&#20540;&#20110;&#27492;&#30456;&#20851;&#65292;&#19968;&#20010;&#26159; forward &#26102;&#20256;&#36882;&#30340;&#36755;&#20837;&#20540;&#65292;&#19968;&#20010;&#26159; backward &#26102;&#21453;&#21521;&#20256;&#36882;&#30340; gradients&#12290;&#25105;&#20204;&#20808;&#21019;&#24314;&#19968;&#20010; Unit &#21333;&#20803;&#26469;&#23384;&#20648;&#30005;&#36335;&#20013;&#30340; wire &#30340; input value &#21644; weights&#65306;</p>
<div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="co">// every Unit corresponds to a wire in the diagrams</span>
<span class="kw">var</span> Unit <span class="op">=</span> <span class="kw">function</span>(value<span class="op">,</span> grad) <span class="op">{</span>
  <span class="co">// value computed in the forward pass</span>
  <span class="kw">this</span>.<span class="at">value</span> <span class="op">=</span> value<span class="op">;</span>
  <span class="co">// the derivative of circuit output w.r.t this unit, computed in backward pass</span>
  <span class="kw">this</span>.<span class="at">grad</span> <span class="op">=</span> grad<span class="op">;</span>
<span class="op">}</span></code></pre></div>
<p>&#38500;&#20102; unit &#25105;&#20204;&#36824;&#35201;&#26377;&#19968;&#20010; <code>+</code>&#65292;&#19968;&#20010; <code>*</code> &#21644;&#19968;&#20010; <code>sig</code> (sigmoid)&#12290;</p>
<ul>
<li><dl>
<dt>multiplyGate <code class="fold">@</code></dt>
<dd><div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="kw">var</span> multiplyGate <span class="op">=</span> <span class="kw">function</span>()<span class="op">{</span> <span class="op">};</span>
<span class="va">multiplyGate</span>.<span class="at">prototype</span> <span class="op">=</span> <span class="op">{</span>
  <span class="dt">forward</span><span class="op">:</span> <span class="kw">function</span>(u0<span class="op">,</span> u1) <span class="op">{</span>
    <span class="co">// store pointers to input Units u0 and u1 and output unit utop</span>
    <span class="kw">this</span>.<span class="at">u0</span> <span class="op">=</span> u0<span class="op">;</span>
    <span class="kw">this</span>.<span class="at">u1</span> <span class="op">=</span> u1<span class="op">;</span>
    <span class="kw">this</span>.<span class="at">utop</span> <span class="op">=</span> <span class="kw">new</span> <span class="at">Unit</span>(<span class="va">u0</span>.<span class="at">value</span> <span class="op">*</span> <span class="va">u1</span>.<span class="at">value</span><span class="op">,</span> <span class="fl">0.0</span>)<span class="op">;</span>
    <span class="cf">return</span> <span class="kw">this</span>.<span class="at">utop</span><span class="op">;</span>
  <span class="op">},</span>
  <span class="dt">backward</span><span class="op">:</span> <span class="kw">function</span>() <span class="op">{</span>
    <span class="co">// take the gradient in output unit and chain it with the</span>
    <span class="co">// local gradients, which we derived for multiply gate before</span>
    <span class="co">// then write those gradients to those Units.</span>
    <span class="kw">this</span>.<span class="va">u0</span>.<span class="at">grad</span> <span class="op">+=</span> <span class="kw">this</span>.<span class="va">u1</span>.<span class="at">value</span> <span class="op">*</span> <span class="kw">this</span>.<span class="va">utop</span>.<span class="at">grad</span><span class="op">;</span>
    <span class="kw">this</span>.<span class="va">u1</span>.<span class="at">grad</span> <span class="op">+=</span> <span class="kw">this</span>.<span class="va">u0</span>.<span class="at">value</span> <span class="op">*</span> <span class="kw">this</span>.<span class="va">utop</span>.<span class="at">grad</span><span class="op">;</span>
  <span class="op">}</span>
<span class="op">}</span></code></pre></div>
</dd>
</dl></li>
<li><dl>
<dt>addGate <code class="fold">@</code></dt>
<dd><div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="kw">var</span> addGate <span class="op">=</span> <span class="kw">function</span>()<span class="op">{</span> <span class="op">};</span>
<span class="va">addGate</span>.<span class="at">prototype</span> <span class="op">=</span> <span class="op">{</span>
  <span class="dt">forward</span><span class="op">:</span> <span class="kw">function</span>(u0<span class="op">,</span> u1) <span class="op">{</span>
    <span class="kw">this</span>.<span class="at">u0</span> <span class="op">=</span> u0<span class="op">;</span>
    <span class="kw">this</span>.<span class="at">u1</span> <span class="op">=</span> u1<span class="op">;</span> <span class="co">// store pointers to input units</span>
    <span class="kw">this</span>.<span class="at">utop</span> <span class="op">=</span> <span class="kw">new</span> <span class="at">Unit</span>(<span class="va">u0</span>.<span class="at">value</span> <span class="op">+</span> <span class="va">u1</span>.<span class="at">value</span><span class="op">,</span> <span class="fl">0.0</span>)<span class="op">;</span>
    <span class="cf">return</span> <span class="kw">this</span>.<span class="at">utop</span><span class="op">;</span>
  <span class="op">},</span>
  <span class="dt">backward</span><span class="op">:</span> <span class="kw">function</span>() <span class="op">{</span>
    <span class="co">// add gate. derivative wrt both inputs is 1</span>
    <span class="kw">this</span>.<span class="va">u0</span>.<span class="at">grad</span> <span class="op">+=</span> <span class="dv">1</span> <span class="op">*</span> <span class="kw">this</span>.<span class="va">utop</span>.<span class="at">grad</span><span class="op">;</span>
    <span class="kw">this</span>.<span class="va">u1</span>.<span class="at">grad</span> <span class="op">+=</span> <span class="dv">1</span> <span class="op">*</span> <span class="kw">this</span>.<span class="va">utop</span>.<span class="at">grad</span><span class="op">;</span>
  <span class="op">}</span>
<span class="op">}</span></code></pre></div>
</dd>
</dl></li>
<li><dl>
<dt>sigmoidGate <code class="fold">@</code></dt>
<dd><div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="kw">var</span> sigmoidGate <span class="op">=</span> <span class="kw">function</span>() <span class="op">{</span>
    <span class="co">// helper function</span>
    <span class="kw">this</span>.<span class="at">sig</span> <span class="op">=</span> <span class="kw">function</span>(x) <span class="op">{</span> <span class="cf">return</span> <span class="dv">1</span> / (<span class="dv">1</span> <span class="op">+</span> <span class="va">Math</span>.<span class="at">exp</span>(<span class="op">-</span>x))<span class="op">;</span> <span class="op">};</span>
<span class="op">};</span>
<span class="va">sigmoidGate</span>.<span class="at">prototype</span> <span class="op">=</span> <span class="op">{</span>
    <span class="dt">forward</span><span class="op">:</span> <span class="kw">function</span>(u0) <span class="op">{</span>
        <span class="kw">this</span>.<span class="at">u0</span> <span class="op">=</span> u0<span class="op">;</span>
        <span class="kw">this</span>.<span class="at">utop</span> <span class="op">=</span> <span class="kw">new</span> <span class="at">Unit</span>(<span class="kw">this</span>.<span class="at">sig</span>(<span class="kw">this</span>.<span class="va">u0</span>.<span class="at">value</span>)<span class="op">,</span> <span class="fl">0.0</span>)<span class="op">;</span>
        <span class="cf">return</span> <span class="kw">this</span>.<span class="at">utop</span><span class="op">;</span>
    <span class="op">},</span>
    <span class="dt">backward</span><span class="op">:</span> <span class="kw">function</span>() <span class="op">{</span>
        <span class="kw">var</span> s <span class="op">=</span> <span class="kw">this</span>.<span class="at">sig</span>(<span class="kw">this</span>.<span class="va">u0</span>.<span class="at">value</span>)<span class="op">;</span>
        <span class="kw">this</span>.<span class="va">u0</span>.<span class="at">grad</span> <span class="op">+=</span> (s <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> s)) <span class="op">*</span> <span class="kw">this</span>.<span class="va">utop</span>.<span class="at">grad</span><span class="op">;</span>
    <span class="op">}</span>
<span class="op">}</span></code></pre></div>
</dd>
</dl></li>
</ul>
<p>&#28982;&#21518;&#23601;&#21487;&#20197;&#25343;&#26469;&#31639;&#20102;&#65306;</p>
<ul>
<li><dl>
<dt>forward <code class="fold">@</code></dt>
<dd><div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="co">// create input units</span>
<span class="kw">var</span> a <span class="op">=</span> <span class="kw">new</span> <span class="at">Unit</span>(<span class="fl">1.0</span><span class="op">,</span> <span class="fl">0.0</span>)<span class="op">;</span>
<span class="kw">var</span> b <span class="op">=</span> <span class="kw">new</span> <span class="at">Unit</span>(<span class="fl">2.0</span><span class="op">,</span> <span class="fl">0.0</span>)<span class="op">;</span>
<span class="kw">var</span> c <span class="op">=</span> <span class="kw">new</span> <span class="at">Unit</span>(<span class="op">-</span><span class="fl">3.0</span><span class="op">,</span> <span class="fl">0.0</span>)<span class="op">;</span>
<span class="kw">var</span> x <span class="op">=</span> <span class="kw">new</span> <span class="at">Unit</span>(<span class="op">-</span><span class="fl">1.0</span><span class="op">,</span> <span class="fl">0.0</span>)<span class="op">;</span>
<span class="kw">var</span> y <span class="op">=</span> <span class="kw">new</span> <span class="at">Unit</span>(<span class="fl">3.0</span><span class="op">,</span> <span class="fl">0.0</span>)<span class="op">;</span>

<span class="co">// create the gates</span>
<span class="kw">var</span> mulg0 <span class="op">=</span> <span class="kw">new</span> <span class="at">multiplyGate</span>()<span class="op">;</span>
<span class="kw">var</span> mulg1 <span class="op">=</span> <span class="kw">new</span> <span class="at">multiplyGate</span>()<span class="op">;</span>
<span class="kw">var</span> addg0 <span class="op">=</span> <span class="kw">new</span> <span class="at">addGate</span>()<span class="op">;</span>
<span class="kw">var</span> addg1 <span class="op">=</span> <span class="kw">new</span> <span class="at">addGate</span>()<span class="op">;</span>
<span class="kw">var</span> sg0 <span class="op">=</span> <span class="kw">new</span> <span class="at">sigmoidGate</span>()<span class="op">;</span>

<span class="co">// do the forward pass</span>
<span class="kw">var</span> forwardNeuron <span class="op">=</span> <span class="kw">function</span>() <span class="op">{</span>
  ax <span class="op">=</span> <span class="va">mulg0</span>.<span class="at">forward</span>(a<span class="op">,</span> x)<span class="op">;</span> <span class="co">// a*x = -1</span>
  by <span class="op">=</span> <span class="va">mulg1</span>.<span class="at">forward</span>(b<span class="op">,</span> y)<span class="op">;</span> <span class="co">// b*y = 6</span>
  axpby <span class="op">=</span> <span class="va">addg0</span>.<span class="at">forward</span>(ax<span class="op">,</span> by)<span class="op">;</span> <span class="co">// a*x + b*y = 5</span>
  axpbypc <span class="op">=</span> <span class="va">addg1</span>.<span class="at">forward</span>(axpby<span class="op">,</span> c)<span class="op">;</span> <span class="co">// a*x + b*y + c = 2</span>
  s <span class="op">=</span> <span class="va">sg0</span>.<span class="at">forward</span>(axpbypc)<span class="op">;</span> <span class="co">// sig(a*x + b*y + c) = 0.8808</span>
<span class="op">};</span>
<span class="at">forwardNeuron</span>()<span class="op">;</span>

<span class="va">console</span>.<span class="at">log</span>(<span class="st">&#39;circuit output: &#39;</span> <span class="op">+</span> <span class="va">s</span>.<span class="at">value</span>)<span class="op">;</span> <span class="co">// prints 0.8808</span></code></pre></div>
</dd>
</dl></li>
<li><dl>
<dt>backward <code class="fold">@</code></dt>
<dd><div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="va">s</span>.<span class="at">grad</span> <span class="op">=</span> <span class="fl">1.0</span><span class="op">;</span>
<span class="va">sg0</span>.<span class="at">backward</span>()<span class="op">;</span> <span class="co">// writes gradient into axpbypc</span>
<span class="va">addg1</span>.<span class="at">backward</span>()<span class="op">;</span> <span class="co">// writes gradients into axpby and c</span>
<span class="va">addg0</span>.<span class="at">backward</span>()<span class="op">;</span> <span class="co">// writes gradients into ax and by</span>
<span class="va">mulg1</span>.<span class="at">backward</span>()<span class="op">;</span> <span class="co">// writes gradients into b and y</span>
<span class="va">mulg0</span>.<span class="at">backward</span>()<span class="op">;</span> <span class="co">// writes gradients into a and x</span></code></pre></div>
</dd>
</dl></li>
<li><dl>
<dt>forward <code class="fold">@</code></dt>
<dd><div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="kw">var</span> step_size <span class="op">=</span> <span class="fl">0.01</span><span class="op">;</span>
<span class="va">a</span>.<span class="at">value</span> <span class="op">+=</span> step_size <span class="op">*</span> <span class="va">a</span>.<span class="at">grad</span><span class="op">;</span> <span class="co">// a.grad is -0.105</span>
<span class="va">b</span>.<span class="at">value</span> <span class="op">+=</span> step_size <span class="op">*</span> <span class="va">b</span>.<span class="at">grad</span><span class="op">;</span> <span class="co">// b.grad is 0.315</span>
<span class="va">c</span>.<span class="at">value</span> <span class="op">+=</span> step_size <span class="op">*</span> <span class="va">c</span>.<span class="at">grad</span><span class="op">;</span> <span class="co">// c.grad is 0.105</span>
<span class="va">x</span>.<span class="at">value</span> <span class="op">+=</span> step_size <span class="op">*</span> <span class="va">x</span>.<span class="at">grad</span><span class="op">;</span> <span class="co">// x.grad is 0.105</span>
<span class="va">y</span>.<span class="at">value</span> <span class="op">+=</span> step_size <span class="op">*</span> <span class="va">y</span>.<span class="at">grad</span><span class="op">;</span> <span class="co">// y.grad is 0.210</span>

<span class="at">forwardNeuron</span>()<span class="op">;</span>
<span class="va">console</span>.<span class="at">log</span>(<span class="st">&#39;circuit output after one backprop: &#39;</span> <span class="op">+</span> <span class="va">s</span>.<span class="at">value</span>)<span class="op">;</span> <span class="co">// prints 0.8825</span></code></pre></div>
<dl>
<dt>&#25105;&#20204;&#36824;&#21487;&#20197;&#26816;&#39564;&#19968;&#19979; bp &#31639;&#20986;&#26469;&#30340; grad &#26159;&#21542;&#27491;&#30830;&#65292;</dt>
<dd><div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="kw">var</span> forwardCircuitFast <span class="op">=</span> <span class="kw">function</span>(a<span class="op">,</span>b<span class="op">,</span>c<span class="op">,</span>x<span class="op">,</span>y) <span class="op">{</span>
  <span class="cf">return</span> <span class="dv">1</span>/(<span class="dv">1</span> <span class="op">+</span> <span class="va">Math</span>.<span class="at">exp</span>( <span class="op">-</span> (a<span class="op">*</span>x <span class="op">+</span> b<span class="op">*</span>y <span class="op">+</span> c)))<span class="op">;</span>
<span class="op">};</span>
<span class="kw">var</span> a <span class="op">=</span> <span class="dv">1</span><span class="op">,</span> b <span class="op">=</span> <span class="dv">2</span><span class="op">,</span> c <span class="op">=</span> <span class="op">-</span><span class="dv">3</span><span class="op">,</span> x <span class="op">=</span> <span class="op">-</span><span class="dv">1</span><span class="op">,</span> y <span class="op">=</span> <span class="dv">3</span><span class="op">;</span>
<span class="kw">var</span> h <span class="op">=</span> <span class="fl">0.0001</span><span class="op">;</span>
<span class="kw">var</span> a_grad <span class="op">=</span> (<span class="at">forwardCircuitFast</span>(a<span class="op">+</span>h<span class="op">,</span>b<span class="op">,</span>c<span class="op">,</span>x<span class="op">,</span>y) <span class="op">-</span> <span class="at">forwardCircuitFast</span>(a<span class="op">,</span>b<span class="op">,</span>c<span class="op">,</span>x<span class="op">,</span>y))/h<span class="op">;</span>
<span class="kw">var</span> b_grad <span class="op">=</span> (<span class="at">forwardCircuitFast</span>(a<span class="op">,</span>b<span class="op">+</span>h<span class="op">,</span>c<span class="op">,</span>x<span class="op">,</span>y) <span class="op">-</span> <span class="at">forwardCircuitFast</span>(a<span class="op">,</span>b<span class="op">,</span>c<span class="op">,</span>x<span class="op">,</span>y))/h<span class="op">;</span>
<span class="kw">var</span> c_grad <span class="op">=</span> (<span class="at">forwardCircuitFast</span>(a<span class="op">,</span>b<span class="op">,</span>c<span class="op">+</span>h<span class="op">,</span>x<span class="op">,</span>y) <span class="op">-</span> <span class="at">forwardCircuitFast</span>(a<span class="op">,</span>b<span class="op">,</span>c<span class="op">,</span>x<span class="op">,</span>y))/h<span class="op">;</span>
<span class="kw">var</span> x_grad <span class="op">=</span> (<span class="at">forwardCircuitFast</span>(a<span class="op">,</span>b<span class="op">,</span>c<span class="op">,</span>x<span class="op">+</span>h<span class="op">,</span>y) <span class="op">-</span> <span class="at">forwardCircuitFast</span>(a<span class="op">,</span>b<span class="op">,</span>c<span class="op">,</span>x<span class="op">,</span>y))/h<span class="op">;</span>
<span class="kw">var</span> y_grad <span class="op">=</span> (<span class="at">forwardCircuitFast</span>(a<span class="op">,</span>b<span class="op">,</span>c<span class="op">,</span>x<span class="op">,</span>y<span class="op">+</span>h) <span class="op">-</span> <span class="at">forwardCircuitFast</span>(a<span class="op">,</span>b<span class="op">,</span>c<span class="op">,</span>x<span class="op">,</span>y))/h<span class="op">;</span></code></pre></div>
</dd>
</dl>
</dd>
</dl></li>
</ul>
</dd>
<dt>Becoming a Backprop Ninja <code class="fold">@</code></dt>
<dd><div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="co">// &#36825;&#31181;&#24773;&#20917;&#26159;&#31616;&#21333;&#30340;</span>
<span class="kw">var</span> x <span class="op">=</span> a <span class="op">+</span> b <span class="op">+</span> c<span class="op">;</span>
<span class="kw">var</span> da <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> dx<span class="op">;</span> <span class="kw">var</span> db <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> dx<span class="op">;</span> <span class="kw">var</span> dc <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> dx<span class="op">;</span>

<span class="co">// &#36825;&#31181;&#24773;&#20917;&#21602;&#65311;</span>
<span class="kw">var</span> x <span class="op">=</span> a <span class="op">*</span> a<span class="op">;</span>
<span class="kw">var</span> da <span class="op">=</span> <span class="co">//???</span>

<span class="co">// &#21487;&#20197;&#20570;&#22914;&#19979;&#32771;&#34385;</span>
<span class="kw">var</span> da <span class="op">=</span> a <span class="op">*</span> dx<span class="op">;</span> <span class="co">// gradient into a from first branch</span>
da <span class="op">+=</span> a <span class="op">*</span> dx<span class="op">;</span> <span class="co">// and add on the gradient from the second branch</span>

<span class="co">// short form instead is:</span>
<span class="kw">var</span> da <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> a <span class="op">*</span> dx<span class="op">;</span></code></pre></div>
<p>&#36824;&#26377;&#26356;&#38590;&#30340;&#20363;&#23376;&#12290;</p>
<p>&#36824;&#26377;&#65292;ReLU &#21602;&#65311;</p>
<div class="sourceCode"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span class="kw">var</span> x <span class="op">=</span> <span class="va">Math</span>.<span class="at">max</span>(a<span class="op">,</span> <span class="dv">0</span>)
<span class="co">// backprop through this gate will then be:</span>
<span class="kw">var</span> da <span class="op">=</span> a <span class="op">&gt;</span> <span class="dv">0</span> <span class="op">?</span> <span class="fl">1.0</span> <span class="op">*</span> dx : <span class="fl">0.0</span><span class="op">;</span></code></pre></div>
<blockquote>
<p>&#8220;Maybe this is not immediately obvious, but this machinery is a powerful hammer for Machine Learning.&#8221;</p>
</blockquote>
</dd>
</dl>
</dd>
</dl></li>
<li>Chapter 2: Machine Learning</li>
</ul>
</dd>
</dl>
<hr />
<h2 id="&#38468;&#24405;">&#38468;&#24405;</h2>
<p>&#36825;&#26159;&#25105;&#24403;&#26102;&#26597;&#30475;&#20102;&#30340;&#19968;&#20123;&#36164;&#26009;&#65292;&#30041;&#20316;&#31508;&#35760;&#12290;</p>
<dl>
<dt>Multilayer perceptron</dt>
<dd><p>A multilayer perceptron (MLP) is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs. An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training the network. MLP is a modification of the standard linear perceptron and can distinguish data that are not linearly separable.</p>
<p>The term &#8220;multilayer perceptron&#8221; often causes confusion. It is argued the model is not a single perceptron that has multiple layers. Rather, it contains many perceptrons that are organised into layers, leading some to believe that a more fitting term might therefore be &#8220;multilayer perceptron network&#8221;. Moreover, these &#8220;perceptrons&#8221; are not really perceptrons in the strictest possible sense, as true perceptrons are a special case of artificial neurons that use a threshold activation function such as the Heaviside step function, whereas the artificial neurons in a multilayer perceptron are free to take on any arbitrary activation function. Consequently, whereas a true perceptron performs binary classification, a neuron in a multilayer perceptron is free to either perform classification or regression, depending upon its activation function.</p>
</dd>
<dt>Artificial neural network</dt>
<dd><p>In machine learning and cognitive science, artificial neural networks (ANNs) are a family of models inspired by biological neural networks (the central nervous systems of animals, in particular the brain) and are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are generally presented as <strong>systems of interconnected &#8220;neurons&#8221; which exchange messages between each other</strong>. The connections have numeric weights that can be tuned based on experience, making neural nets <strong>adaptive to inputs</strong> and <strong>capable of learning</strong>.</p>
<p>Like other machine learning methods - systems that learn from data - neural networks have been used to solve a wide variety of tasks that are hard to solve using ordinary rule-based programming, including computer vision and speech recognition.</p>
</dd>
</dl>
<div class="figure">
<img src="http://docs.opencv.org/2.4/_images/mlp.png" alt="Multi-Layer Perceptrons (MLP)" />
<p class="caption">Multi-Layer Perceptrons (MLP)</p>
</div>
<div class="figure">
<img src="https://upload.wikimedia.org/wikiversity/en/thumb/2/29/Neural-Network-Example-2.jpg/500px-Neural-Network-Example-2.jpg" alt="We can classify people in this problem using a single layer perceptron" />
<p class="caption">We can classify people in this problem using a single layer perceptron</p>
</div>
<div class="figure">
<img src="https://upload.wikimedia.org/wikiversity/en/c/cd/Neural-Network-Example-3.png" alt="A perceptron learns by a trial and error like method. It takes the input such as our math and Star Trek scores and outputs what it thinks the persons job is. Based on how far off its guess is, we will adjust each weight slowly to compensate. This will move its answers over time to be more accurate" />
<p class="caption">A perceptron learns by a trial and error like method. It takes the input such as our math and Star Trek scores and outputs what it thinks the persons job is. Based on how far off its guess is, we will adjust each weight slowly to compensate. This will move its answers over time to be more accurate</p>
</div>
<p><img src="https://upload.wikimedia.org/wikiversity/en/thumb/4/4a/Neural-Network-Example-4.jpg/500px-Neural-Network-Example-4.jpg" /> <img src="https://upload.wikimedia.org/wikiversity/en/3/36/Neural-Network-Example-5.jpg" /></p>
<div class="figure">
<img src="https://upload.wikimedia.org/wikiversity/en/thumb/b/b3/Neural-Network-Example-6.jpg/600px-Neural-Network-Example-6.jpg" alt="A Multilayer perceptron is two single layer perceptrons stacked on top of each other with typically a sigmoidal activation function between the two layers to make the numbers more crisp." />
<p class="caption">A Multilayer perceptron is two single layer perceptrons stacked on top of each other with typically a sigmoidal activation function between the two layers to make the numbers more crisp.</p>
</div>
<p>All the neurons in MLP are similar. Each of them has <strong>several input links</strong> (it takes the output values from several neurons in the <strong>previous layer</strong> as input) and <strong>several output links</strong> (it passes the response to several neurons in the <strong>next layer</strong>). The values retrieved from the previous layer are summed up with certain weights, individual for each neuron, plus the bias term. The sum is transformed using the activation function <span class="math inline">\(f\)</span> that may be also different for different neurons.</p>
<div class="figure">
<img src="http://docs.opencv.org/2.4/_images/neuron_model.png" />

</div>
<p>In other words, given the outputs <span class="math inline">\(x_j\)</span> of the layer <span class="math inline">\(n\)</span> , the outputs <span class="math inline">\(y_i\)</span> of the layer <span class="math inline">\(n+1\)</span> are computed as:</p>
<p><span class="math display">\[
u_i =  \sum _j (w^{n+1}_{i,j}*x_j) + w^{n+1}_{i,bias}
\]</span></p>
<p><span class="math display">\[
y_i = f(u_i)
\]</span></p>
<p>Different activation functions may be used. ML implements three standard functions:</p>
<ol>
<li><p><strong>Identity function</strong> ( <code class="sourceCode cpp">CvANN_MLP::IDENTITY</code> ): <span class="math inline">\(f(x)=x\)</span></p></li>
<li><p><strong>Symmetrical sigmoid</strong><a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> ( <code class="sourceCode cpp">CvANN_MLP::SIGMOID_SYM</code> ): <span class="math inline">\(f(x)=\beta*(1-e^{-\alpha x})/(1+e^{-\alpha x} )\)</span>, which is the default choice for MLP. The standard sigmoid with <span class="math inline">\(\beta =1\)</span>, <span class="math inline">\(\alpha =1\)</span> is shown below:</p>
<div class="figure">
<img src="http://docs.opencv.org/2.4/_images/sigmoid_bipolar.png" />

</div></li>
<li><p><strong>Gaussian function</strong> ( <code class="sourceCode cpp">CvANN_MLP::GAUSSIAN</code> ): <span class="math inline">\(f(x)=\beta e^{-\alpha x^2}\)</span>, which is not completely supported at the moment.</p></li>
</ol>
<p>In ML, all the neurons have the same activation functions, with the same free parameters (<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>) that are specified by user and are not altered by the training algorithms.</p>
<p>So, the whole trained network works as follows:</p>
<ol>
<li>Take the <strong>feature vector as input</strong>. The vector size is equal to the size of the input layer.</li>
<li>Pass values as input to <strong>the first hidden layer</strong> .</li>
<li><strong>Compute outputs</strong> of the hidden layer using the weights and the activation functions.</li>
<li>Pass outputs further <strong>downstream until you compute the output layer</strong>.</li>
</ol>
<p>So, to compute the network, you need to know all the weights <span class="math inline">\(w^{n+1}_{i,j}\)</span>. The weights are computed by the training algorithm. The algorithm takes a training set, multiple input vectors with the corresponding output vectors, and iteratively adjusts the weights to enable the network to give the desired response to the provided input vectors.</p>
<p>The larger the network size (the number of hidden layers and their sizes) is, the more the <strong>potential network flexibility</strong> is. The error on the training set could be made arbitrarily small. But at the same time the learned network also &#8220;learns&#8221; the noise present in the training set, so the error on the test set usually starts increasing after the network size reaches a limit. Besides, the larger networks are trained much longer than the smaller ones, so it is reasonable to pre-process the data, using <code class="sourceCode cpp">PCA::<span class="kw">operator</span>()</code> or similar technique, and train a smaller network on only essential features.</p>
<p>Another MLP feature is an inability to handle categorical data as is. However, there is a workaround. If a certain feature in the input or output (in case of <span class="math inline">\(n\)</span>-class classifier for <span class="math inline">\(n&gt;2\)</span>) layer is categorical and can take <span class="math inline">\(M&gt;2\)</span> different values, it makes sense to represent it as a binary tuple of <span class="math inline">\(M\)</span> elements, where the <span class="math inline">\(i\)</span>-th element is 1 if and only if the feature is equal to the <span class="math inline">\(i\)</span>-th value out of <span class="math inline">\(M\)</span> possible. It increases the size of the input/output layer but speeds up the training algorithm convergence and at the same time enables &#8220;fuzzy&#8221; values of such variables, that is, a tuple of probabilities instead of a fixed value.</p>
<p>ML implements two algorithms for training MLP&#8217;s. The first algorithm is a classical random sequential <strong>back-propagation</strong><a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a> algorithm. The second (default) one is a <strong>batch RPROP algorithm</strong>.</p>
<h2 id="cvann_mlp">CvANN_MLP</h2>
<dl>
<dt><code class="sourceCode cpp">CvANN_MLP::create</code></dt>
<dd><p>Constructs MLP with the specified topology.</p>
<p>Unlike many other models in ML that are constructed and trained at once, in the MLP model these steps are separated. First, a network with the specified topology is created using the non-default constructor or the method <code class="sourceCode cpp">CvANN_MLP::create()</code>. All the weights are set to zeros. Then, the network is <strong>trained using a set of input and output vectors</strong>. The training procedure can be repeated more than once, that is, the weights can be adjusted based on the new training data.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">void</span> CvANN_MLP::create( <span class="dt">const</span> Mat&amp; layerSizes,
                        <span class="dt">int</span> activateFunc=CvANN_MLP::SIGMOID_SYM,
                        <span class="dt">double</span> fparam1=<span class="dv">0</span>,
                        <span class="dt">double</span> fparam2=<span class="dv">0</span> );

<span class="dt">void</span> CvANN_MLP::create( <span class="dt">const</span> CvMat* layerSizes,
                        <span class="dt">int</span> activateFunc=CvANN_MLP::SIGMOID_SYM,
                        <span class="dt">double</span> fparam1=<span class="dv">0</span>,
                        <span class="dt">double</span> fparam2=<span class="dv">0</span> );</code></pre></div>
<p>Parameters</p>
<ol style="list-style-type: lower-roman">
<li><code>layerSizes</code> &#8211; Integer vector specifying the number of neurons in each layer including the input and output layers.</li>
<li><code>activateFunc</code> &#8211; Parameter specifying the activation function for each neuron: one of <code class="sourceCode cpp">CvANN_MLP::IDENTITY</code>, <code class="sourceCode cpp">CvANN_MLP::SIGMOID_SYM</code>, and <code class="sourceCode cpp">CvANN_MLP::GAUSSIAN</code>.</li>
<li><code>fparam1</code> &#8211; Free parameter of the activation function, <span class="math inline">\(\alpha\)</span>. See the formulas in the introduction section.</li>
<li><code>fparam2</code> &#8211; Free parameter of the activation function, <span class="math inline">\(\beta\)</span>. See the formulas in the introduction section.</li>
</ol>
</dd>
<dt><code class="sourceCode cpp">CvANN_MLP::train</code></dt>
<dd><p>Trains/updates MLP.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">int</span> CvANN_MLP::train( <span class="dt">const</span> Mat&amp; inputs,
                      <span class="dt">const</span> Mat&amp; outputs,
                      <span class="dt">const</span> Mat&amp; sampleWeights,
                      <span class="dt">const</span> Mat&amp; sampleIdx=Mat(),
                      CvANN_MLP_TrainParams params=CvANN_MLP_TrainParams(),
                      <span class="dt">int</span> flags=<span class="dv">0</span> );

<span class="dt">int</span> CvANN_MLP::train( <span class="dt">const</span> CvMat* inputs,
                      <span class="dt">const</span> CvMat* outputs,
                      <span class="dt">const</span> CvMat* sampleWeights,
                      <span class="dt">const</span> CvMat* sampleIdx=<span class="dv">0</span>,
                      CvANN_MLP_TrainParams params=CvANN_MLP_TrainParams(),
                      <span class="dt">int</span> flags=<span class="dv">0</span> );</code></pre></div>
<ol style="list-style-type: lower-roman">
<li><code>inputs</code> &#8211; Floating-point matrix of input vectors, one vector per row.</li>
<li><code>outputs</code> &#8211; Floating-point matrix of the corresponding output vectors, one vector per row.</li>
<li><code>sampleWeights</code> &#8211; (<code>RPROP</code> only) Optional floating-point vector of weights for each sample. Some samples may be more important than others for training. You may want to raise the weight of certain classes to find the right balance between hit-rate and false-alarm rate, and so on.</li>
<li><code>sampleIdx</code> &#8211; Optional integer vector indicating the samples (rows of inputs and outputs) that are taken into account. params &#8211; Training parameters. See the <code class="sourceCode cpp">CvANN_MLP_TrainParams</code> description.</li>
<li><code>flags</code>
<ul>
<li><p>Various parameters to control the training algorithm. A combination of the following parameters is possible:</p>
<ul>
<li><code>UPDATE_WEIGHTS</code> Algorithm updates the network weights, rather than computes them from scratch. In the latter case the weights are initialized using the Nguyen-Widrow algorithm.</li>
<li><code>NO_INPUT_SCALE</code> Algorithm does not normalize the input vectors. If this flag is not set, the training algorithm normalizes each input feature independently, shifting its mean value to 0 and making the standard deviation equal to 1. If the network is assumed to be updated frequently, the new training data could be much different from original one. In this case, you should take care of proper normalization.</li>
<li><code>NO_OUTPUT_SCALE</code> Algorithm does not normalize the output vectors. If the flag is not set, the training algorithm normalizes each output feature independently, by transforming it to the certain range depending on the used activation function.</li>
</ul></li>
</ul></li>
</ol>
</dd>
<dt><code class="sourceCode cpp">CvANN_MLP::predict</code></dt>
<dd><p>Predicts responses for input samples.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">float</span> CvANN_MLP::predict( <span class="dt">const</span> Mat&amp; inputs,
                          Mat&amp; outputs) <span class="dt">const</span>;

<span class="dt">float</span> CvANN_MLP::predict( <span class="dt">const</span> CvMat* inputs,
                          CvMat* outputs) <span class="dt">const</span>;</code></pre></div>
<p>If you are using the default <code>cvANN_MLP::SIGMOID_SYM</code> activation function with the default parameter values fparam1=0 and fparam2=0 then the function used is y = 1.7159<em>tanh(2/3 </em> x), so the output will range from [-1.7159, 1.7159], instead of [0,1].</p>
</dd>
</dl>
<p><code>int CvANN_MLP::get_layer_count()</code></p>
<p><code class="sourceCode cpp"><span class="dt">const</span> CvMat* CvANN_MLP::get_layer_sizes();</code></p>
<p><code class="sourceCode cpp"><span class="dt">double</span> *CvANN_MLP::get_weights( <span class="dt">int</span> layer );</code></p>
<h2 id="an-example">An example</h2>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="ot">#include &lt;opencv2/core/core.hpp&gt;</span>
<span class="ot">#include &lt;opencv2/highgui/highgui.hpp&gt;</span>
<span class="ot">#include &lt;opencv2/ml/ml.hpp&gt;</span>
<span class="ot">#include &lt;iostream&gt;</span>
<span class="ot">#include &lt;string&gt;</span>

<span class="kw">using</span> <span class="kw">namespace</span> std;
<span class="kw">using</span> <span class="kw">namespace</span> cv;

<span class="dt">int</span> main()
{
    CvANN_MLP bp;

    <span class="co">//  CvANN_MLP_TrainParams::CvANN_MLP_TrainParams()</span>
    <span class="co">//  {</span>
    <span class="co">//      term_crit = cvTermCriteria(</span>
    <span class="co">//              CV_TERMCRIT_ITER + CV_TERMCRIT_EPS, 1000, 0.01 );</span>
    <span class="co">//      train_method = RPROP;</span>
    <span class="co">//      bp_dw_scale = bp_moment_scale = 0.1;</span>
    <span class="co">//      rp_dw0 = 0.1; rp_dw_plus = 1.2; rp_dw_minus = 0.5;</span>
    <span class="co">//      rp_dw_min = FLT_EPSILON; rp_dw_max = 50.;</span>
    <span class="co">//  }</span>
    CvANN_MLP_TrainParams params;

    <span class="co">// CvANN_MLP_TrainParams::BACKPROP The back-propagation algorithm.</span>
    <span class="co">// CvANN_MLP_TrainParams::RPROP The RPROP algorithm.</span>
    params.train_method = CvANN_MLP_TrainParams::BACKPROP;

    <span class="co">// Strength of the weight gradient term. The recommended value is about 0.1</span>
    <span class="co">// params.bp_dw_scale = 0.1;</span>

    <span class="co">// Strength of the momentum term (the difference between weights on the 2</span>
    <span class="co">// previous iterations). This parameter provides some inertia to smooth the</span>
    <span class="co">// random fluctuations of the weights. It can vary from 0 (the feature is</span>
    <span class="co">// disabled) to 1 and beyond. The value 0.1 or so is good enough</span>
    <span class="co">// params.bp_moment_scale = 0.1;</span>

    <span class="co">// Initial value \Delta_0 of update-values \Delta_{ij}.</span>
    <span class="co">// params.rp_dw0 = 0.1;</span>

    <span class="co">// Increase factor \eta^+. It must be &gt;1.</span>
    <span class="co">// params.rp_dw_plus = 1.2;</span>

    <span class="co">// Decrease factor \eta^-. It must be &lt;1.</span>
    <span class="co">// params.rp_dw_minus = 0.5;</span>

    <span class="co">// Update-values lower limit \Delta_{min}. It must be positive.</span>
    <span class="co">// params.rp_dw_min = FLT_EPSILON;</span>

    <span class="co">// Update-values upper limit \Delta_{max}. It must be &gt;1.</span>
    <span class="co">// params.rp_dw_max = 50.;</span>

    <span class="co">// void CvANN_MLP::create(const Mat&amp; layerSizes,</span>
    <span class="co">//                        int activateFunc=CvANN_MLP::SIGMOID_SYM,</span>
    <span class="co">//                        double fparam1=0,</span>
    <span class="co">//                        double fparam2=0 )</span>
    <span class="co">// * layerSizes &#8211; #neurons in each layer including the input/output layers</span>
    <span class="co">// * activateFunc &#8211; activation function for each neuron</span>
    <span class="co">//      CvANN_MLP::IDENTITY</span>
    <span class="co">//      CvANN_MLP::SIGMOID_SYM</span>
    <span class="co">//      CvANN_MLP::GAUSSIAN</span>
    <span class="co">// * fparam1 &#8211; Free parameter of the activation function, \alpha</span>
    <span class="co">// * fparam2 &#8211; Free parameter of the activation function, \beta</span>
    Mat layerSizes=(Mat_&lt;<span class="dt">int</span>&gt;(<span class="dv">1</span>,<span class="dv">5</span>) &lt;&lt; <span class="dv">5</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">5</span>);
    bp.create( layerSizes, CvANN_MLP::SIGMOID_SYM );

    <span class="co">// int CvANN_MLP::train(</span>
    <span class="co">//      const Mat&amp; inputs,</span>
    <span class="co">//      const Mat&amp; outputs,</span>
    <span class="co">//      const Mat&amp; sampleWeights,</span>
    <span class="co">//      const Mat&amp; sampleIdx=Mat(),</span>
    <span class="co">//      CvANN_MLP_TrainParams params=CvANN_MLP_TrainParams()</span>
    <span class="co">//      int flags=0 )</span>

    <span class="dt">float</span> trainingData[<span class="dv">3</span>][<span class="dv">5</span>] = {
        {   <span class="dv">1</span>,   <span class="dv">2</span>,   <span class="dv">3</span>,   <span class="dv">4</span>,  <span class="dv">5</span> },
        { <span class="dv">111</span>, <span class="dv">112</span>, <span class="dv">113</span>, <span class="dv">114</span>, <span class="dv">115</span>},
        {  <span class="dv">21</span>,  <span class="dv">22</span>,  <span class="dv">23</span>,  <span class="dv">24</span>,  <span class="dv">25</span>},
    };
    Mat trainingDataMat(<span class="dv">3</span>, <span class="dv">5</span>, CV_32FC1, trainingData);

    <span class="dt">float</span> labels[<span class="dv">3</span>][<span class="dv">5</span>] = {
        { <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span> },
        { <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span> },
        { <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span> },
    };
    Mat labelsMat(<span class="dv">3</span>, <span class="dv">5</span>, CV_32FC1, labels);

    bp.train( trainingDataMat, labelsMat, Mat(), Mat(), params );

    <span class="co">// Data for visual representation</span>
    <span class="dt">int</span> width = <span class="dv">512</span>, height = <span class="dv">512</span>;
    Mat image = Mat::zeros( height, width, CV_8UC3 );
    Vec3b green( <span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">0</span> ), blue ( <span class="dv">255</span>, <span class="dv">0</span>, <span class="dv">0</span> );

    <span class="co">// Show the decision regions given by the SVM</span>
    <span class="kw">for</span> ( <span class="dt">int</span> i = <span class="dv">0</span>; i &lt; image.rows; ++i ) {
        <span class="kw">for</span> ( <span class="dt">int</span> j = <span class="dv">0</span>; j &lt; image.cols; ++j ) {
            Mat sampleMat = (Mat_&lt;<span class="dt">float</span>&gt;(<span class="dv">1</span>,<span class="dv">5</span>) &lt;&lt; i,j,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>);
            Mat responseMat;
            bp.predict( sampleMat, responseMat );
            <span class="dt">float</span> *p = responseMat.ptr&lt;<span class="dt">float</span>&gt;(<span class="dv">0</span>);
            <span class="dt">float</span> response = <span class="fl">0.0f</span>;
            <span class="kw">for</span>( <span class="dt">int</span> k=<span class="dv">0</span>; k &lt; <span class="dv">5</span>; k++ ) {
                response += p[k];
            }
            <span class="kw">if</span> ( response &gt; <span class="dv">2</span> ) {
                image.at&lt;Vec3b&gt;(j, i)  = green;
            } <span class="kw">else</span> {
                image.at&lt;Vec3b&gt;(j, i)  = blue;
            }
        }
    }

    <span class="co">// Show the training data</span>
    circle( image, Point(<span class="dv">501</span>,  <span class="dv">10</span>), <span class="dv">5</span>, Scalar(  <span class="dv">0</span>,   <span class="dv">0</span>,   <span class="dv">0</span>), <span class="dv">-1</span>, <span class="dv">8</span> );
    circle( image, Point(<span class="dv">255</span>,  <span class="dv">10</span>), <span class="dv">5</span>, Scalar(<span class="dv">255</span>, <span class="dv">255</span>, <span class="dv">255</span>), <span class="dv">-1</span>, <span class="dv">8</span> );
    circle( image, Point(<span class="dv">501</span>, <span class="dv">255</span>), <span class="dv">5</span>, Scalar(<span class="dv">255</span>, <span class="dv">255</span>, <span class="dv">255</span>), <span class="dv">-1</span>, <span class="dv">8</span> );
    circle( image, Point( <span class="dv">10</span>, <span class="dv">501</span>), <span class="dv">5</span>, Scalar(<span class="dv">255</span>, <span class="dv">255</span>, <span class="dv">255</span>), <span class="dv">-1</span>, <span class="dv">8</span> );

    imwrite(<span class="st">&quot;result.png&quot;</span>, image);       <span class="co">// save the image</span>
    imshow(<span class="st">&quot;BP Simple Example&quot;</span>, image); <span class="co">// show it to the user</span>
    cvWaitKey(<span class="dv">0</span>);
}</code></pre></div>
<hr />
<p><strong>&#21442;&#32771;&#25991;&#29486;</strong></p>
<ol>
<li><a href="https://en.wikipedia.org/wiki/Artificial_neural_network">Artificial neural network - Wikipedia, the free encyclopedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Backpropagation">Backpropagation - Wikipedia, the free encyclopedia</a></li>
<li><a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html">Backpropagation</a> &#9829;</li>
<li><a href="http://ropencv.aduda.eu/doc/OpenCV/CvANNMLPTrainParams.html">Class: OpenCV::CvANNMLPTrainParams &#8212; Documentation by YARD 0.8.7.4</a></li>
<li><a href="https://en.wikiversity.org/wiki/Learning_and_Neural_Networks">Learning and neural networks - Wikiversity</a></li>
<li><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer perceptron - Wikipedia, the free encyclopedia</a></li>
<li><a href="http://docs.opencv.org/2.4/modules/ml/doc/neural_networks.html">Neural Networks &#8212; OpenCV 2.4.12.0 documentation</a></li>
<li><a href="http://www.cnblogs.com/ronny/p/opencv_road_more_01.html">OpenCV&#36827;&#38454;&#20043;&#36335;&#65306;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#36710;&#29260;&#23383;&#31526; - &#9734;Ronny&#20022; - &#21338;&#23458;&#22253;</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid function - Wikipedia, the free encyclopedia</a></li>
<li><a href="http://blog.csdn.net/xiaowei_cqu/article/details/9027617">&#12304;&#27169;&#24335;&#35782;&#21035;&#12305;OpenCV&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476; CvANN_MLP - &#23567;&#39759;&#30340;&#20462;&#34892;&#36335; - &#21338;&#23458;&#39057;&#36947; - CSDN.NET</a></li>
<li><a href="http://blog.csdn.net/xiaowei_cqu/article/details/9023247">&#12304;&#27169;&#24335;&#35782;&#21035;&#12305;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476; BPNN - &#23567;&#39759;&#30340;&#20462;&#34892;&#36335; - &#21338;&#23458;&#39057;&#36947; - CSDN.NET</a></li>
<li><a href="http://www.cnblogs.com/ronny/p/ann_01.html">&#24863;&#30693;&#22120;&#19982;&#26799;&#24230;&#19979;&#38477; - &#9734;Ronny&#20022; - &#21338;&#23458;&#22253;</a></li>
<li><a href="http://www.cnblogs.com/ronny/p/ann_02.html">&#31070;&#32463;&#32593;&#32476;&#65306;&#22810;&#23618;&#32593;&#32476;&#19982;C++&#23454;&#29616; - &#9734;Ronny&#20022; - &#21338;&#23458;&#22253;</a></li>
<li><a href="http://docs.opencv.org/master/d0/dce/classcv_1_1ml_1_1ANN__MLP.html#ac24cc2e2fc5cd1dd74fd5da31886fbb7">OpenCV: cv::ml::ANN_MLP Class Reference</a></li>
<li><a href="http://www.opencv.org.cn/opencvdoc/2.3.2/html/modules/imgproc/doc/miscellaneous_transformations.html">Miscellaneous Image Transformations &#8212; OpenCV 2.3.2 documentation</a></li>
<li><a href="http://docs.opencv.org/master/db/d7d/classcv_1_1ml_1_1StatModel.html#a1f3854a1d367973da4c27dc5f54d3348">OpenCV: cv::ml::StatModel Class Reference</a></li>
<li><a href="http://docs.opencv.org/master/d0/dce/classcv_1_1ml_1_1ANN__MLP.html#afb51e414f22dd69f281a569145ccfad7">OpenCV: cv::ml::ANN_MLP Class Reference</a></li>
<li><a href="http://docs.opencv.org/master/dc/d32/classcv_1_1ml_1_1TrainData.html">OpenCV: cv::ml::TrainData Class Reference</a></li>
<li><a href="http://docs.opencv.org/master/dc/d32/classcv_1_1ml_1_1TrainData.html#a1f2950eb49e251cafa3a62a8afc8ed72">OpenCV: cv::ml::TrainData Class Reference</a></li>
<li><a href="http://www.codeproject.com/Articles/13582/Back-propagation-Neural-Net">Back-propagation Neural Net - CodeProject</a></li>
<li><a href="https://msdn.microsoft.com/en-us/magazine/jj658979.aspx">Test Run - Neural Network Back-Propagation for Programmers</a></li>
</ol>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://docs.python.org/2/library/shutil.html">10.10. shutil &#8212; High-level file operations &#8212; Python 2.7.11 documentation</a><a href="#fnref1">&#8617;</a></p></li>
<li id="fn2"><p><a href="http://deeplearning.net/software/theano/tutorial/">Tutorial &#8212; Theano 0.8.0 documentation</a>.<a href="#fnref2">&#8617;</a></p></li>
<li id="fn3"><p>see <a href="http://deeplearning.net/software/theano/tutorial/numpy.html#broadcasting">NumPy refresher &#8212; Theano 0.8.0 documentation</a>.<a href="#fnref3">&#8617;</a></p></li>
<li id="fn4"><p><a href="http://deeplearning.net/software/theano/tutorial/gradients.html">Derivatives in Theano &#8212; Theano 0.8.0 documentation</a>.<a href="#fnref4">&#8617;</a></p></li>
<li id="fn5"><p><a href="http://deeplearning.net/software/theano/tutorial/conditions.html">Conditions &#8212; Theano 0.8.0 documentation</a>.<a href="#fnref5">&#8617;</a></p></li>
<li id="fn6"><p><a href="https://en.wikipedia.org/wiki/NAND_gate">NAND gate - Wikipedia, the free encyclopedia</a>.<a href="#fnref6">&#8617;</a></p></li>
<li id="fn7"><p>A sigmoid function is a mathematical function having an &#8220;S&#8221; shape (sigmoid curve).</p>
<p>&#21333;&#35843;&#36882;&#22686;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#26080;&#38480;&#27425;&#21487;&#24494;&#12290;&#24403;&#19988;&#20165;&#24403;&#26435;&#20540;&#36739;&#22823;&#26102;&#21487;&#20197;&#36924;&#36817;&#38408;&#20540;&#20989;&#25968;&#65292;&#24403;&#26435;&#20540;&#36739;&#23567;&#26102;&#36924;&#32447;&#24615;&#20989;&#25968;&#12290;</p>
<p>Besides the logistic function (<span class="math inline">\(f(x) = \frac{L}{1 + \mathrm e^{-k(x-x_0)}}\)</span>), sigmoid functions include the ordinary arctangent, the hyperbolic tangent, the Gudermannian function, and the error function (<span class="math inline">\(\operatorname{erf}(x) = \frac{2}{\sqrt\pi}\int_0^x e^{-t^2}\,\mathrm dt\)</span>), but also the generalised logistic function and algebraic functions like <span class="math inline">\(f(x)=\tfrac{x}{\sqrt{1+x^2}}\)</span>.</p>
<div class="figure">
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Gjl-t%28x%29.svg/700px-Gjl-t%28x%29.svg.png" />

</div>
<p>&#160;<a href="#fnref7">&#8617;</a></p></li>
<li id="fn8"><p>Backpropagation, an abbreviation for &#8220;backward propagation of errors&#8221;, is a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of a loss function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the loss function.</p>
<p>BP &#31639;&#27861;&#23601;&#26159;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#32593;&#32476;&#30340;&#26435;&#20540;&#20351;&#24471;&#36755;&#20986;&#19982;&#36755;&#20837;&#20043;&#38388;&#30340;&#23454;&#38469;&#26144;&#23556;&#20851;&#31995;&#19982;&#25152;&#26399;&#26395;&#30340;&#26144;&#23556;&#20851;&#31995;&#19968;&#33268;&#65292;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#36890;&#36807;&#35843;&#25972;&#21508;&#23618;&#26435;&#20540;&#27714;&#30446;&#26631;&#20989;&#25968;&#26368;&#23567;&#21270;&#12290;</p>
<p>Backpropagation requires a known, desired output for each input value in order to calculate the loss function gradient. It is therefore usually considered to be a supervised learning method, although it is also used in some unsupervised networks such as autoencoders. It is a generalization of the delta rule to multi-layered feedforward networks, made possible by using the chain rule to iteratively compute gradients for each layer. Backpropagation requires that the activation function used by the artificial neurons (or &#8220;nodes&#8221;) be differentiable.</p>
<p>Since backpropagation uses the gradient descent method, one needs to calculate the derivative of the squared error function with respect to the weights of the network. Assuming one output neuron,[note 2] the squared error function is:</p>
<p><span class="math display">\[E = \tfrac{1}{2}(t - y)^2,\]</span></p>
<dl>
<dt>where</dt>
<dd><span class="math inline">\(E\)</span> is the squared error,
</dd>
<dd><span class="math inline">\(t\)</span> is the target output for a training sample, and
</dd>
<dd><span class="math inline">\(y\)</span> is the actual output of the output neuron.
</dd>
</dl>
<p>The factor of <span class="math inline">\(\textstyle\frac{1}{2}\)</span> is included to cancel the exponent when differentiating. Later, the expression will be multiplied with an arbitrary learning rate, so that it doesn&#8217;t matter if a constant coefficient is introduced now.</p>
<p>For each neuron <span class="math inline">\(j\)</span>, its output <span class="math inline">\(o_{j}\)</span> is defined as</p>
<p><span class="math display">\[o_{j} = \varphi(\mbox{net}_{j}) = \varphi\left(\sum_{k=1}^{n}w_{kj}o_k\right).\]</span></p>
<p>The input <span class="math inline">\(\mbox{net}_{j}\)</span> to a neuron is the weighted sum of outputs o_k of previous neurons. If the neuron is in the first layer after the input layer, the <span class="math inline">\(o_k\)</span> of the input layer are simply the inputs <span class="math inline">\(x_k\)</span> to the network. The number of input units to the neuron is <span class="math inline">\(n\)</span>. The variable <span class="math inline">\(w_{ij}\)</span> denotes the weight between neurons <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</p>
<p>The activation function <span class="math inline">\(\varphi\)</span> is in general non-linear and differentiable. A commonly used activation function is the logistic function:</p>
<p><span class="math display">\[\varphi(z) = \frac{1}{1+e^{-z}}\]</span></p>
<p>which has a nice derivative of:</p>
<p><span class="math display">\[\frac {\partial\varphi}{\partial z} = \varphi(1-\varphi)\]</span></p>
<p>Calculating the partial derivative of the error with respect to a weight <span class="math inline">\(w_{ij}\)</span> is done using the chain rule twice:</p>
<p><span class="math display">\[\frac{\partial E}{\partial w_{ij}} = \frac{\partial E}{\partial o_j} \frac{\partial o_j}{\partial\mathrm{net_j}} \frac{\partial \mathrm{net_j}}{\partial w_{ij}}\]</span></p>
<p>In the last term of the right-hand side of the following, only one term in the sum <span class="math inline">\(\mathrm{net_j}\)</span> depends on <span class="math inline">\(w_{ij}\)</span>, so that</p>
<p><span class="math display">\[\frac{\partial \mathrm{net_j}}{\partial w_{ij}} = \frac{\partial}{\partial w_{ij}}\left(\sum_{k=1}^{n}w_{kj}o_k\right) = o_i.\]</span></p>
<p>If the neuron is in the first layer after the input layer, <span class="math inline">\(o_i\)</span> is just <span class="math inline">\(x_i\)</span>. The derivative of the output of neuron <span class="math inline">\(j\)</span> with respect to its input is simply the partial derivative of the activation function (assuming here that the logistic function is used):</p>
<p><span class="math display">\[\frac{\partial o_j}{\partial\mathrm{net_j}} = \frac {\partial}{\partial \mathrm{net_j}}\varphi(\mathrm{net_j}) = \varphi(\mathrm{net_j})(1-\varphi(\mathrm{net_j}))\]</span></p>
<p>This is the reason why backpropagation requires the activation function to be differentiable.</p>
<p>The first term is straightforward to evaluate if the neuron is in the output layer, because then <span class="math inline">\(o_j = y\)</span> and</p>
<p><span class="math display">\[\frac{\partial E}{\partial o_j} = \frac{\partial E}{\partial y} = \frac{\partial}{\partial y} \frac{1}{2}(t - y)^2 = y - t\]</span></p>
<p>However, if <span class="math inline">\(j\)</span> is in an arbitrary inner layer of the network, finding the derivative E with respect to <span class="math inline">\(o_j\)</span> is less obvious.</p>
<p>Considering <span class="math inline">\(E\)</span> as a function of the inputs of all neurons <span class="math inline">\(L = {u, v, \dots, w}\)</span> receiving input from neuron <span class="math inline">\(j\)</span>,</p>
<p><span class="math display">\[\frac{\partial E(o_j)}{\partial o_j} = \frac{\partial E(\mathrm{net}_u, \mathrm{net}_v, \dots, \mathrm{net}_w)}{\partial o_j}\]</span></p>
<p>and taking the total derivative with respect to <span class="math inline">\(o_j\)</span>, a recursive expression for the derivative is obtained:</p>
<p><span class="math display">\[\frac{\partial E}{\partial o_j} = \sum_{l \in L} \left(\frac{\partial E}{\partial \mathrm{net}_l}\frac{\partial \mathrm{net}_l}{\partial o_j}\right) = \sum_{l \in L} \left(\frac{\partial E}{\partial o_{l}}\frac{\partial o_{l}}{\partial \mathrm{net}_l}w_{jl}\right)\]</span></p>
<p>Therefore, the derivative with respect to <span class="math inline">\(o_j\)</span> can be calculated if all the derivatives with respect to the outputs <span class="math inline">\(o_l\)</span> of the next layer - the one closer to the output neuron &#8211; are known.</p>
<p>Putting it all together:</p>
<p><span class="math display">\[\dfrac{\partial E}{\partial w_{ij}} = \delta_{j} o_{i}\]</span></p>
<p>with</p>
<p><span class="math display">\[\delta_{j} = \frac{\partial E}{\partial o_j} \frac{\partial o_j}{\partial\mathrm{net_j}} = \begin{cases}
(o_{j}-t_{j})\varphi(\mbox{net}_{j})(1-\varphi(\mbox{net}_{j})) &amp; \mbox{if } j \mbox{ is an output neuron,}\\
(\sum_{l\in L} \delta_{l} w_{jl})\varphi(\mbox{net}_{j})(1-\varphi(\mbox{net}_{j}))  &amp; \mbox{if } j \mbox{ is an inner neuron.}
\end{cases}\]</span></p>
<p>To update the weight <span class="math inline">\(w_{ij}\)</span> using gradient descent, one must choose a learning rate, <span class="math inline">\(\alpha\)</span>. The change in weight, which is added to the old weight, is equal to the product of the learning rate and the gradient, multiplied by -1:</p>
<p><span class="math display">\[\Delta w_{ij} = - \alpha \frac{\partial E}{\partial w_{ij}}\]</span></p>
<p>The <span class="math inline">\(\textstyle -1\)</span> is required in order to update in the direction of a minimum, not a maximum, of the error function.</p>
<p>For a single-layer network, this expression becomes the Delta Rule. To better understand how backpropagation works, here is an example to illustrate it: The Back Propagation Algorithm, page 20.</p>
<p>Limitations</p>
<ol>
<li>Gradient descent can find the local minimum instead of the global minimum Gradient descent with backpropagation is not guaranteed to global minimum of the error function, but only a local minimum; also, it has trouble crossing plateaus in the error function landscape. This issue, caused by the non-convexity of error functions in neural networks, was long thought to be a major drawback, but in a 2015 review article, Yann LeCun et al. argue that in many practical problems, it isn&#8217;t.</li>
<li>Backpropagation learning does not require normalization of input vectors; however, normalization could improve performance</li>
</ol>
<p>A simple neural network with two input units and one output unit:</p>
<div class="figure">
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/A_simple_neural_network_with_two_input_units_and_one_output_unit.png/250px-A_simple_neural_network_with_two_input_units_and_one_output_unit.png" />

</div>
<p>gradient descent:</p>
<div class="figure">
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Extrema_example.svg/250px-Extrema_example.svg.png" />

</div>
<p>&#160;<a href="#fnref8">&#8617;</a></p></li>
</ol>
</div>
</div>
<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" class="blur-svg">
    <defs>
        <filter id="blur-filter">
            <feGaussianBlur stdDeviation="3"></feGaussianBlur>
        </filter>
    </defs>
</svg>
<script src="../lazyload.min.js"></script>
<script src="../jquery-3.0.0.min.js"></script>
<script src="../jquery.idTabs.min.js"></script>
<script src="../egg.min.js"></script>
<script src="../clipboard.min.js"></script>
<script src="../notes.js"></script>
</body>
</html>
